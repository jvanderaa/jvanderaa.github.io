{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83c\udfe0 Home","text":"<p> |  |  |  |  | </p> <p>Thank you for visiting my blog. My goal out of this blog is provide helpfulness around networking, network automation, and perhaps a few personal view points. The majority of the posts are going to be related to tech, and usually networking and network automation.</p>"},{"location":"#blog","title":"Blog","text":""},{"location":"about/","title":"About","text":"<p>I'm a veteran Network Engineer who has finally caught on how to put together CS teachings with a Networking degree. Working on doing almost all things via automation.</p> <p>Self re-taught network/automation engineer. I've primarily been digging around within Ansible and Python. I've been doing Python networking since 2015 when a speaker came in and talked about how he had put together a NOC for a conference heavily leveraging automation capabilities via Python. Since then I have been going deep down the rabbit hole that is Network Automation.</p> <p>In 2020 Cisco released the Cisco DevNet certification program. I am one of the first 500 individuals to achieve a Cisco DevNet certification, with passing first the DevNet Associate exam. Later I followed up that certification with the DevNet Professional certification as well.</p> <p></p>"},{"location":"about/#my-work","title":"My Work","text":""},{"location":"about/#book","title":"Book","text":"<p>During the pandemic, I thought what better way to get started than with writing a book about getting started with open source network tools. Get it on LeanPub</p>"},{"location":"about/#speaking-engagements","title":"Speaking Engagements","text":"<p>2023 Internet2 Technology Exchange At Internet2's TechEx 2023 I gave an overview of Nautobot to the Network Automation community and introduced several of the core Nautobot Open Source Ecosystem Plugins such as Nautobot Golden Config, and the new configuration remediation capabilities built in.</p> <p>2023 WWT Automation Day - Minneapolis In this speaking engagement I had the opportunity to talk about Nautobot as the Open Source Network Automation and SOT applications. I gave an overview of what Nautobot brings to the table and incorporated demos of using real world scripts to complete activities leveraging the Nautobot open source ecosystem.</p> <p>2023 Summer MNNUG Introduction of Nautobot and the core functionality for the the audience. Drove attendance with the focus on Network Automation that saw individuals drive from 3 hours away!</p> <p>2022 NANOG86 In 2022 I had the opportunity to complete my first NANOG talk, on automating circuit maintenance notifications. This section will outline my continued source of talks:</p> <ul> <li>NANOG86 - Automating Circuit Maintenance Notifications</li> </ul> <p>2021 Ansible Meetup Minneapolis Ansible Meetup January 2021 - Ansible Records Ansible - ARA</p> <p>2020 Interop Interop Network Automation CICD</p> <p>2020 Ansible Meetup - Minneapolis Ansible Meetup April 2020 Talk</p>"},{"location":"about/#podcasts","title":"Podcasts","text":"<ul> <li>Packet Pushers - Ansible or Terraform</li> <li>Network Automation Nerds - Interview Part 1</li> <li>Network Automation Nerds - Interview Part 2</li> </ul>"},{"location":"about/#ansible-content","title":"Ansible Content","text":"<p>In 2019 I worked to develop a Network Automation Course with Ansible for Packet Pushers. This was in my transition time to Network to Code. Note that the course was developed at a time when only NetBox was around. At this point in time I would swap out NetBox in favor of Nautobot. </p> <ul> <li>Packet Pushers Network Automation with Ansible: YouTube</li> <li>Network Automation Course (Packet Pushers): Ignition page.</li> <li>Minneapolis Ansible Meetup April 2020 Talk</li> <li>First half using NetBox with the NetBox Ansible Collections</li> <li>Second half on creating your own custom filters</li> <li>Ansible Guest Blog Post - Using NetBox as Ansible Source of Truth</li> <li>Minneapolis Ansible Meetup January 2021 - Ansible Records</li> </ul>"},{"location":"about/#telemetry-content","title":"Telemetry Content","text":"<ul> <li>How to Introduce Telemetry Streaming (gNMI) in Your Network with SNMP with Telegraf</li> <li>Upcoming: DZone: How to Introduce Telemetry Streaming (gNMI) in Your Network with SNMP with Telegraf</li> </ul>"},{"location":"about/#ntc-blog-posts-i-wrote","title":"NTC Blog Posts I Wrote","text":"<ul> <li>Network Telemetry for SNMP Devices</li> <li>How to Monitor Your VPN Infrastructure with Netmiko, NTC-Templates, and a Time Series Database</li> <li>Monitor Your Network With gNMI, SNMP, and Grafana</li> <li>Monitoring Websites with Telegraf and Prometheus</li> <li>Alerting with Prometheus</li> </ul>"},{"location":"about/#cisco-champion","title":"Cisco Champion","text":"<p>2021 - Cisco Champion</p> <p></p>"},{"location":"book/","title":"Book","text":"Open Source Network Management Cover <p>I have created a book that is all about getting started with open source tools. Open Source Network Management, a book about getting started with using open source tools to manage your network environment. This book guides one along the way of installing, initial configuration, and basic ways to use the tools. All these are done with installing on a single virtual machine or on an Intel NUC system.</p> <p>The book is published on LeanPub that is a place where I can continue to make updates to the book while providing the content to the readers sooners than later. The current state of the book is at a strong technical release state. I am working through having some editing done on the book yet, but no edits of the technical components will be made.</p> <p>The tools covered in the book thus far:</p> <ul> <li>Nautobot</li> <li>Hashicorp Vault</li> <li>InfluxData Telegraf</li> <li>Prometheus</li> <li>Grafana</li> <li>NGINX</li> </ul> <p>I encourage you to take a look, with the purchase you will be able to get the book in multiple formats including PDF, epub, and mobi formats.</p>"},{"location":"book/#amazon-links","title":"Amazon Links","text":"<p>I have made the book available now on Amazon Kindle and in the physical form from Amazon Kindle Direct Publishing. Which the book will be printed on demand in a physical format! Check out Amazon for these options - Amazon</p> <p>-Josh</p>"},{"location":"links/","title":"Links","text":""},{"location":"links/#my-content-on-other-sites","title":"My Content on Other Sites","text":"<p>Using NetBox as a SoT for Ansible + Diving into Ansible Filters In the video I go over NetBox as your Source of Truth, and walk one through getting started with using Ansible as that Source of Truth. In the later portion I go through using Ansible Filters to audit NetBox data to the device data.</p> <p>Introduction to Ansible for Network Automation This is a course that I created for Packet Pushers Ignition site. It is meant as real world introduction of Network Automation leveraging Ansilbe. The course takes you through many common tasks accomplished within Ansible, providing many examples.</p>"},{"location":"links/#online-network-simulators-free","title":"Online Network Simulators (Free)","text":"<p>Online VIRL NRE Labs YouTube Video Introducing VIRL free</p>"},{"location":"links/#podcasts","title":"Podcasts","text":"<p>The shows listed here are particular episodes that have resonated with me. I may be missing some, especially going back a few years as I just really started in on these podcasts lately even though they have been around for some time.</p>"},{"location":"links/#zigbits-network-design","title":"Zigbits Network Design","text":"<p>Main Page: Zigbits.tech</p> <p>CI/CD Podcast One of my favorites on CI/CD, including definitions and examples.</p> <p>Designing for DevOps Nicholos Russo having a conversation around Designing for DevOps</p>"},{"location":"links/#packet-pushers","title":"Packet Pushers","text":"<p>Introducing NRE Labs NRE Labs is a site that I'm going to recommend frequently. It is a place to get started with tutorials about Network Automation. Go here for free and relevant learning!</p> <p>Building a Network Automation Framework - Ken Celenza A podcast about building automation frameworks. This is a long one, but excellent for what an automated environment will look like</p> <p>The Source of Truth Shall Set You Free (To Automate) Podcast of using Netbox, Grafana, Influx, and a modern company system</p>"},{"location":"links/#the-network-collective","title":"The Network Collective","text":"<p>Grassroots Automation The Network Collective brings many contributors to the automation community to the table to discuss various technologies related to Network Automation.</p>"},{"location":"links/#automationdevops","title":"Automation/DevOps","text":""},{"location":"links/#blog-sites","title":"Blog Sites","text":"<p>Byrn Baker Nick Russo - @nickrusso42518 Colin McCarthy - @Colinnation Katherine McNamara - @kmcnam1 John Capobiano Jason Edelman Ken Celenza Scott Lowe</p>"},{"location":"links/#links-to-checkout-further","title":"Links to checkout further","text":"<p>Building Dynamic Documentation Using Parser Genie Parsers</p>"},{"location":"links/#jsonyaml","title":"JSON/YAML","text":"<p>David Barroso - JSON in Flat Format</p>"},{"location":"links/#ansible","title":"Ansible","text":"<p>Pinakes - Self Service portal EcoSystem Homepage Network CLI Connection Types Ansible Lint Github Link Ken's List of Links IPvSean IP Address Filter Cisco pyATS Ansible Tools ASA for Vagrant Ansible 2.5 Networking Changes Copy SSH Keys, format of playbook, etc. Guy who wrote a book on dev ops Jeff Geerling - Ansible with Win10 Save IOS configs with Ansible Ansible Network Automation Github Page Create a list from output of Ansible YAML Guide 1 Coding Packets Ansible</p>"},{"location":"links/#monitoring-and-telemetry-links","title":"Monitoring and Telemetry Links","text":"<p>Panos2Grafana Panograf APC UPS Monitoring </p>"},{"location":"links/#python","title":"Python","text":"<p>Real Python Code Quality</p>"},{"location":"links/#nornir","title":"Nornir","text":"<p>TBD, content coming soon!</p>"},{"location":"links/#jenkins","title":"Jenkins","text":"<p>Setting up Ansible in Jenkins</p>"},{"location":"links/#helpful-tools","title":"Helpful Tools","text":"<p>Template Rendering Online Tester PiKVM Proxmox Kubernetes Environment </p>"},{"location":"links/#database-tools","title":"Database Tools","text":"<p>DBeaver - Thanks @itdependsnet  </p>"},{"location":"links/#technology-overviews","title":"Technology Overviews","text":"<p>Ansible Networking Github Page VxLAN Overview 1 Cisco DC Spine &amp; Leaf Design Overview WhitePaper Cisco Security Events - Syslog Ivan BGP in EVPN Spanning Tree: Nexus 5k Guide Lower end Cat Switches Guide to better SSH Security VSS Main Cumulus Comparing Commands</p>"},{"location":"links/#telemetry","title":"Telemetry","text":"<p>Cisco Getting Started with Streaming Telemetry Cisco Scaling Telemetry with IOS-XR &amp; InfluxData</p>"},{"location":"links/#generic-hardware-links","title":"Generic Hardware Links","text":"<p>USB-C 4k video explained</p>"},{"location":"links/#mac-hacks","title":"Mac Hacks","text":"<p>macOS Setup Guide - sourabhajaj</p>"},{"location":"links/#new-tech","title":"New Tech","text":""},{"location":"links/#gns3-tech","title":"GNS3 Tech","text":""},{"location":"links/#gns3-automation","title":"GNS3 Automation","text":"<p>NetPanda Ansible+GNS3</p>"},{"location":"links/#dmvpn","title":"DMVPN","text":"<p>GNS3 DMVPN Intro GNS3 Learning Lab - Python for Network Engineers $$$$ Greg Mueller Youtube Series</p>"},{"location":"links/#vxlan","title":"VxLAN","text":"<p>VxLAN GNS3 CSR1000V OSPF VxLAN Overview Video Ansible PB to push VNIs</p>"},{"location":"links/#vagrant","title":"Vagrant","text":"<p>Getting started with Network Vagrant Images</p>"},{"location":"links/#programming-nuggets","title":"Programming Nuggets","text":""},{"location":"links/#netmiko","title":"Netmiko","text":"<p>Kirk Byers Home Page Loading base configurations into VIRL Images (GNS3) Theading with Netmiko Example</p>"},{"location":"links/#yang-modeling","title":"YANG Modeling","text":"<p>Cisco Yang Develpment Kit - Github Cisco YANG developemnt Kit Readthedocs</p>"},{"location":"links/#python_1","title":"Python","text":"<p>Python Bandit (Security) Python Cheat Sheets Python Switcher/Case Statement</p>"},{"location":"links/#cheat-sheets","title":"Cheat Sheets","text":"<p>Markdown Markdown Wordpress Packet Life Protocols Cheat Sheets Online Subnet Calculator Page</p>"},{"location":"links/#web-net-development","title":"Web Net Development","text":""},{"location":"links/#django","title":"Django","text":"<p>This is the my original network development platform. I went with it due to challenges that Flask presented around getting it to look \"Pretty\" using Bootstrap. This looked like the right decision for the time. I would probably recommend Network Engineers using Flask for web development at this point. Django LDAP Example</p>"},{"location":"links/#flask","title":"Flask","text":"<p>This is a micro-services web setup. It comes very basic and is very easy to get moving. First recommendation - Flask Mega-Tuturial. It is awesome. Well worth the purchase.</p> <p>Flask Mega-Tutorial Flask LDAP</p>"},{"location":"links/#github-pages-help","title":"Github Pages Help","text":"<p>Original Github pages links</p>"},{"location":"links/#open-source-projects","title":"Open Source Projects","text":"<p>Fabio Load Balancer</p>"},{"location":"links/#product-documentation-links","title":"Product Documentation Links","text":"<p>Cat9300</p>"},{"location":"links/#career","title":"Career","text":"<p>PacketPushers: Portfolio</p>"},{"location":"links/#home-office-links","title":"Home Office Links","text":"<p>Great Priced Standing Desk</p>"},{"location":"links/#raspberry-pi-links","title":"Raspberry Pi Links","text":"<p>Setup as DNS Server Raw output of DNS Server Link</p>"},{"location":"links/#gns3-tips","title":"GNS3 Tips","text":"<p>If looking to connect via Netmiko to GNS3, this may help:</p> <pre><code>net_connect = ConnectHandler(\n    device_type='generic_termserver_telnet',        # changed\n    ip='127.0.0.1',\n    username='pyclass',\n    password='cisco',\n    global_delay_factor=4,\n    default_enter='\\r\\n',\n    port=5000)\n</code></pre>"},{"location":"links/#inspirational-links","title":"Inspirational Links","text":"<p>Admiral McRaven @ UT</p>"},{"location":"links/#appendix","title":"Appendix","text":""},{"location":"links/#sublime-text-editor-links","title":"Sublime Text Editor Links","text":"<p>Download: Sublime Text Main Page Syncing Mac: Map end/home keybindings Space Gray Themes? Open From Terminal with word Sublime</p>"},{"location":"ansible-blocks/","title":"Ansible Blocks","text":"<p>One of the more interesting features that I have just come across within the Ansible automation world is that of the <code>block</code>. I find this very helpful for both error handling, and also grouping tasks into logical separation.</p>","tags":["ansible"]},{"location":"ansible-blocks/#ansible-official-link","title":"Ansible Official Link","text":"<p>Ansible Docs: Block</p> <p>Blocks allow for logical grouping of tasks and in play error handling. Most of what you can apply to a single task can be applied at the block level, which also makes it much easier to set data or directives common to the tasks. This does not mean the directive affects the block itself, but is inherited by the tasks enclosed by a block. i.e. a when will be applied to the tasks, not the block itself.</p>","tags":["ansible"]},{"location":"ansible-blocks/#so-what-is-this-about","title":"So what is this about?","text":"<p>The primary reason to use blocks within Ansible is for error handling. I liken this a lot to the Python <code>try:</code> and <code>except:</code> exception handling. You are able to group tasks into one error \"group\" and then provides for rescue blocks and always executes blocks. This can be extremely helpful.</p>","tags":["ansible"]},{"location":"ansible-blocks/#using-with-when","title":"Using with when","text":"<p>I've found that a second place to put in blocks within Ansible is to also pair it with a <code>when:</code> statement to help separate out tasks. Some may put this into a different play, but the down side of this is when you are leveraging variables. With separate plays you will be defining variables within each play. With using blocks to define what to do when, can be very helpful.</p>","tags":["ansible"]},{"location":"network-cicd/","title":"Network CI/CD - work in progress (Links to other videos/pages)","text":"<p>At DevNet Create 2018 there is a video that was put together related to CI/CD in the NetDevOps world. This is something that is very exciting to see that there has been time put in to putting that together. The video can be seen here:</p> <p>DevNet Create Video Pete Lumbis CI/CD Info</p> <p>With tools such as VIRL, EveNG, and GNS3, there should be a methodology to be able to put a network together to do testing. I have not yet put together the full pipeline, but is something that I'm very interested in getting built some day.</p> <p>Once a network engineer has a full tested pipeline, we can maybe finally get to doing changes with more confidence and more during normal hours.</p>","tags":["netdevops","ci/cd"]},{"location":"network-cicd/#tools-that-im-aware-of-for-doing-the-cicd-pipeline-include-definitely-not-limited-to-are","title":"Tools that I'm aware of for doing the CI/CD pipeline include (definitely not limited to) are:","text":"<ul> <li>Jenkins  </li> <li>Travis  </li> <li>Drone  </li> <li>Gitlab  </li> <li>Bamboo  </li> </ul>","tags":["netdevops","ci/cd"]},{"location":"network-cicd/#some-good-resources-that-are-available-for-building-cisco-vagrant-vms","title":"Some good resources that are available for building Cisco Vagrant VMs","text":"<p>https://github.com/hpreston/vagrant_net_prog/tree/master/box_building#cisco-nexus-9000v https://techbloc.net/archives/1925 https://techbloc.net/archives/1865 </p>","tags":["netdevops","ci/cd"]},{"location":"getting-started-with-the-blog/","title":"Getting Started with the Blog","text":"<p>Why this post? Because I decided to change the style of how I was hosting my blog. Before I had decided to just host the blog on something that was easy to get to and update. I could have kept on blogging there, but I found making blog posts a little bit more difficult than what I wanted to. I also wanted to learn some of the <code>new</code> ways of doing things within networking technologies.</p> <p>With this, I decided to bring my blog over to a static site generator. I'm not doing anything significantly crazy with a blog site, other than hopefully creating some useful content. So static site generation brought me over to Github.</p>","tags":["blog"]},{"location":"getting-started-with-the-blog/#what-i-wanted-to-accomplish","title":"What I wanted to accomplish","text":"<p>What I wanted to accomplish with my blog: - Create some useful content that others may find helpful - Leverage Markdown for quick document creation (Previous site was time consuming in my mind to create content, but it did get a start) - If possible, figure out how to appropriately handle CI/CD - Maybe, maybe, look to moving to a Python static site generator, since I am doing most of my own work in Python. Perhaps GoLang if there is such a thing in that sphere. We will see.</p> <p>The first four posts on this page are pieces that I was able to quickly move over from the previous blogging platform over to the markdown flavor. This explains the timing of this post with having older posts on the blog.</p>","tags":["blog"]},{"location":"getting-started-with-the-blog/#evaluation","title":"Evaluation","text":"<p>I originally started with Gitlab, knowing that they had a good exposure of the CI/CD process and had it all integrated. I didn't want to try to integrate a different solution into the Github arena if possible. I tried originally forking the <code>jekyll</code> format over, but this didn't get going well. The CSS never quite made it into the page, so I was frustrated and decided to try Github.</p> <p>Over at Github, I could start to get the content, and as soon as I would push a new commit to master I would get an email a few minutes later saying that the build had failed. No other helpful information in the email, just that the Jekyll build had failed. After about four of these messages I decided to try just doing Jekyll on a new VM host of my own and see if I could get it working there.</p>","tags":["blog"]},{"location":"getting-started-with-the-blog/#decision-making","title":"Decision Making","text":"<p>I started with a fresh Jekyll page, and immediately things came right up. Finally some progress! Eventually, I found my way to the Jekyll Quickstart/Docs, following that tutorial and looking at other blogging pages on various Github/Gitlab pages I figured out the structure a little bit more.</p> <p>I then cloned the repository from Gitlab pages to my local instance and gave it a run. The default page showed immediately. From there I went ahead and copied my posts into the <code>_posts</code> directory and the content was right there. I decided to push the content to my Gitlab pages and low and behold it worked there as well! I was in business.</p>","tags":["blog"]},{"location":"getting-started-with-the-blog/#ssl-and-custom-domain","title":"SSL and Custom Domain","text":"<p>There are lots of articles all over the web on how to do a custom domain on Gitlab pages. So I'm not going to provide details, but high level:</p> <ul> <li>Make sure your repository name is <code>&lt;userid&gt;.gitlab.io</code></li> <li>Point your DNS records at Gitlab and away you go</li> </ul> <p>SSL was a little more tricky. I had found an \"official\" link from Gitlab, but that was not helpful to me. I couldn't get the certificate information to show up with Let's Encrypt. I eventually came across this blog on how to do certificates with Let's Encrypt via Gitlab pages. Once I completed the work done described there, I was able to get my certificates from Let's Encrypt, and I now have a SSL blog.</p>","tags":["blog"]},{"location":"getting-started-with-the-blog/#final-decision-gitlab-pages","title":"Final Decision: Gitlab Pages","text":"<p>At that point, I finally had a page out on the web and am at the point that I am now. I'm going to continue to evolve what the pages will look like. I've still got some more to learn about how to get Jekyll in the right setup. Maybe a trip to using Python Pelican to build a flavor. But for now, I have a place where I can post material, and so far with writing this post, things are much quicker.</p>","tags":["blog"]},{"location":"keeping-up-on-tech/","title":"Keeping Up on Tech","text":"<p>Life always gets busy. That is one thing that you always hear about. I am definitely in that boat as well that things are getting busy. With this in mind, I have never felt better about my ability to learn new things in the field. I do a few things that I feel are probably unique that I should share. </p> <p>The first opportunity that has never been more capable of doing is to listen to podcasts. The commuting time to/from work is a tremendous opportunity. I leverage three podcasts as my primary learning mechanism these days. </p>","tags":["learning","productivity"]},{"location":"keeping-up-on-tech/#podcasts","title":"Podcasts","text":"","tags":["learning","productivity"]},{"location":"keeping-up-on-tech/#packet-pushers","title":"Packet Pushers","text":"<p>The first go to podcast is Packet Pushers. They have several different channels available, and my preference is the Fat Pipe podcast feed. This gets you a few different shows, that includes The Network Break, Weekly Show, and Datanaughts. There are more shows being added all of the time based on a criteria.</p> <p>There is a membership system available called Ignition at their site. They also have a paid opportunity to help contribute back to the hosts and other premium content. Ideally I will be budgeting for multiple memberships, this being one of them. I just have not done this quite yet. This comes in at $99/year</p>","tags":["learning","productivity"]},{"location":"keeping-up-on-tech/#the-network-collective","title":"The Network Collective","text":"<p>Network Collective is a recent addition and I look forward to the podcasts when it shows up in the feed. So far there are a couple of shows available. The podcast has a several list of hosts including  Jordan Martin, Russ White, and Eyvonne Sharp. They have quite the variety of topics that constantly peak my interest. The down side of this is the annual cost for the premium access IMO, sitting at $250/year.</p>","tags":["learning","productivity"]},{"location":"keeping-up-on-tech/#history-of-networking","title":"History of Networking","text":"<p>Perhaps one of the premier shows of The Network Collective podcast network is the History of Networking series. The show brings on notable guests in the history of the development of the networking field as we know it today. This is a must check out regardless if there are other podcast series on this network.</p>","tags":["learning","productivity"]},{"location":"keeping-up-on-tech/#zigbits-network-design-podcast","title":"Zigbits Network Design Podcast","text":"<p>This is a new one to me as of 2018. Quickly a favorite of mine as well. This podcast emphesis is on network design, and brings a whole differnet view than the Packet Pushers or The Network Collective. From episodes including CI/CD, Cisco ISE, or DevOps, this is a deeper podcast and much deeper topics, which is a must checkout.</p>","tags":["learning","productivity"]},{"location":"keeping-up-on-tech/#slack-channels","title":"Slack Channels","text":"<p>This is one of my other favorites and perhaps the newest methodology. It is really a new take on an old format, IRC. Slack has created a place that using Markdown (sensing a theme) is prevelant to create a community of users. The first channel that I joined in the Network Automation spirit was that of NetworkToCode. You have to put a request to join, and once in you are welcomed kindly. If getting into using Slack, or any of the modern chat applications, I strongly recommend learning what Markdown can do. </p> <p>The first two podcast networks that were mentioned of Packet Pushers and  The Network Collective each have their own slack channels. Packet Pushers slack channel is free to join while The Network Collective has Slack as one of the membership benefits that is included in the $250 annual membership.</p>","tags":["learning","productivity"]},{"location":"keeping-up-on-tech/#twitter","title":"Twitter","text":"<p>Social media is the thing that seems to be either very hot and very cold. A lot of good inspirational group of engineers in the field of Network Automation find themselves on Twitter. Yes, there is a negative stigma around social media, but I do recommed at least following a good group of people that are influential in Network Automation arena. I follow the hosts of the podcast networks listed above, a group of individuals putting out good work within Python and Ansible networking.</p>","tags":["learning","productivity"]},{"location":"keeping-up-on-tech/#home-lab","title":"Home Lab","text":"<p>The last, and maybe the least of those mentioned, is a home lab. I have gone back and forth from having actual server hardware running, to removing the hardware. I've had network lab gear up and running as well, and that is for the most part retired. I am now looking at a home lab that is having virtualized platforms running on a desktop and a few Raspberry Pi pieces hanging around.</p> <p>The next iteration of a lab for myself will likely include a cloud environment as well. I'm inspired by the podcasts that Nicholas Russo (https://twitter.com/nickrusso42518) has put together within the Zigbits Network Design Podcast. He recently talked about his home lab not being powered up for a couple of years as he moved to AWS for his development work. I'll encourage you to listen to the podcast linked on the Podcasts and Links page.</p>","tags":["learning","productivity"]},{"location":"microsegmentation/","title":"Micro Segmentation vs Segmentation","text":"<p>In a recent podcast there was some discussion that it sounded like the term Micro Segmentation was being used where it was really traditional segmentation. So I thought I would put out a few thoughts on this front.</p>","tags":["network design"]},{"location":"microsegmentation/#what-is-segmnentation-in-networking","title":"What is Segmnentation in Networking","text":"<p>Segmentation is a methodology to create separatet zones of sorts of various traffic types. Various places you may want to do this is within a campus environment to separate students from faculty, or engineering from finance. The list of examples goes on and on. Go to a basic reading of VLANs and you will get the idea of what segmentation is. Once you have VLANs, really segmentation then builds upon this and allows policy to be applied. This policy can be whether or not hosts should be able to talk to each other, or various traffic treatments (QoS). This is something that is well covered already and I do not wish to cover more.</p>","tags":["network design"]},{"location":"microsegmentation/#what-is-micro-segmentation","title":"What is Micro Segmentation?","text":"<p>So the newer term is Micro Segmentation. This is exactly as what was covered before but doing it at an even more detailed level. This is getting into being able to apply policy to individual hosts within a segment, creating a micro-segmentation effort. </p>","tags":["network design"]},{"location":"microsegmentation/#in-practice","title":"In Practice","text":"Source Host Destination Host Able to apply policy - Segmentation Able to apply policy - Micro Segmentation Host A Host B No Yes Host A Host C No Yes Host A Host D Yes, at that L3 device/firewall Yes Host B Host C No Yes Host B Host D Yes, at that L3 device/firewall Yes Host C Host D Yes, at that L3 device/firewall Yes <p>Given the diagram above of two network segments with a firewall in between, we will cover what you can enforce policy with and what you can't. In this drawing this is a firewall that separates the network segments. However, you could also have this be a L3 device, such as a L3 switch, or a router where there are ACLs in place.</p> <p>With traditional segmentation, you can apply policy only between the two segments. Host D will have policy applied to try to talk with Host A, B, or C. Host A, B, and C will be able to communicate between each other without any policy being applied.</p>","tags":["network design"]},{"location":"microsegmentation/#micro-segmentation","title":"Micro Segmentation","text":"<p>In that same diagram with Micro Segmentation practices applied, not only can you have policy applied between Host D and the trio of hosts (A, B, C), but you can also apply policy between hosts A, B, C on the same network segment. So if you wanted to lock down so that Host A can only talk out of the segment, but not to any other hosts within the same segment, this is possible. This is the major difference between standard Segmentation and Micro Segmentation, the ability to prevent east-west traffic between hosts in the same network segments.</p>","tags":["network design"]},{"location":"microsegmentation/#how-to","title":"How to?","text":"<p>So how is this done you may ask? There may be more ways about this, but the ones that I'm aware of are:</p> <ul> <li>Controller Based Wireless (Preventing host to host communication)</li> <li>Cisco ISE</li> <li>Cisco ACI</li> <li>Private VLANs (Administratively burdensome)</li> <li>NSX</li> <li>Host based firewalls (More burden) (Maybe Illumio can help here? - Packet Pushers BIB Illumio)</li> </ul>","tags":["network design"]},{"location":"microsegmentation/#does-this-work","title":"Does this work?","text":"<p>Absolutely. I have done host to host policy enforcement within a VLAN using Cisco ISE in a campus LAN environment. There was a requirement to prevent one host to talk to any other host within the same network segment. Cisco ISE definitely delivered on this, within the policy matrix by defining policy that a tag (say 6) could not talk to that same tag (6). If you have some unmanaged switches in the wire that don't have the hosts wired up directly to a switchport, this is no longer feasible.</p>","tags":["network design"]},{"location":"microsegmentation/#summary","title":"Summary","text":"<p>So in summary, hopefully this helps clear up some ideas about what Segmentation and Micro Segmentation are in the industry. This is my take on what the difference between Segmentation and Micro Segmentation and that there is a difference. Not just segmenting at a L3 boundry.</p>","tags":["network design"]},{"location":"microsegmentation/#discussion","title":"Discussion","text":"<p>I have not figured out yet how to add disqus to the blog at this point. Hopefully I can get that added soon.</p>","tags":["network design"]},{"location":"microsegmentation/#notes","title":"Notes","text":"<p>Drawing completed at draw.io.</p>","tags":["network design"]},{"location":"discontiguous_masks/","title":"Discontiguous Masks","text":"<p>Discontiguous masks are something that is going to be somewhat historic within the network design toolbox. It is basically a methodology of looking at particular bits of a network/host definition. The big thing to recall is that as a packet crosses a network device it does so within a packet. The packet is nothing more than a stream of bits. Within the packet header there are bits that define the source network address and the destination network address. This is where discontiguous masks come into play. With a system that can leverage discontiguous masks, you can access information about any part of the network bits, not just starting reading and then stopping (or vice versa) when you look at a bit boundry masking only.</p> <p>This is a originally posted from my previous blog here  Previous Post. I plan to give this a complete re-write when I have the opportunity.</p>","tags":["network design","segmentation"]},{"location":"discontiguous_masks/#original-post","title":"Original post","text":"<p>Simply put, discontiguous masks are those that are represented by a potentially alternating sets of 1s and 0s within a mask (subnet/standard mask or wild card). This does not really apply to a subnet mask as you have a network portion of the address, and then the host portion, so everything is contiguous. The primary place that you will see this is in Access Control Lists or ACLs (for more info on ACLs, here is a quick link to a Wikipedia article. This may be a topic later on within this blog http://en.wikipedia.org/wiki/Standard_Access_Control_List).  </p> <p>The first two use cases that these come in handy for are for Quality of Service ACLs and Security/Firewall ACLs. There are many more uses out there, but these are the two primary ones that I have come across. Feel free to leave comments, and I will dive into it, modifying this post for this.  </p> <p>My first tip for understanding this concept is that you need to think the way that a router (or other network device) thinks. Everything that passes through a router or switch is passing through the device in a logical method. The router and switch are looking at binary 1s and 0s as they cross the wire.  </p> <p>Everything for this discussion revolves around binary addressing. Remember that currently in the IPv4 space that there are 32 bits to a network address. That is the IP address 192.168.0.1 can be translated into binary bits of <code>1100 0000 . 1010 1000 . 0000 0000 . 0000 0001</code>.</p> <p>Discontiguous masks are as they state, a mask that can be discontiguous. Many devices in the industry have support for what is a contiguous mask. This does not have do with wild card or subnet masking yet. It just matters how the devices check to see if something matches. In a standard mask or subnet mask, the mask that you define may look something like 255.255.255.0 (11111111.11111111.11111111.00000000). In this case you will be doing a logical \"AND\" operation where the bits that you care about are represented by a \"1\".  The bits that are represented by a \"0\" are those that you do not care about. In the Cisco world of wild card masking, it is the inverse. Most of my examples will be in wild card notation, since this is where it is most prevalent in my world.</p> <p>A discontiguous mask does not follow this. First, let's take the previous example of the mask 255.255.255.0 and turn it into a wild card mask, so that we can follow along in the same fashion moving forward. This then turns into a 0.0.0.255 mask. Simply changing the important bits, or the bits that you care about from a \"1\" to a \"0\" and vice versa. Binary representation is <code>00000000.00000000.00000000.11111111</code>. So now, the discontiguous part. Let's say you run a network where you have 10 sites. They are all the same IP address wise, except that you change the third octet to represent your site. Let's use addressing 192.168.0.0 - 192.168.255.255 to represent this. Site A has the IP addresses 192.168.1.0/24 (contiguous masking) and site B has the IP addresses  192.168.2.0/24. You assign a server the address of X.X.X.2 at each site. Now you want to write an ACL that will match that at each site.</p> <p>The process that I would follow, is we need to figure out what is important. As we look at the 10 site  addresses (we will just use two, but we will see the commonality) that we care about.</p> IP address Binary 192.168.1.2 <code>1100 0000 . 1010 1000 . 0000 0001 . 0000 0010</code> 192.168.2.2 <code>1100 0000 . 1010 1000 . 0000 0010 . 0000 0010</code> 192.168.3.2 <code>1100 0000 . 1010 1000 . 0000 0011 . 0000 0010</code> 192.168.4.2 <code>1100 0000 . 1010 1000 . 0000 0100 . 0000 0010</code> 192.168.5.2 <code>1100 0000 . 1010 1000 . 0000 0101 . 0000 0010</code> 192.168.6.2 <code>1100 0000 . 1010 1000 . 0000 0110 . 0000 0010</code> 192.168.7.2 <code>1100 0000 . 1010 1000 . 0000 0111 . 0000 0010</code> 192.168.8.2 <code>1100 0000 . 1010 1000 . 0000 1000 . 0000 0010</code> 192.168.9.2 <code>1100 0000 . 1010 1000 . 0000 1001 . 0000 0010</code> 192.168.10.2 <code>1100 0000 . 1010 1000 . 0000 1010 . 0000 0010</code> <p>You notice a little bit of a pattern develop. So if I were to look at a wild card mask here, let's look at each individual octet one at a time. I encourage you to do so when trying to create your own. We know that in this world that all of the addresses will start with 192.168. on all of the sites. This makes the first two octets very easy to write the wild card mask. You care about all of the bits in the first two octets. So they are simply \"0\". So far we have 0.0.?.? for a mask.  </p> <p>The third octet is where it becomes more difficult. As we take a look at the binary information above, you will notice that there isn't a complete pattern in that octet (with only 10 sites, there is a little bit of a pattern, but if this were to get expanded out to say 100 or 200 sites with each the same addressing, then it becomes more difficult. We will say for our discussion, that we don't care what the third octet is. It can be 2, it could be 254 or 140. For simplicity sake of discontiguous explanation, this is the case, we don't care about any of the bits. So now we would have the mask 0.0.255.?</p> <p>The last octet is once again easy in this case. We care about every bit. So it is \"0\" again. Our complete wild card mask would be 0.0.255.0 in this case. This is where discontiguous masks are very powerful. Now, let's say that we add a second server at each site. The easiest thing to do from a network perspective is set the IP address to .3 at the site. These two servers do the same functionality in a high availability design, so you want all traffic to be treated as equal. You simply change the very last bit in the octet to be a 1 (in that you don't care if it is a 0 or a 1), and you have your new wild card mask of 0.0.255.1 that will match traffic on two IP addresses at a site instead of just one.</p> <p>In a contiguous state, there would need to be a clear \"border\" between the 0s and 1s, that you can't go back and forth on. So <code>0000000000000001111111111111111</code> is a contiguous mask. The <code>000000000000000011111111000000000</code> mask would not play well with hardware that does not support it.</p> <p>Remember, think about how a router thinks in order to understand clearly what is happening.</p> <p>Happy masking! </p>","tags":["network design","segmentation"]},{"location":"ansible-output-work/","title":"Ansible - Working with command output","text":"<p>You have decided to move forward with using/trying Ansible. You can now connect to a device and get a green success that you get a hello world like command such as <code>show hostname</code> or <code>show inventory</code> and get the GREEN success on Ansible. Now what. You may want to see the output of the command that you sent and got information back. This is your post on getting started.</p> <p>This is the process that I typically go through when developing a playbook for use. Let's say this is a playbook that you wish to just get show information out of the device, say investigating if there are any configurations that are applied that would be part of a CVE bug, or just operational status.</p> <p>During this post I will relate the Ansible data structures/formats to that of Python. So the terms will be dictionary (hashes) and lists (lists).</p> <p>Playbook Design Process</p> <ol> <li>Make sure that I can connect to the devices with a simple show command</li> <li>Get necessary show output</li> <li>Debug the outputs of the show commands</li> <li>Set facts or take more action based on other outputs</li> </ol> <p>This can get extremely elaborate. I am going to attempt to keep this about the debug commands along the way.</p>","tags":["ansible","cisco"]},{"location":"ansible-output-work/#lab-setup","title":"Lab Setup","text":"<p>First I will just give a quick diagram of the lab environment. This is simulated with EVE-NG. I will be accessing the devices via a management network to show various things.</p> <p></p> <p>I am going to connect to just Cisco IOS and Cisco ASA virtual images for this. That can extend as well to any other platform using the standard Ansible 2.9 network modules.</p>","tags":["ansible","cisco"]},{"location":"ansible-output-work/#playbook-play-and-task","title":"Playbook Play and Task","text":"<p>First, the initial connection and a simple show command.</p>","tags":["ansible","cisco"]},{"location":"ansible-output-work/#playbook-a","title":"Playbook A","text":"","tags":["ansible","cisco"]},{"location":"ansible-output-work/#task-1-a1","title":"Task 1 (A1)","text":"<p>The play in the playbook is going to log in and execute the following two tasks:</p> <ol> <li>Task 1: Issue command <code>show run interface loopback 0</code> -&gt; Save to a variable named show_commands</li> <li>Task 2: Debug the output of the variable show_commands, which is stored for each device that is connected to.</li> </ol> <pre><code>---\n# yamllint disable rule:truthy\n- name: Test command outputs\n  connection: network_cli\n  hosts: cisco_routers\n  gather_facts: no\n  become: yes\n  become_method: enable\n\n  tasks:\n    - name: IOS &gt;&gt; Show commands\n      ios_command:\n        commands:\n          - show run interface loopback 0\n      register: show_commands\n\n    - name: SYS &gt;&gt; DEBUG OUTPUT\n      debug:\n        msg: \"{{ show_commands }}\"\n</code></pre> <p>Here is the output from connecting to a single device:</p> <pre><code>PLAY [Test command outputs] ****************************************************\n\nTASK [IOS &gt;&gt; Show commands] ****************************************************\nok: [rtr01]\n\nTASK [SYS &gt;&gt; DEBUG OUTPUT] *****************************************************\nok: [rtr01] =&gt; {\n    \"msg\": {\n        \"changed\": false,\n        \"failed\": false,\n        \"stdout\": [\n            \"Building configuration...\\n\\nCurrent configuration : 68 bytes\\n!\\ni\nnterface Loopback0\\n ip address 10.100.100.1 255.255.255.255\\nend\"\n        ],\n        \"stdout_lines\": [\n            [\n                \"Building configuration...\",\n                \"\",\n                \"Current configuration : 68 bytes\",\n                \"!\",\n                \"interface Loopback0\",\n                \" ip address 10.100.100.1 255.255.255.255\",\n                \"end\"\n            ]\n        ]\n    }\n}\n\nPLAY RECAP *********************************************************************\nrtr01                      : ok=2    changed=0    unreachable=0    failed=0\n</code></pre> <p>As we look at the output, the task itself creates the part <code>\"msg\":</code> This itself shows the output dictionary. This has several <code>keys:</code> and <code>values</code>. Breaking down each of the keys and values in the output:</p> <ol> <li><code>changed</code>: This is a boolean field where you will see if the variable stored (output of a task) had made a change. Most <code>*_command</code> outputs will not make changes to the devices.</li> <li><code>failed</code>: Did the task have a failed return code or not</li> <li><code>stdout</code>: The standard output, including escape characters, of the output, this output is a list</li> <li><code>stdout_lines</code>: This is a more human readable output format, that puts the line breaks in. This output is in the format of a list</li> </ol> <p>These outputs are in the forms of lists, so if we want to get access to the actual string of the command <code>show run interface loopback 0</code>? We need to access the <code>show_commands['stdout'][0]</code>. This will be shown with the updated playbook (a second debug has been added):</p> <pre><code>---\n# yamllint disable rule:truthy\n- name: Test command outputs\n  connection: network_cli\n  hosts: cisco_routers\n  gather_facts: no\n  become: yes\n  become_method: enable\n  tasks:\n    - name: IOS &gt;&gt; Show commands\n      ios_command:\n        commands:\n          - show run interface loopback 0\n      register: show_commands\n\n    - name: SYS &gt;&gt; DEBUG OUTPUT\n      debug:\n        msg: \"{{ show_commands }}\"\n\n    - name: SYS &gt;&gt; DEBUG to get to the actual output\n      debug:\n        msg: \"{{ show_commands['stdout'][0] }}\"\n</code></pre> <p>This now yields this output:</p> <pre><code>PLAY [Test of connectivity] ****************************************************\n\nTASK [IOS &gt;&gt; Show commands] ****************************************************\nok: [rtr01]\n\nTASK [SYS &gt;&gt; DEBUG OUTPUT] *****************************************************\nok: [rtr01] =&gt; {\n    \"msg\": {\n        \"changed\": false,\n        \"failed\": false,\n        \"stdout\": [\n            \"Building configuration...\\n\\nCurrent configuration : 68 bytes\\n!\\ni\nnterface Loopback0\\n ip address 10.100.100.1 255.255.255.255\\nend\"\n        ],\n        \"stdout_lines\": [\n            [\n                \"Building configuration...\",\n                \"\",\n                \"Current configuration : 68 bytes\",\n                \"!\",\n                \"interface Loopback0\",\n                \" ip address 10.100.100.1 255.255.255.255\",\n                \"end\"\n            ]\n        ]\n    }\n}\n\nTASK [SYS &gt;&gt; DEBUG to get to the actual output] ********************************\nok: [rtr01] =&gt; {\n    \"msg\": \"Building configuration...\\n\\nCurrent configuration : 68 bytes\\n!\\nin\nterface Loopback0\\n ip address 10.100.100.1 255.255.255.255\\nend\"\n}\n\nPLAY RECAP *********************************************************************\nrtr01                      : ok=3    changed=0    unreachable=0    failed=0\n</code></pre>","tags":["ansible","cisco"]},{"location":"ansible-output-work/#why-lists","title":"Why Lists?","text":"<p>So why are <code>stdout</code> and <code>stdout_lines</code> are in the type of lists? This goes back to the section of the Ansible module <code>ios_commands</code> where <code>commands</code> is fed a list. This means that you can send multiple commands in a single task. Updating the playbook to be this:</p> <pre><code>---\n# yamllint disable rule:truthy\n- name: Test command outputs\n  connection: network_cli\n  hosts: cisco_routers\n  gather_facts: no\n  become: yes\n  become_method: enable\n\n  tasks:\n    - name: IOS &gt;&gt; Show commands\n      ios_command:\n        commands:\n          - show run interface loopback 0\n          - ping 8.8.8.8 source loopback 0 repeat 20 size 1500\n      register: show_commands\n\n    - name: SYS &gt;&gt; DEBUG OUTPUT\n      debug:\n        msg: \"{{ show_commands }}\"\n\n    - name: SYS &gt;&gt; DEBUG to get to the actual output for 1st command run\n      debug:\n        msg: \"{{ show_commands['stdout'][0] }}\"\n\n    - name: SYS &gt;&gt; DEBUG to get the ping results\n      debug:\n        msg: \"{{ show_commands['stdout'][1] }}\"\n</code></pre> <p>Which now yields:</p> <pre><code>PLAY [Test command outputs] ****************************************************\n\nTASK [IOS &gt;&gt; Show commands] ****************************************************\nok: [rtr01]\n\nTASK [SYS &gt;&gt; DEBUG OUTPUT] *****************************************************\n\nok: [rtr01] =&gt; {\n    \"msg\": {\n        \"changed\": false,\n        \"failed\": false,\n        \"stdout\": [\n            \"Building configuration...\\n\\nCurrent configuration : 68 bytes\\n!\\ninterface Loopback0\\n ip address 10.100.100.1 255.255.255.255\\nend\",\n            \"Type escape sequence to abort.\\nSending 20, 1500-byte ICMP Echos to 8.8.8.8, timeout is 2 seconds:\\nPacket sent with a source address of 10.100.100.1 \\n!!!!!!!!!!!!!!!!!!!!\\nSuccess rate is 100 percent (20/20), round-trip min/avg/max = 86/108/213 ms\",\n            \"Type escape sequence to abort.\\nSending 20, 1500-byte ICMP Echos to 1.1.1.1, timeout is 2 seconds:\\nPacket sent with a source address of 10.100.100.1 \\n!!!!!!!!!!!!!!!!!!!!\\nSuccess rate is 100 percent (20/20), round-trip min/avg/max = 82/108/217 ms\"\n        ],\n        \"stdout_lines\": [\n            [\n                \"Building configuration...\",\n                \"\",\n                \"Current configuration : 68 bytes\",\n                \"!\",\n                \"interface Loopback0\",\n                \" ip address 10.100.100.1 255.255.255.255\",\n                \"end\"\n            ],\n            [\n                \"Type escape sequence to abort.\",\n                \"Sending 20, 1500-byte ICMP Echos to 8.8.8.8, timeout is 2 seconds:\",\n                \"Packet sent with a source address of 10.100.100.1 \",\n                \"!!!!!!!!!!!!!!!!!!!!\",\n                \"Success rate is 100 percent (20/20), round-trip min/avg/max = 86/108/213 ms\"\n            ]\n        ]\n    }\n}\n\nTASK [SYS &gt;&gt; DEBUG to get to the actual output for 1st command run] ***\nok: [rtr01] =&gt; {\n    \"msg\": \"Building configuration...\\n\\nCurrent configuration : 68 bytes\\n!\\nin\nterface Loopback0\\n ip address 10.100.100.1 255.255.255.255\\nend\"\n}\n\nTASK [SYS &gt;&gt; DEBUG to get the ping results] ************************************\nok: [rtr01] =&gt; {\n    \"msg\": \"Type escape sequence to abort.\\nSending 20, 1500-byte ICMP Echos to\n8.8.8.8, timeout is 2 seconds:\\nPacket sent with a source address of 10.100.100.\n1 \\n!!!!!!!!!!!!!!!!!!!!\\nSuccess rate is 100 percent (20/20), round-trip min/av\ng/max = 41/108/260 ms\"\n}\n\nPLAY RECAP *********************************************************************\nrtr01                      : ok=4    changed=0    unreachable=0    failed=0\n</code></pre> <p>We can now see how you may get at particular command outputs, while running multiple commands during one task on the device. From what I can tell, these commands are run sequentially, and not with separate SSH sessions, as during my testing I only ever saw a single SSH session on the device.</p>","tags":["ansible","cisco"]},{"location":"ansible-output-work/#accessing-variables-from-other-tasks","title":"Accessing variables from other tasks","text":"<p>This is something that I stumbled upon at some point that was helpful in multiple play playbooks. You have multiple plays in a playbook right? So how do you get at information from a previous task? You access it via the keyword variable <code>hostvars</code>. I've added a second play to the previous playbook. I also added another DNS provider to test my pings to in order to show this.</p> <pre><code>---\n# yamllint disable rule:truthy\n# yamllint disable rule:line-length\n- name: Test command outputs\n  connection: network_cli\n  hosts: cisco_routers\n  gather_facts: no\n  become: yes\n  become_method: enable\n\n  tasks:\n    - name: IOS &gt;&gt; Show commands\n      ios_command:\n        commands:\n          - show run interface loopback 0\n          - ping 8.8.8.8 source loopback 0 repeat 20 size 1500\n          - ping 1.1.1.1 source loopback 0 repeat 20 size 1500\n      register: show_commands\n\n    - name: SYS &gt;&gt; DEBUG OUTPUT\n      debug:\n        msg: \"{{ show_commands }}\"\n\n    - name: SYS &gt;&gt; DEBUG to get to the actual output for 1st command run\n      debug:\n        msg: \"{{ show_commands['stdout'][0] }}\"\n\n    - name: SYS &gt;&gt; DEBUG to get the ping results\n      debug:\n        msg: \"{{ show_commands['stdout'][1] }}\"\n\n- name: See output from previous play\n  connection: local\n  hosts: local\n  gather_facts: no\n\n  tasks:\n    - name: SYS &gt;&gt; Debug variable from previous task\n      debug:\n        msg: \"{{ hostvars['rtr01']['show_commands']['stdout'][2] }}\"\n</code></pre> <p>This now has the output of the ping test to 1.1.1.1 in the output.</p> <pre><code>PLAY [Test command outputs] ****************************************************\n\n~~~~ PLAY OUTPUT TRUNCATED FOR BREVITY ~~~~~\n\nPLAY [See output from previous play] *******************************************\n\nTASK [SYS &gt;&gt; Debug variable from previous task] ********************************\nok: [localhost] =&gt; {\n    \"msg\": \"Type escape sequence to abort.\\nSending 20, 1500-byte ICMP Echos to\n1.1.1.1, timeout is 2 seconds:\\nPacket sent with a source address of 10.100.100.\n1 \\n!!!!!!!!!!!!!!!!!!!!\\nSuccess rate is 100 percent (20/20), round-trip min/av\ng/max = 63/108/236 ms\"\n}\n\nPLAY RECAP *********************************************************************\nlocalhost                  : ok=1    changed=0    unreachable=0    failed=0\nrtr01                      : ok=4    changed=0    unreachable=0    failed=0\n</code></pre>","tags":["ansible","cisco"]},{"location":"ansible-output-work/#looping-over-the-output","title":"Looping over the output","text":"<p>You can also loop over the output of the commands as well. I added in some more lines to the debug that will show how you can loop over all of the commands you issued. In a future post we will discuss on how to debug through using <code>with_items</code>.</p> <pre><code>    - name: SYS &gt;&gt; DEBUG to see loop\n      debug:\n        msg: \"{{ item }}\"\n      with_items: \"{{ show_commands['stdout'] }}\"\n</code></pre> <p>We want to get to each of the <code>stdout</code> outputs. I've added <code>with_items</code> and we have a new variable of <code>item</code> that we reference in the message. We now get the following output related to that task:</p> <pre><code>TASK [SYS &gt;&gt; DEBUG to see loop] ************************************************\nok: [rtr01] =&gt; (item=Building configuration...\n\nCurrent configuration : 68 bytes\n!\ninterface Loopback0\n ip address 10.100.100.1 255.255.255.255\nend) =&gt; {\n    \"msg\": \"Building configuration...\\n\\nCurrent configuration : 68 bytes\\n!\\nin\nterface Loopback0\\n ip address 10.100.100.1 255.255.255.255\\nend\"\n}\nok: [rtr01] =&gt; (item=Type escape sequence to abort.\nSending 20, 1500-byte ICMP Echos to 8.8.8.8, timeout is 2 seconds:\nPacket sent with a source address of 10.100.100.1\n!!!!!!!!!!!!!!!!!!!!\nSuccess rate is 100 percent (20/20), round-trip min/avg/max = 86/108/213 ms) =&gt;\n{\n    \"msg\": \"Type escape sequence to abort.\\nSending 20, 1500-byte ICMP Echos to\n 8.8.8.8, timeout is 2 seconds:\\nPacket sent with a source address of 10.100.10\n0.1 \\n!!!!!!!!!!!!!!!!!!!!\\nSuccess rate is 100 percent (20/20), round-trip min\n/avg/max = 86/108/213 ms\"\n}\nok: [rtr01] =&gt; (item=Type escape sequence to abort.\nSending 20, 1500-byte ICMP Echos to 1.1.1.1, timeout is 2 seconds:\nPacket sent with a source address of 10.100.100.1\n!!!!!!!!!!!!!!!!!!!!\nSuccess rate is 100 percent (20/20), round-trip min/avg/max = 82/108/217 ms) =&gt;\n{\n    \"msg\": \"Type escape sequence to abort.\\nSending 20, 1500-byte ICMP Echos to\n 1.1.1.1, timeout is 2 seconds:\\nPacket sent with a source address of 10.100.10\n0.1 \\n!!!!!!!!!!!!!!!!!!!!\\nSuccess rate is 100 percent (20/20), round-trip min\n/avg/max = 82/108/217 ms\"\n}\n</code></pre>","tags":["ansible","cisco"]},{"location":"ansible-output-work/#final-run","title":"Final Run","text":"<p>Here is the final playbook</p> <pre><code>---\n# yamllint disable rule:truthy\n# yamllint disable rule:line-length\n- name: Test command outputs\n  connection: network_cli\n  hosts: cisco_routers\n  gather_facts: no\n  become: yes\n  become_method: enable\n\n  tasks:\n    - name: IOS &gt;&gt; Show commands\n      ios_command:\n        commands:\n          - show run interface loopback 0\n          - ping 8.8.8.8 source loopback 0 repeat 20 size 1500\n          - ping 1.1.1.1 source loopback 0 repeat 20 size 1500\n      register: show_commands\n\n    - name: SYS &gt;&gt; DEBUG OUTPUT\n      debug:\n        msg: \"{{ show_commands }}\"\n\n    - name: SYS &gt;&gt; DEBUG to get to the actual output for 1st command run\n      debug:\n        msg: \"{{ show_commands['stdout'][0] }}\"\n\n    - name: SYS &gt;&gt; DEBUG to get the ping results\n      debug:\n        msg: \"{{ show_commands['stdout'][1] }}\"\n\n    - name: SYS &gt;&gt; DEBUG to see loop\n      debug:\n        msg: \"{{ item }}\"\n      with_items: \"{{ show_commands['stdout'] }}\"\n\n- name: See output from previous play\n  connection: local\n  hosts: local\n  gather_facts: no\n\n  tasks:\n    - name: SYS &gt;&gt; Debug variable from previous task\n      debug:\n        msg: \"{{ hostvars['rtr01']['show_commands']['stdout'][2] }}\"\n</code></pre> <p>Final Run Output</p> <pre><code>PLAY [Test command outputs] ****************************************************\n\nTASK [IOS &gt;&gt; Show commands] ****************************************************\nok: [rtr01]\n\nTASK [SYS &gt;&gt; DEBUG OUTPUT] *****************************************************\nok: [rtr01] =&gt; {\n    \"msg\": {\n        \"changed\": false,\n        \"failed\": false,\n        \"stdout\": [\n            \"Building configuration...\\n\\nCurrent configuration : 68 bytes\\n!\\ni\nnterface Loopback0\\n ip address 10.100.100.1 255.255.255.255\\nend\",\n            \"Type escape sequence to abort.\\nSending 20, 1500-byte ICMP Echos to\n 8.8.8.8, timeout is 2 seconds:\\nPacket sent with a source address of 10.100.100\n.1 \\n!!!!!!!!!!!!!!!!!!!!\\nSuccess rate is 100 percent (20/20), round-trip min/a\nvg/max = 86/108/213 ms\",\n            \"Type escape sequence to abort.\\nSending 20, 1500-byte ICMP Echos to\n 1.1.1.1, timeout is 2 seconds:\\nPacket sent with a source address of 10.100.100\n.1 \\n!!!!!!!!!!!!!!!!!!!!\\nSuccess rate is 100 percent (20/20), round-trip min/a\nvg/max = 82/108/217 ms\"\n        ],\n        \"stdout_lines\": [\n            [\n                \"Building configuration...\",\n                \"\",\n                \"Current configuration : 68 bytes\",\n                \"!\",\n                \"interface Loopback0\",\n                \" ip address 10.100.100.1 255.255.255.255\",\n                \"end\"\n            ],\n            [\n                \"Type escape sequence to abort.\",\n                \"Sending 20, 1500-byte ICMP Echos to 8.8.8.8, timeout is 2 secon\nds:\",\n                \"Packet sent with a source address of 10.100.100.1 \",\n                \"!!!!!!!!!!!!!!!!!!!!\",\n                \"Success rate is 100 percent (20/20), round-trip min/avg/max = 8\n6/108/213 ms\"\n            ],\n            [\n                \"Type escape sequence to abort.\",\n                \"Sending 20, 1500-byte ICMP Echos to 1.1.1.1, timeout is 2 secon\nds:\",\n                \"Packet sent with a source address of 10.100.100.1 \",\n                \"!!!!!!!!!!!!!!!!!!!!\",\n                \"Success rate is 100 percent (20/20), round-trip min/avg/max = 8\n2/108/217 ms\"\n            ]\n        ]\n    }\n}\n\nTASK [SYS &gt;&gt; DEBUG to get to the actual output for first command of show run] ***\nok: [rtr01] =&gt; {\n    \"msg\": \"Building configuration...\\n\\nCurrent configuration : 68 bytes\\n!\\nint\nerface Loopback0\\n ip address 10.100.100.1 255.255.255.255\\nend\"\n}\n\nTASK [SYS &gt;&gt; DEBUG to get the ping results] ************************************\nok: [rtr01] =&gt; {\n    \"msg\": \"Type escape sequence to abort.\\nSending 20, 1500-byte ICMP Echos to\n8.8.8.8, timeout is 2 seconds:\\nPacket sent with a source address of 10.100.100\n.1 \\n!!!!!!!!!!!!!!!!!!!!\\nSuccess rate is 100 percent (20/20), round-trip min/\navg/max = 86/108/213 ms\"\n}\n\nTASK [SYS &gt;&gt; DEBUG to see loop] ************************************************\nok: [rtr01] =&gt; (item=Building configuration...\n\nCurrent configuration : 68 bytes\n!\ninterface Loopback0\n ip address 10.100.100.1 255.255.255.255\nend) =&gt; {\n    \"msg\": \"Building configuration...\\n\\nCurrent configuration : 68 bytes\\n!\\nin\nterface Loopback0\\n ip address 10.100.100.1 255.255.255.255\\nend\"\n}\nok: [rtr01] =&gt; (item=Type escape sequence to abort.\nSending 20, 1500-byte ICMP Echos to 8.8.8.8, timeout is 2 seconds:\nPacket sent with a source address of 10.100.100.1\n!!!!!!!!!!!!!!!!!!!!\nSuccess rate is 100 percent (20/20), round-trip min/avg/max = 86/108/213 ms) =&gt;\n{\n    \"msg\": \"Type escape sequence to abort.\\nSending 20, 1500-byte ICMP Echos to\n 8.8.8.8, timeout is 2 seconds:\\nPacket sent with a source address of 10.100.10\n0.1 \\n!!!!!!!!!!!!!!!!!!!!\\nSuccess rate is 100 percent (20/20), round-trip min\n/avg/max = 86/108/213 ms\"\n}\nok: [rtr01] =&gt; (item=Type escape sequence to abort.\nSending 20, 1500-byte ICMP Echos to 1.1.1.1, timeout is 2 seconds:\nPacket sent with a source address of 10.100.100.1\n!!!!!!!!!!!!!!!!!!!!\nSuccess rate is 100 percent (20/20), round-trip min/avg/max = 82/108/217 ms) =&gt;\n{\n    \"msg\": \"Type escape sequence to abort.\\nSending 20, 1500-byte ICMP Echos to\n1.1.1.1, timeout is 2 seconds:\\nPacket sent with a source address of 10.100.100\n.1 \\n!!!!!!!!!!!!!!!!!!!!\\nSuccess rate is 100 percent (20/20), round-trip min/\navg/max = 82/108/217 ms\"\n}\n\nPLAY [See output from previous play] *******************************************\n\nTASK [SYS &gt;&gt; Debug variable from previous task] ********************************\nok: [localhost] =&gt; {\n    \"msg\": \"Type escape sequence to abort.\\nSending 20, 1500-byte ICMP Echos to\n1.1.1.1, timeout is 2 seconds:\\nPacket sent with a source address of 10.100.100.\n1 \\n!!!!!!!!!!!!!!!!!!!!\\nSuccess rate is 100 percent (20/20), round-trip min/av\ng/max = 82/108/217 ms\"\n}\n\nPLAY RECAP *********************************************************************\nlocalhost                  : ok=1    changed=0    unreachable=0    failed=0\nrtr01                      : ok=5    changed=0    unreachable=0    failed=0\n</code></pre> <p>Hope that this has been helpful!</p>","tags":["ansible","cisco"]},{"location":"ansible-cli-vs-ios-high-level/","title":"Ansible differences between ios config and cli config","text":"<p>This is a post that I'm going to review some of the differences between the ios_config module and the new cli_config module within Ansible networking. I became interested in the module after a recent discussion between the two. I have decided to take a look at the differences between the two.</p> <p>This is not an under the hood look at the modules. This has already been covered very well (and with better graphics than I can produce) here at the Ansible Blog look for \"cli_command and cli_config\" with your browser find function.</p> <p>I may also try to take a look at some of the other modules as well as time may permit. Next up on my interest of is the NXOS commands. I may also be limited a touch on some of the other major platforms out there, but hopefully I can find some legitimately and provide some value back.</p>","tags":["ansible","cli_command","ios_command","cisco"]},{"location":"ansible-cli-vs-ios-high-level/#differences","title":"Differences","text":"","tags":["ansible","cli_command","ios_command","cisco"]},{"location":"ansible-cli-vs-ios-high-level/#parameters","title":"Parameters","text":"<p>First the differences come in a couple of front and center options. First, in <code>cli_config</code> there are a few more options to do with committing configurations. These play a role in having a \"uniform\" module for pushing to all sorts of devices like IOS, JUNOS, and the such.</p> <p>Lines vs config</p> <p>One of the major differences in the paramaters comes on how you put a configuration into the module. With the original <code>ios_config</code> you get to pass The ordered set of commands to the module. This means that you can apply multiple commands within one statement.</p> <p>With <code>cli_config</code> you are passing a string into the module that is The config to be pushed to the network device. </p> <p>This difference is a very important one. For instance if you wanted to apply multiple lines to a configuration you will need to find another way with <code>cli_config</code> that previously was very simple to read:</p> <pre><code>  tasks:\n    - name: IOS &gt;&gt; No shut the interfaces\n      ios_config:\n        lines:\n          - description ** Configured by Ansible **\n          - no shutdown\n        parents: interface GigabitEthernet1/0\n</code></pre> <p>After doing a few different tests including using the <code>|</code> character to send multiple lines, <code>\\n</code> as a new line character, and using <code>with_items</code> all to no avail. Last step I tried to use the old carriage return <code>\\r</code> in the config at which point it was successful.</p> <pre><code>  tasks:\n    - name: CLI &gt;&gt; No shut the interfaces\n      cli_config:\n        config: \"interface Gig1/0\\rdescription **CLI Config!**\\rno shutdown\"\n</code></pre>","tags":["ansible","cli_command","ios_command","cisco"]},{"location":"ansible-cli-vs-ios-high-level/#templating","title":"Templating","text":"<p>Templating is also a little different. From the main module page you can see an example that is the following:</p>","tags":["ansible","cli_command","ios_command","cisco"]},{"location":"ansible-cli-vs-ios-high-level/#cli-config","title":"CLI Config","text":"<pre><code>- name: configure device with config (CLI)\n  cli_config:\n    config: \"{{ lookup('template', 'basic/config.j2') }}\"\n</code></pre> <p>IOS Config <pre><code>- name: configure device with config (IOS)\n  ios_config:\n    src: config.j2\n</code></pre></p> <p>So the only real difference is the lookup module used in the CLI version. This is pretty straight forward to see what it is doing. It is using the lookup filter, of type template. Then the 2nd argument is the template file that you wish to render.</p>","tags":["ansible","cli_command","ios_command","cisco"]},{"location":"ansible-cli-vs-ios-high-level/#execution-information","title":"Execution Information","text":"<p>This is maybe the biggest difference that I have found between the ios_config module and the cli_config module. When storing results of the configuration module execution, you will only get back two fields - changed and failed. You will not be able to see what was executed that you can see with the ios_config module.</p>","tags":["ansible","cli_command","ios_command","cisco"]},{"location":"ansible-cli-vs-ios-high-level/#lab-setup","title":"Lab Setup","text":"<p>The lab setup for this is pretty simple. I have added a Cisco IOS L2 switch image to the previous lab that I had in the previous post. This is really just for a device to connect to.</p> <p>I am configuring a port channel, only because that is something that I had lined up quick in the test, no other particular reason.</p> <p>The Jinja2 template file that I am calling in this execution is the following:</p> <pre><code>interface Port-channel5\n switchport trunk allowed vlan 2,4,5\n switchport trunk encapsulation dot1q\n switchport mode trunk\n spanning-tree portfast edge trunk\n</code></pre> <p>Here is the playbook run with the CLI module:</p> <pre><code>---\n# yamllint disable rule:truthy\n# yamllint disable rule:line-length\n- name: Switch config\n  connection: network_cli\n  hosts: switches\n  gather_facts: no\n  become: yes\n  become_method: enable\n  tags: ['switches']\n\n  tasks:\n\n    - name: CLI &gt;&gt; Configure Port channel\n      cli_config:\n        config: \"{{ lookup('template', 'port_channel.j2') }}\"\n      register: cli_output\n\n    - name: DEBUG\n      debug:\n        msg: \"{{ item }}\"\n      with_items:\n        - \"{{ cli_output }}\"\n</code></pre> <p>Output from this is:</p> <pre><code>PLAY [Switch config] *******************************************************************************\n\nTASK [CLI &gt;&gt; Configure Port channel] ***************************************************************\nchanged: [sw01]\n\nTASK [DEBUG] ***************************************************************************************\nok: [sw01] =&gt; (item={'failed': False, u'changed': True}) =&gt; {\n    \"msg\": {\n        \"changed\": true,\n        \"failed\": false\n    }\n}\n\nPLAY RECAP *****************************************************************************************\nsw01                       : ok=2    changed=1    unreachable=0    failed=0\n</code></pre> <p>Moving to virtually the same playbook here:</p> <pre><code>---\n# yamllint disable rule:truthy\n# yamllint disable rule:line-length\n- name: Switch config\n  connection: network_cli\n  hosts: switches\n  gather_facts: no\n  become: yes\n  become_method: enable\n  tags: ['switches']\n\n  tasks:\n\n    - name: IOS &gt;&gt; Configure port channel\n      ios_config:\n        src: port_channel.j2\n      register: ios_output\n\n    - name: DEBUG\n      debug:\n        msg: \"{{ item }}\"\n      with_items:\n        - \"{{ ios_output }}\"\n</code></pre> <p>The resulting output also includes banners, commands, and updates.</p> <pre><code>PLAY [Switch config] ***********************************************************\n\nTASK [IOS &gt;&gt; Configure port channel] *******************************************\nchanged: [sw01]\n\nTASK [DEBUG] *******************************************************************\nok: [sw01] =&gt; (item={'failed': False, u'commands': [u'interface Port-channel5',\nu'switchport trunk allowed vlan 2,4,6'], u'changed': True, u'updates': [u'interf\nace Port-channel5', u'switchport trunk allowed vlan 2,4,6'], u'banners': {}}) =&gt;\n{\n    \"msg\": {\n        \"banners\": {},\n        \"changed\": true,\n        \"commands\": [\n            \"interface Port-channel5\",\n            \"switchport trunk allowed vlan 2,4,6\"\n        ],\n        \"failed\": false,\n        \"updates\": [\n            \"interface Port-channel5\",\n            \"switchport trunk allowed vlan 2,4,6\"\n        ]\n    }\n}\n\nPLAY RECAP *********************************************************************\nsw01                       : ok=2    changed=1    unreachable=0    failed=0\n</code></pre>","tags":["ansible","cli_command","ios_command","cisco"]},{"location":"ansible-network-engine-ntc-templates/","title":"Ansible Network Engine and NTC Templates","text":"<p>In this post we will talk about primarily three components that will work together to get structured data out of the command line of a Cisco device. The three pieces are:</p> <ul> <li>Ansible Network Engine </li> <li>Google's TextFSM</li> <li>Network to Code Templates</li> </ul>","tags":["ansible","ntc","parsing","cisco"]},{"location":"ansible-network-engine-ntc-templates/#why-this-post","title":"Why this Post?","text":"<p>I'm writing this post because I was initially hesitant to start using the Ansible role originally when I was doing everything pretty well with the generic modules that come available with Ansible. I was challenged to migrate a Python script that was using TextFSM and Netmiko to be in Ansible. So I was originally aware of Ansible Network Engine, but had not done anything with it. So what better time than to put it to practice than when it is needed.</p>","tags":["ansible","ntc","parsing","cisco"]},{"location":"ansible-network-engine-ntc-templates/#ansible-network-engine","title":"Ansible Network Engine","text":"<p>The Ansible network engine is an Ansible role that is being developed by the Red Hat Ansible team. From the Github page:</p> <p>This role provides the foundation for building network roles by providing modules and plug-ins that are common to all Ansible Network roles.</p> <p>Within Ansible Network Engine you have the ability to <code>parse</code> the output of text. You can write your own parser, or leverage some work that has been done by others (and willing to put the work out for the good of the community - not stealing it).</p> <p>I'm going to recommend to read more on Ansible Network Engine parser with your own text parsing to go to this site - https://termlen0.github.io/2018/06/26/observations/ This post is more about using already existing TextFSM parsers with the help from NTC than the parser itself.</p> <p>A second link found recently is from the Ansible linklight (learning) team. Take a look here if wanting to do more with Parsers. https://github.com/ansible/workshops/tree/master/exercises/ansible_network/supplemental/3-1-parser</p>","tags":["ansible","ntc","parsing","cisco"]},{"location":"ansible-network-engine-ntc-templates/#textfsm","title":"TextFSM","text":"<p>From the Github page: </p> <p>TextFSM is a Python module which implements a template based state machine for parsing semi-formatted text. Originally developed to allow programmatic access to information returned from the command line interface (CLI) of networking devices.</p> <p>Basically the gist of things is that TextFSM takes text that is output from a show command and puts it into structured data. It does this using a regex pattern matching setup under the hood. This can be helpful for grabbing information out of a text blob issues to networking devices.</p> <p>You must first install the textfsm python module for this to work. To install, I recommend installing on both Python2 and Python3 in case the Ansible version is still using Python2:</p> <pre><code>pip install textfsm\n</code></pre> <pre><code>pip3 install textfsm\n</code></pre>","tags":["ansible","ntc","parsing","cisco"]},{"location":"ansible-network-engine-ntc-templates/#textfsm-parser","title":"TextFSM Parser","text":"<p>Digging into the code on the Ansible Network Engine Github page you will find the file: https://github.com/ansible-network/network-engine/blob/devel/library/textfsm_parser.py</p> <p>This is the TextFSM parser engine that is able to be leveraged. Looking at the Python file and the <code>EXAMPLES</code> section you can find much more information about how to leverage the particular module. From the Python file it has the following section:</p> <pre><code>- name: store returned facts into a key call output\n  textfsm_parser:\n    file: files/parser_templates/show_interface.yaml\n    content: \"{{ lookup('file', 'output/show_interfaces.txt') }}\"\n    name: output\n</code></pre> <p>Breaking this down further helps to get to the point. </p> <p>Line 1: This is the name of the Ansible task, nothing new here Line 2: Calls the plugin <code>textfsm_parser</code> Line 3: <code>File</code>, this is the parsing file that you are leveraging in the task, and where you would call the file location for the ntc template Line 4: <code>Content</code>, this is what you are going to send through the parser. In the example given it is a file, but you can also have a variable of say output from a previous command run put in here Line 5: <code>name</code>, this is where you will store the output data, it will be in a structured format</p> <p>This will not get into reading the output of the structured data. For more on that please take a look back at my previous post on working without output </p> <p>The original tricky part was the part about the <code>File</code>. Originally a lot of posts related to having a parser file all set to go. My original thinking was I don't see those parsers, but they are in the examples. I decided to try to point the textfsm parser at an NTC template that had been downloaded. After this, success. </p> <p><code>Content</code> is the text that you want to send through the parser. So a variable or a text file</p> <p>The <code>name</code> portion is what you are registering as facts that can be accessed underneath <code>ansible_facts</code>. </p>","tags":["ansible","ntc","parsing","cisco"]},{"location":"ansible-network-engine-ntc-templates/#ntc-templates","title":"NTC Templates","text":"<p>NTC (Network to Code) has a community environment with a significant number of parsers available. I have found these particularly helpful in the Cisco environment. To install - checkout the Github page.</p> <p>https://github.com/networktocode/ntc-templates</p> <p>These are files, so the best methodology I have found is to download these to your machine using the git process of cloning. There are updates made regularly, so make sure to do <code>git pull</code> to get the most recent version. </p> <p>Pro tip: You may want to install these to your home directory. This is the default directory if memory serves me right that Netmiko will look for the textfsm templates as well</p>","tags":["ansible","ntc","parsing","cisco"]},{"location":"ansible-network-engine-ntc-templates/#sample","title":"Sample","text":"<p>Here is the playbook that I will run against the lab environment.</p> <pre><code>---\n# yamllint disable rule:truthy\n# yamllint disable rule:line-length\n- name: Switch config\n  connection: network_cli\n  hosts: switches\n  gather_facts: no\n  become: yes\n  become_method: enable\n  roles:\n    - ansible-network.network-engine\n\n  tasks:\n\n    - name: CLI &gt;&gt; Get CDP neighbors\n      ios_command:\n        commands:\n          - show cdp neighbors\n      register: command_output\n\n    - name: SYS &gt;&gt; Parse CDP Information\n      textfsm_parser:\n        file: \"/opt/ntc-templates/templates/cisco_ios_show_cdp_neighbors.template\"\n        content: \"{{ command_output.stdout[0] }}\"\n        name: cdp_facts\n\n    - name: DEBUG &gt;&gt; Print output\n      debug:\n        msg: \"{{ ansible_facts.cdp_facts }}\"\n</code></pre>","tags":["ansible","ntc","parsing","cisco"]},{"location":"ansible-network-engine-ntc-templates/#task-1-get-cdp-neighbor-information","title":"Task 1: Get CDP Neighbor information","text":"<p>This task is going to log in and get the CDP neighbor information from the device and register it to a fact <code>command_output</code></p>","tags":["ansible","ntc","parsing","cisco"]},{"location":"ansible-network-engine-ntc-templates/#task-2-send-through-the-parser","title":"Task 2: Send through the parser","text":"<p>This is where the parser comes in, it will take the command output taken in the first task and send it through the textfsm parser. This then registers the information underneath <code>ansible_facts</code>. The next task is where you will see that output.</p>","tags":["ansible","ntc","parsing","cisco"]},{"location":"ansible-network-engine-ntc-templates/#task-3-prints-the-output","title":"Task 3: Prints the output","text":"<p>You will see the information underneath the <code>ansible_facts</code> to get at the information as it sits parsed.</p>","tags":["ansible","ntc","parsing","cisco"]},{"location":"ansible-network-engine-ntc-templates/#sample-output","title":"Sample Output","text":"<pre><code>PLAY [Switch config] *******************************************************************************\n\nTASK [CLI &gt;&gt; Get CDP neighbors] ********************************************************************\nok: [sw01]\n\nTASK [SYS &gt;&gt; Parse CDP Information] ****************************************************************\nok: [sw01]\n\nTASK [DEBUG &gt;&gt; Print output] ***********************************************************************\nok: [sw01] =&gt; {\n    \"msg\": [\n        {\n            \"CAPABILITY\": \"R S\",\n            \"LOCAL_INTERFACE\": \"Gig 1/0\",\n            \"NEIGHBOR\": \"Switch\",\n            \"NEIGHBOR_INTERFACE\": \"Gig 1/0\",\n            \"PLATFORM\": \"I\"\n        },\n        {\n            \"CAPABILITY\": \"R S\",\n            \"LOCAL_INTERFACE\": \"Gig 1/1\",\n            \"NEIGHBOR\": \"Switch\",\n            \"NEIGHBOR_INTERFACE\": \"Gig 1/1\",\n            \"PLATFORM\": \"I\"\n        },\n        {\n            \"CAPABILITY\": \"R\",\n            \"LOCAL_INTERFACE\": \"Gig 0/0\",\n            \"NEIGHBOR\": \"router_edge\",\n            \"NEIGHBOR_INTERFACE\": \"Gig 0/2\",\n            \"PLATFORM\": \"B\"\n        }\n    ]\n}\n\nPLAY RECAP *****************************************************************************************\nsw01                       : ok=3    changed=0    unreachable=0    failed=0\n</code></pre>","tags":["ansible","ntc","parsing","cisco"]},{"location":"ansible-network-engine-ntc-templates/#summary","title":"Summary","text":"<p>Putting all of these together into a playbook you can more easily get at information presented from a network device command line. Let's say you wanted to get CDP neighbors. The CDP neighbor command output is tough to work with, other than seeing if something is in the output. </p>","tags":["ansible","ntc","parsing","cisco"]},{"location":"ansible-ios-banner/","title":"Ansible IOS Banner","text":"<p>Today's post is going to be a short and sweet one (unless I get to writing two). I'm going to take a look at <code>ios_banner</code> module. This one is pretty much straight to the point, what it states, modifying the banner on an IOS device. There are multiple reasons to want to manipulate the banner on a Cisco device. We will leave those reasons to you and the organization that you are a part of for that. For now, we will take a real quick look at the module.</p>","tags":["ansible","cisco","ios_banner"]},{"location":"ansible-ios-banner/#module-documentation","title":"Module Documentation","text":"<p>First, the module documentation page is here.</p>","tags":["ansible","cisco","ios_banner"]},{"location":"ansible-ios-banner/#getting-started-with-the-lab","title":"Getting Started with the Lab","text":"<p>I'm starting out with no banner on the page of my system as evident from this login:</p> <p>Cisco Router Login</p> <pre><code>Escape character is '^]'.\n\nUsername: \n</code></pre>","tags":["ansible","cisco","ios_banner"]},{"location":"ansible-ios-banner/#ios-banner-play-tasks","title":"IOS Banner Play / tasks","text":"<p>Let's go ahead and apply a banner to the login with the following tasks:</p> <pre><code>    - name: IOS &gt;&gt; Set banner to single login\n      ios_banner:\n        banner: login\n        state: present\n        text: \"Quick banner, this device is being managed by Ansible.\"\n      register: output\n\n    - name: DEBUG &gt;&gt; Output\n      debug:\n        msg: \"{{ output }}\"\n</code></pre> <p>I now have the following banner showing up on the login to the Cisco device over SSH.</p> <p>Banner on Router</p> <pre><code>Quick banner, this device is being managed by Ansible.\n</code></pre>","tags":["ansible","cisco","ios_banner"]},{"location":"ansible-ios-banner/#output-of-ios_banner","title":"Output of ios_banner","text":"<p>Let's take a look at the output of the <code>ios_banner</code> module when saved to a variable. We get the following output to the screen:</p> <pre><code>TASK [DEBUG &gt;&gt; Output] *********************************************************\nok: [rtr01] =&gt; {\n    \"msg\": {\n        \"changed\": true,\n        \"commands\": [\n            \"banner login @\\nQuick banner, this device is being managed by Ansible.\\n@\"\n        ],\n        \"failed\": false\n    }\n}\n</code></pre> <p>There are three \"outputs\" to the variable. Changed, commands, and failed. </p> <p>Changed looks to be the true/false of was the device changed as part of the play execution. comamnds are what actually was run on the Cisco device from config mode. failed is the state of the task, true/false</p>","tags":["ansible","cisco","ios_banner"]},{"location":"ansible-ios-banner/#multiline-banner","title":"Multiline banner","text":"<p>To set a multi-line banner on something, it is as simple as using the <code>|</code> or <code>&gt;</code> keys that are part of YAML. These again are functions known within YAML and not something specific to Ansible, so this is something that would carry over between languages/tools that are using YAML as the formatting.</p> <pre><code>  tasks:\n    - name: IOS &gt;&gt; Set banner to single login\n      ios_banner:\n        banner: login\n        state: present\n        text: |\n          ===This device is being managed by Ansible===\n          Making changes at your own risk!\n      register: output\n\n    - name: DEBUG &gt;&gt; Output\n      debug:\n        msg: \"{{ output }}\"\n</code></pre> <p>This has successfully added a multiple line banner to the configuration:</p> <pre><code>TASK [IOS &gt;&gt; Set banner to single login] ***************************************\nchanged: [rtr01]\n\nTASK [DEBUG &gt;&gt; Output] *********************************************************\nok: [rtr01] =&gt; {\n    \"msg\": {\n        \"changed\": true,\n        \"commands\": [\n            \"banner login @\\n===This device is being managed by Ansible===\\nMaki\nng changes at your own risk!\\n@\"\n        ],\n        \"failed\": false\n    }\n}\n</code></pre> <p>A quick look at the configuration itself in IOS:</p> <pre><code>!\nbanner login ^C\n===This device is being managed by Ansible===\nMaking changes at your own risk!\n^C\n!\n</code></pre> <p>This automatically puts the <code>^C</code> as the character delineation for you, as that was not something that was specified within the module itself.</p>","tags":["ansible","cisco","ios_banner"]},{"location":"ansible-ios-banner/#setting-multiple-banners","title":"Setting Multiple Banners","text":"<p>If you want to set multiple banners, say <code>exec</code>, <code>login</code>, and <code>motd</code>, you will want to change this to leveraging <code>with_items</code>. This way Ansible will iterate and set all of these. Here is the Play.</p> <p>Notice the changes on line 3 below has been changed from the banner <code>login</code> to the variable <code>{{ item }}</code>. <code>with_items</code> has been added on line 8. And we have set this to change the banner for <code>motd</code> (Message of the Day), <code>login</code>, and <code>exec</code>. </p> <pre><code>    - name: IOS &gt;&gt; Set banner to single login\n      ios_banner:\n        banner: \"{{ item }}\"\n        state: present\n        text: |\n          ===This device is being managed by Ansible===\n          Making changes at your own risk!\n      with_items:\n        - motd\n        - login\n        - exec\n      register: output\n\n    - name: DEBUG &gt;&gt; Output\n      debug:\n        msg: \"{{ output }}\"\n</code></pre> <p>Output <pre><code>TASK [IOS &gt;&gt; Set banner to single login] ***************************************\nchanged: [rtr01] =&gt; (item=motd)\nok: [rtr01] =&gt; (item=login)\nchanged: [rtr01] =&gt; (item=exec)\n\nTASK [DEBUG &gt;&gt; Output] *********************************************************\nok: [rtr01] =&gt; {\n    \"msg\": {\n        \"changed\": true,\n        \"msg\": \"All items completed\",\n        \"results\": [\n            {\n                \"_ansible_ignore_errors\": null,\n                \"_ansible_item_label\": \"motd\",\n                \"_ansible_item_result\": true,\n                \"_ansible_no_log\": false,\n                \"_ansible_parsed\": true,\n                \"changed\": true,\n                \"commands\": [\n                    \"banner motd @\\n===This device is being managed by Ansible===\\nMaking changes at your own risk!\\n@\"\n                ],\n                \"failed\": false,\n                \"invocation\": {\n                    \"module_args\": {\n                        \"auth_pass\": null,\n                        \"authorize\": null,\n                        \"banner\": \"motd\",\n                        \"host\": null,\n                        \"password\": null,\n                        \"port\": null,\n                        \"provider\": null,\n                        \"ssh_keyfile\": null,\n                        \"state\": \"present\",\n                        \"text\": \"===This device is being managed by Ansible===\\nMaking changes at your own risk!\\n\",\n                        \"timeout\": null,\n                        \"username\": null\n                    }\n                },\n                \"item\": \"motd\"\n            },\n            {\n                \"_ansible_ignore_errors\": null,\n                \"_ansible_item_label\": \"login\",\n                \"_ansible_item_result\": true,\n                \"_ansible_no_log\": false,\n                \"_ansible_parsed\": true,\n                \"changed\": false,\n                \"commands\": [],\n                \"failed\": false,\n                \"invocation\": {\n                    \"module_args\": {\n                        \"auth_pass\": null,\n                        \"authorize\": null,\n                        \"banner\": \"login\",\n                        \"host\": null,\n                        \"password\": null,\n                        \"port\": null,\n                        \"provider\": null,\n                        \"ssh_keyfile\": null,\n                        \"state\": \"present\",\n                        \"text\": \"===This device is being managed by Ansible===\\nMaking changes at your own risk!\\n\",\n                        \"timeout\": null,\n                        \"username\": null\n                    }\n                },\n                \"item\": \"login\"\n            },\n            {\n                \"_ansible_ignore_errors\": null,\n                \"_ansible_item_label\": \"exec\",\n                \"_ansible_item_result\": true,\n                \"_ansible_no_log\": false,\n                \"_ansible_parsed\": true,\n                \"changed\": true,\n                \"commands\": [\n                    \"banner exec @\\n===This device is being managed by Ansible===\\nMaking changes at your own risk!\\n@\"\n                ],\n                \"failed\": false,\n                \"invocation\": {\n                    \"module_args\": {\n                        \"auth_pass\": null,\n                        \"authorize\": null,\n                        \"banner\": \"exec\",\n                        \"host\": null,\n                        \"password\": null,\n                        \"port\": null,\n                        \"provider\": null,\n                        \"ssh_keyfile\": null,\n                        \"state\": \"present\",\n                        \"text\": \"===This device is being managed by Ansible===\\nMaking changes at your own risk!\\n\",\n                        \"timeout\": null,\n                        \"username\": null\n                    }\n                },\n                \"item\": \"exec\"\n            }\n        ]\n    }\n}\n</code></pre></p>","tags":["ansible","cisco","ios_banner"]},{"location":"ansible-ios-banner/#summary","title":"Summary","text":"<p>The <code>ios_banner</code> module is a quick and handy module for those that need to have banners as part of the operating entity. There are many reasons for banners that this is not going to explore further, there are plenty of other resources (including possible Legal ones) available for this discussion. Hopefully this has been a good primer of what things look like for the <code>ios_banner</code> and what output looks like.</p>","tags":["ansible","cisco","ios_banner"]},{"location":"ansible-ios-vlan/","title":"Ansible IOS VLAN","text":"<p>Back to it finally. Going to take a look at the Ansible module ios_vlan. The purpose of this is to provide a declarative module for managing VLANs on IOS devices. In this I will be using IOSv-L2 images. There are a few interesting quirks (as I will call it) within the parameters for the module.</p>","tags":["ansible","ios_vlan","cisco"]},{"location":"ansible-ios-vlan/#module-documentation","title":"Module Documentation","text":"<p>First, the module documentation page is here.</p>","tags":["ansible","ios_vlan","cisco"]},{"location":"ansible-ios-vlan/#getting-started-with-the-module","title":"Getting Started with the module","text":"","tags":["ansible","ios_vlan","cisco"]},{"location":"ansible-ios-vlan/#vlans-pre-module-work","title":"VLANs pre-module work","text":"<p>Starting out the switch is pretty bare as it relates to the number of VLANs. There is VLAN2 defined on the switch that has an uplink to the edge (of the lab) router. The base VLANs are the only other ones on the device:</p> <pre><code>#show vlan\n\nVLAN Name                             Status    Ports\n---- -------------------------------- --------- -------------------------------\n1    default                          active    Gi0/1, Gi0/3, Gi1/0, Gi1/2\n                                                Gi1/3\n2    TRANSIT                          active    Gi0/0, Gi1/1\n1002 fddi-default                     act/unsup \n1003 token-ring-default               act/unsup \n1004 fddinet-default                  act/unsup \n1005 trnet-default                    act/unsup \n\nVLAN Type  SAID       MTU   Parent RingNo BridgeNo Stp  BrdgMode Trans1 Trans2\n---- ----- ---------- ----- ------ ------ -------- ---- -------- ------ ------\n1    enet  100001     1500  -      -      -        -    -        0      0   \n2    enet  100002     1500  -      -      -        -    -        0      0   \n1002 fddi  101002     1500  -      -      -        -    -        0      0   \n1003 tr    101003     1500  -      -      -        -    -        0      0   \n1004 fdnet 101004     1500  -      -      -        ieee -        0      0   \n1005 trnet 101005     1500  -      -      -        ibm  -        0      0   \n\nRemote SPAN VLANs\n------------------------------------------------------------------------------\n\n\nPrimary Secondary Type              Ports\n------- --------- ----------------- ------------------------------------------\n</code></pre>","tags":["ansible","ios_vlan","cisco"]},{"location":"ansible-ios-vlan/#building-the-play","title":"Building the Play","text":"<p>The module has the following parameters, required ones in bold. This skips over the deprecated parameters:</p> <ul> <li>aggregate: List of VLANs definitions</li> <li>associated_interfaces: Checks for the operational state of the interface</li> <li>delay: default to 10 seconds, how long to wait for the declarative state to be seen</li> <li>interfaces: a list of interfaces that should have the VLAN assigned to it</li> <li>name: Name of the VLAN</li> <li>purge: Purge VLANs not defined in the aggregate parameter</li> <li>state: present/absent/active/suspend - the state that it should be in</li> <li>vlan_id: ID of the VLAN</li> </ul> <p>So what does aggregate mean? It sounds like that if you wanted to have a large list of VLANs, this is the way to go with a task. First attempt at seeing what it does, the following playbook was setup</p> <pre><code>---\n# yamllint disable rule:truthy\n# yamllint disable rule:line-length\n- name: Switch config\n  connection: network_cli\n  hosts: sw19\n  gather_facts: no\n  become: yes\n  become_method: enable\n  tasks:\n    - name: IOS &gt;&gt; VLAN Updates\n      ios_vlan:\n        aggregate:\n          - 2\n          - 5\n        vlan_id: 5\n        name: TEST VLAN 5\n        state: present\n      register: command_output\n\n    - name: DEBUG &gt;&gt; VLAN Update\n      debug:\n        msg: \"{{ command_output }}\"\n</code></pre> <p>When executing it came across an error that gave some more insight that was not portrayed on the module definition page.</p> <pre><code>ansible-playbook output_test.yml -i ./lab_hosts\n\nPLAY [Switch config] ***********************************************************\n\nTASK [IOS &gt;&gt; VLAN Updates] *****************************************************\nfatal: [sw19]: FAILED! =&gt; {\"changed\": false, \"msg\": \"parameters are mutually exclusive: vlan_id, aggregate\"}\n    to retry, use: --limit @/Users/joshv/Documents/Ansible/output_test.retry\n\nPLAY RECAP *********************************************************************\nsw19                       : ok=0    changed=0    unreachable=0    failed=1   \n</code></pre> <p>Modifying the playbook with the fatal error message out. It now looks like this:</p> <pre><code>---\n# yamllint disable rule:truthy\n# yamllint disable rule:line-length\n- name: Switch config\n  connection: network_cli\n  hosts: sw19\n  gather_facts: no\n  become: yes\n  become_method: enable\n  tasks:\n    - name: IOS &gt;&gt; VLAN Updates\n      ios_vlan:\n        aggregate: \n          - { 'vlan_id': 2, 'name': 'TRANSIT' }\n          - { 'vlan_id': 5, 'name': 'Test VLAN' }\n        state: present\n      register: command_output\n\n    - name: DEBUG &gt;&gt; VLAN Update\n      debug:\n        msg: \"{{ command_output }}\"\n</code></pre> <p>This will now deploy in aggregate all of the VLANs that are being defined in the list of dictionaries. Looking at the output this is what is now on the switch:</p> <pre><code>#show vlan\n\nVLAN Name                             Status    Ports\n---- -------------------------------- --------- -------------------------------\n1    default                          active    Gi0/1, Gi0/3, Gi1/0, Gi1/2\n                                                Gi1/3\n2    TRANSIT                          active    Gi0/0, Gi1/1\n5    Test VLAN                        active    Gi0/2\n1002 fddi-default                     act/unsup \n1003 token-ring-default               act/unsup \n1004 fddinet-default                  act/unsup \n1005 trnet-default                    act/unsup \n\nVLAN Type  SAID       MTU   Parent RingNo BridgeNo Stp  BrdgMode Trans1 Trans2\n---- ----- ---------- ----- ------ ------ -------- ---- -------- ------ ------\n1    enet  100001     1500  -      -      -        -    -        0      0   \n2    enet  100002     1500  -      -      -        -    -        0      0   \n5    enet  100005     1500  -      -      -        -    -        0      0   \n1002 fddi  101002     1500  -      -      -        -    -        0      0   \n1003 tr    101003     1500  -      -      -        -    -        0      0   \n1004 fdnet 101004     1500  -      -      -        ieee -        0      0   \n1005 trnet 101005     1500  -      -      -        ibm  -        0      0   \n\nRemote SPAN VLANs\n------------------------------------------------------------------------------\n\n\nPrimary Secondary Type              Ports\n------- --------- ----------------- ------------------------------------------\n</code></pre>","tags":["ansible","ios_vlan","cisco"]},{"location":"ansible-ios-vlan/#changing-the-vlans-on-the-device","title":"Changing the VLANs on the device","text":"<p>Removing a VLAN that is not supposed to be on the device is incredibly simple with this aggregate feature as well. If we change the play to looking like this</p> <pre><code>---\n# yamllint disable rule:truthy\n# yamllint disable rule:line-length\n- name: Switch config\n  connection: network_cli\n  hosts: sw19\n  gather_facts: no\n  become: yes\n  become_method: enable\n  tasks:\n    - name: IOS &gt;&gt; VLAN Updates\n      ios_vlan:\n        aggregate: \n          - { 'vlan_id': 2, 'name': 'TRANSIT', state: present }\n          - { 'vlan_id': 5, 'name': 'Test VLAN', state: absent }\n      register: command_output\n\n    - name: DEBUG &gt;&gt; VLAN Update\n      debug:\n        msg: \"{{ command_output }}\"\n</code></pre> <p>The resulting play execution shows that the VLAN is removed from the command output.</p> <pre><code>PLAY [Switch config] ***********************************************************\n\nTASK [IOS &gt;&gt; VLAN Updates] *****************************************************\nchanged: [sw19]\n\nTASK [DEBUG &gt;&gt; VLAN Update] ****************************************************\nok: [sw19] =&gt; {\n    \"msg\": {\n        \"changed\": true, \n        \"commands\": [\n            \"no vlan 5\"\n        ], \n        \"failed\": false\n    }\n}\n\nPLAY RECAP *********************************************************************\nsw19                       : ok=2    changed=1    unreachable=0    failed=0   \n</code></pre>","tags":["ansible","ios_vlan","cisco"]},{"location":"ansible-ios-vlan/#adding-a-vlan-and-assigning-to-interface","title":"Adding a VLAN and assigning to Interface","text":"<p>Want to create an interface and assign it to a VLAN quickly? Here is where the ios_vlan module may be able to help very quickly. In the lab it is very simple as there are only a few interfaces on the layer 2 switch that we have.</p>","tags":["ansible","ios_vlan","cisco"]},{"location":"ansible-ios-vlan/#play-definition","title":"Play Definition","text":"<pre><code>---\n# yamllint disable rule:truthy\n# yamllint disable rule:line-length\n- name: Switch config\n  connection: network_cli\n  hosts: sw19\n  gather_facts: no\n  become: yes\n  become_method: enable\n  tasks:\n    - name: IOS &gt;&gt; VLAN Updates\n      ios_vlan:\n        vlan_id: 12\n        name: test-vlan\n        interfaces:\n          - GigabitEthernet1/2\n      register: command_output\n\n    - name: DEBUG &gt;&gt; VLAN Update\n      debug:\n        msg: \"{{ command_output }}\"\n</code></pre>","tags":["ansible","ios_vlan","cisco"]},{"location":"ansible-ios-vlan/#playbook-execution-adding-a-vlan","title":"Playbook Execution - Adding a VLAN","text":"<p>This play execution will both add a VLAN to the switch, and assign the interfaces to the VLAN as an access port. With the output from the execution the module registers each of the commands that are being issued to the switch. This shows the VLAN is first created, then goes into the interface assigned as a parameter. Lastly it sets that interface to being an access interface in the VLAN.</p> <pre><code>PLAY [Switch config] ***********************************************************\n\nTASK [IOS &gt;&gt; VLAN Updates] *****************************************************\nchanged: [sw19]\n\nTASK [DEBUG &gt;&gt; VLAN Update] ****************************************************\nok: [sw19] =&gt; {\n    \"msg\": {\n        \"changed\": true, \n        \"commands\": [\n            \"vlan 12\", \n            \"name test-vlan\", \n            \"interface GigabitEthernet1/2\", \n            \"switchport mode access\", \n            \"switchport access vlan 12\"\n        ], \n        \"failed\": false\n    }\n}\n\nPLAY RECAP *********************************************************************\nsw19                       : ok=2    changed=1    unreachable=0    failed=0   \n</code></pre>","tags":["ansible","ios_vlan","cisco"]},{"location":"ansible-ios-vlan/#ios_vlan-purge-parameter","title":"IOS_VLAN - Purge Parameter","text":"<p>Originally when looking at this module I kind of passed over purge parameter. It was mentioned that this is used with conjunction of the aggregrate parameter. My original thinking when I read aggregate was that this was somehow related to Link Aggregation Control Protocol. Now with looking at the module much more in depth, I see that was a wrong assumption (in case it was for others). This adds significant power, to make sure that a switch is configured the way that you define within a play/task and stays configured that way. If some rogue actor has added a VLAN, how will you ever know. So for this next test, I went and created three manual VLANs on the switch for VLANs 10, 13, and 100. </p> <pre><code>VLAN Name                             Status    Ports\n---- -------------------------------- --------- -------------------------------\n1    default                          active    Gi0/1, Gi0/3, Gi1/0, Gi1/3\n2    TRANSIT                          active    Gi0/0, Gi1/1\n5    Test VLAN                        active    Gi0/2\n10   MANUAL                           active    \n12   QB VLAN                          active    Gi1/2\n13   MANUAL13                         active    \n100  MANUAL100                        active    \n1002 fddi-default                     act/unsup \n1003 token-ring-default               act/unsup \n1004 fddinet-default                  act/unsup \n1005 trnet-default                    act/unsup \n\nVLAN Type  SAID       MTU   Parent RingNo BridgeNo Stp  BrdgMode Trans1 Trans2\n---- ----- ---------- ----- ------ ------ -------- ---- -------- ------ ------\n1    enet  100001     1500  -      -      -        -    -        0      0   \n2    enet  100002     1500  -      -      -        -    -        0      0   \n5    enet  100005     1500  -      -      -        -    -        0      0   \n10   enet  100010     1500  -      -      -        -    -        0      0   \n12   enet  100012     1500  -      -      -        -    -        0      0   \n13   enet  100013     1500  -      -      -        -    -        0      0   \n\nVLAN Type  SAID       MTU   Parent RingNo BridgeNo Stp  BrdgMode Trans1 Trans2\n---- ----- ---------- ----- ------ ------ -------- ---- -------- ------ ------\n100  enet  100100     1500  -      -      -        -    -        0      0   \n1002 fddi  101002     1500  -      -      -        -    -        0      0   \n1003 tr    101003     1500  -      -      -        -    -        0      0   \n1004 fdnet 101004     1500  -      -      -        ieee -        0      0   \n1005 trnet 101005     1500  -      -      -        ibm  -        0      0   \n\nRemote SPAN VLANs\n------------------------------------------------------------------------------\n\n\nPrimary Secondary Type              Ports\n------- --------- ----------------- ------------------------------------------\n</code></pre>","tags":["ansible","ios_vlan","cisco"]},{"location":"ansible-ios-vlan/#play-setup-purge-vlans","title":"Play Setup - Purge VLANs","text":"<pre><code>---\n# yamllint disable rule:truthy\n# yamllint disable rule:line-length\n- name: Switch config\n  connection: network_cli\n  hosts: sw19\n  gather_facts: no\n  become: yes\n  become_method: enable\n  tasks:\n    - name: IOS &gt;&gt; VLAN Updates\n      ios_vlan:\n        aggregate: \n          - { 'vlan_id': 2, 'name': 'TRANSIT', state: present }\n          - { 'vlan_id': 5, 'name': 'Test VLAN', state: present }\n          - { 'vlan_id': 12, 'name': 'THE TEST VLAN', state: present }\n        purge: yes\n      register: command_output\n\n    - name: DEBUG &gt;&gt; VLAN Update\n      debug:\n        msg: \"{{ command_output }}\"\n</code></pre>","tags":["ansible","ios_vlan","cisco"]},{"location":"ansible-ios-vlan/#play-execution","title":"PLAY EXECUTION","text":"<p>Below you will find the play execution and the resulting commands sent to the switch. To show this I did have to run the playbook twice as the bug that I found did not run properly the first time.</p> <p>One important note that I did find when testing this playbook, at least within Ansible version 2.7.5 to use the purge function, you must use the keyword yes instead of true. If you use true the purge function will not work.</p> <pre><code>PLAY [Switch config] ***********************************************************\n\nTASK [IOS &gt;&gt; VLAN Updates] *****************************************************\nchanged: [sw19]\n\nTASK [DEBUG &gt;&gt; VLAN Update] ****************************************************\nok: [sw19] =&gt; {\n    \"msg\": {\n        \"changed\": true, \n        \"commands\": [\n            \"vlan 5\", \n            \"name Test VLAN\", \n            \"vlan 12\", \n            \"name THE TEST VLAN\", \n            \"no vlan 10\", \n            \"no vlan 13\", \n            \"no vlan 100\", \n            \"no vlan 1002\", \n            \"no vlan 1003\", \n            \"no vlan 1004\", \n            \"no vlan 1005\"\n        ], \n        \"failed\": false\n    }\n}\n\nPLAY RECAP *********************************************************************\nsw19                       : ok=2    changed=1    unreachable=0    failed=0  \n</code></pre>","tags":["ansible","ios_vlan","cisco"]},{"location":"ansible-ios-vlan/#resulting-vlan-configuration","title":"Resulting VLAN Configuration","text":"<pre><code>#show vlan\n\nVLAN Name                             Status    Ports\n---- -------------------------------- --------- -------------------------------\n1    default                          active    Gi0/1, Gi0/3, Gi1/0, Gi1/3\n2    TRANSIT                          active    Gi0/0, Gi1/1\n5    Test VLAN                        active    Gi0/2\n12   THE TEST VLAN                    active    Gi1/2\n1002 fddi-default                     act/unsup \n1003 token-ring-default               act/unsup \n1004 fddinet-default                  act/unsup \n1005 trnet-default                    act/unsup \n\nVLAN Type  SAID       MTU   Parent RingNo BridgeNo Stp  BrdgMode Trans1 Trans2\n---- ----- ---------- ----- ------ ------ -------- ---- -------- ------ ------\n1    enet  100001     1500  -      -      -        -    -        0      0   \n2    enet  100002     1500  -      -      -        -    -        0      0   \n5    enet  100005     1500  -      -      -        -    -        0      0   \n12   enet  100012     1500  -      -      -        -    -        0      0   \n1002 fddi  101002     1500  -      -      -        -    -        0      0   \n1003 tr    101003     1500  -      -      -        -    -        0      0   \n1004 fdnet 101004     1500  -      -      -        ieee -        0      0   \n1005 trnet 101005     1500  -      -      -        ibm  -        0      0   \n\nRemote SPAN VLANs\n------------------------------------------------------------------------------\n\n\nPrimary Secondary Type              Ports\n------- --------- ----------------- ------------------------------------------\n</code></pre>","tags":["ansible","ios_vlan","cisco"]},{"location":"ansible-ios-vlan/#summary","title":"Summary","text":"<p>In summary there are some pieces that need to get worked out. This was the first time that I had taken a look at the module. For the work that I've done previously I was just using Jinja2 templates to assign VLAN configuration to an interface. Looking closer at this module there is a lot of power to make sure that the proper VLANs are configured everywhere that you need, and be able to eliminate others. This module is definitely something that you should keep in your pocket.</p>","tags":["ansible","ios_vlan","cisco"]},{"location":"ansible-cisco-ios-interface/","title":"Ansible Cisco IOS Interface Module","text":"<p>Update: <code>ios_interface</code> is to be deprecated as of Ansible 2.13  </p> <p>In this post I will be taking a deeper look at the ios_interface module. This module is used to configure individual interfaces on a Cisco IOS device. The documentation for the module is located here. In this module I did have to dig into the actual Python file, and that is located here.  </p> <p>Edit: Had to update the link due to the change in Ansible coming in 2.10. I have hard linked to the IOS Interfaces module.</p> <p>This module does not configure the layer 2 or layer 3 information on an interface. There are other modules that are used for configuring these particular pieces.</p>"},{"location":"ansible-cisco-ios-interface/#a-look-at-the-parameters","title":"A look at the Parameters","text":""},{"location":"ansible-cisco-ios-interface/#required-parameters","title":"Required Parameters","text":"<p>This module only has a single required module. </p> <ul> <li>name: Name of the interface that is being configured, such as GigabitEthernet0/0/0 </li> </ul> <p>Using the module with just the one required item of name is pretty uneventful. If you wish to see the output I'm going to put that at the very bottom as an Appendix type item if you wish to see that output.</p>"},{"location":"ansible-cisco-ios-interface/#optional-parameters","title":"Optional Parameters","text":"<ul> <li>aggregate: This is what you will need to use if you want to configure multiple interfaces within the same task execution (or a loop of course)  </li> <li>delay: Time to wait before checking the state of an interface, defaults to 10 seconds</li> <li>description: Interface description, follows the Cisco command description under the interface configuration  </li> <li>duplex: (full/half/auto) duplex settings in the interface configuration  </li> <li>enabled: (yes/no) interface link status, should it be enabled/disabled  </li> <li>mtu: MTU setting, follows the mtu configuration under the interface configuration</li> <li>neighbors: Checking for the operational state, using LLDP information, either with the sub-parameter host or port </li> <li>rx_rate: Stated as Receiver rate in bits per second, not sure what this does  </li> <li>speed: Interface speed in Mbps, corresponds to the Cisco command speed under interface configuration</li> <li>tx_rate: Stated as Transmit rate in bits per second, not sure what it does  </li> </ul> <p>Note - Operational State </p> <p>As I'm writing (and not having tested yet) the operational state if you are configuring an interface and looking to validate the neighbors, you may want to up the delay time based on the LLDP neighbor timers. These timers may be longer than the default 10 seconds.  </p>"},{"location":"ansible-cisco-ios-interface/#rx_rate-and-tx_rate","title":"rx_rate and tx_rate","text":"<p>From the documentation on the module this appears to perhaps to be related to the actual interface transmit and receive rates that is being reported by the device. The documentation has some references to ge and le which would be comparisons. Based on the Python file and the variables named want_tx_rate and want_rx_rate within the Python file, this does in fact appear to be related to the interface traffic amount.</p>"},{"location":"ansible-cisco-ios-interface/#parameter-details-aggregate","title":"Parameter Details: Aggregate","text":"<p>This is what I will say is a group of interfaces to configure within a single task. This is where you will configure multiples of the ios_interface task. To leverage this you will need to create a dictionary (Array) with the required parameters for the module.</p>"},{"location":"ansible-cisco-ios-interface/#a-look-at-the-module-in-action","title":"A look at the module in action","text":"<p>First, jumping deep in. Going to take a look at what it looks like to configure interfaces using the aggregate parameter. This is going to configure specific details about two interfaces on the switch itself.</p>"},{"location":"ansible-cisco-ios-interface/#pre-change-config","title":"Pre-Change Config","text":"<p>Before the change there is just the <code>media-type</code> and the <code>negotiation</code> set to auto. These are default out of the box.</p> <pre><code>sw19#show run int gig0/1\nBuilding configuration...\n\nCurrent configuration : 71 bytes\n!\ninterface GigabitEthernet0/1\n media-type rj45\n negotiation auto\nend\n\nsw19#show run int gig0/2\nBuilding configuration...\n\nCurrent configuration : 71 bytes\n!\ninterface GigabitEthernet0/2\n media-type rj45\n negotiation auto\nend\n</code></pre>"},{"location":"ansible-cisco-ios-interface/#playbook","title":"Playbook","text":"<pre><code>---\n# yamllint disable rule:truthy\n# yamllint disable rule:line-length\n- name: Switch config\n  connection: network_cli\n  hosts: sw19\n  gather_facts: no\n  become: yes\n  become_method: enable\n  tasks:\n    - name: IOS &gt;&gt; VLAN Updates\n      ios_interface:\n        aggregate:\n          - {name: GigabitEthernet0/1, description: \"First Ansible Configured Interface\", enabled: no}\n          - {name: GigabitEthernet0/2, description: \"Second Ansible Configured Interface\", enabled: yes}\n        speed: 100\n        duplex: full\n      register: output\n\n    - name: DEBUG &gt;&gt; output\n      debug:\n        msg: \"{{ output }}\"\n</code></pre>"},{"location":"ansible-cisco-ios-interface/#ansible-output","title":"Ansible Output","text":"<p>Here we see that the commands being sent to the device are to set the speed, duplex to full, interface description, and then shutting down the interface that was set to disabled.</p> <pre><code>PLAY [Switch config] ***********************************************************\n\nTASK [IOS &gt;&gt; VLAN Updates] *****************************************************\nchanged: [sw19]\n\nTASK [DEBUG &gt;&gt; output] *********************************************************\nok: [sw19] =&gt; {\n    \"msg\": {\n        \"changed\": true, \n        \"commands\": [\n            \"interface GigabitEthernet0/1\", \n            \"speed 100\", \n            \"description First Ansible Configured Interface\", \n            \"duplex full\", \n            \"shutdown\", \n            \"interface GigabitEthernet0/2\", \n            \"speed 100\", \n            \"description Second Ansible Configured Interface\", \n            \"duplex full\"\n        ], \n        \"failed\": false\n    }\n}\n\nPLAY RECAP *********************************************************************\nsw19                       : ok=2    changed=1    unreachable=0    failed=0   \n</code></pre>"},{"location":"ansible-cisco-ios-interface/#post-execution-configuration","title":"Post Execution Configuration","text":"<p>Working through this, it looks like the speed cannot be configured as autonegotation is set on the interface. I believe that this is something that is primarily set because of using a virtualized switch platform. I plan to open up a bug report on this soon. We see exactly what we expect in the configuration after the Ansible output. We see interface description configured on each interface, the interface shutdown or enabled. </p> <pre><code>sw19#show run int gig0/1\nBuilding configuration...\n\nCurrent configuration : 129 bytes\n!\ninterface GigabitEthernet0/1\n description First Ansible Configured Interface\n shutdown\n media-type rj45\n negotiation auto\nend\n\nsw19#show run int gig0/2\nBuilding configuration...\n\nCurrent configuration : 120 bytes\n!\ninterface GigabitEthernet0/2\n description Second Ansible Configured Interface\n media-type rj45\n negotiation auto\nend\n</code></pre>"},{"location":"ansible-cisco-ios-interface/#creating-a-loopback-interface","title":"Creating a Loopback Interface","text":"<p>In the second example of the playbook we will create additional Loopback addresses. </p>"},{"location":"ansible-cisco-ios-interface/#pre-change-configuration","title":"Pre-Change Configuration","text":"<p>Here is the output of the <code>show ip int breif</code> of the switch before adding loopbacks.</p> <pre><code>Interface              IP-Address      OK? Method Status                Protocol\nGigabitEthernet0/0     unassigned      YES unset  up                    up      \nGigabitEthernet0/1     unassigned      YES unset  administratively down down    \nGigabitEthernet0/2     unassigned      YES unset  up                    up      \nGigabitEthernet0/3     unassigned      YES unset  up                    up      \nGigabitEthernet1/0     unassigned      YES unset  up                    up      \nGigabitEthernet1/1     unassigned      YES unset  up                    up      \nGigabitEthernet1/2     unassigned      YES unset  up                    up      \nGigabitEthernet1/3     unassigned      YES unset  up                    up      \nLoopback0              10.100.100.100  YES manual up                    up      \nPort-channel5          unassigned      YES unset  down                  down    \nPort-channel6          unassigned      YES unset  down                  down    \nVlan2                  172.16.1.2      YES manual up                    up   \n</code></pre> <p>Here we only see one loopback address, Loopback0.</p>"},{"location":"ansible-cisco-ios-interface/#playbook_1","title":"Playbook","text":"<p>The playbook I'm going to add a loopback interface, but there will not be an address configured on it, you will need to use <code>ios_l3_interface</code> in conjunction with this if using <code>ios_interface</code> for loopbacks. </p> <pre><code>---\n# yamllint disable rule:truthy\n# yamllint disable rule:line-length\n- name: Switch config\n  connection: network_cli\n  hosts: sw19\n  gather_facts: no\n  become: yes\n  become_method: enable\n  tasks:\n    - name: IOS &gt;&gt; VLAN Updates\n      ios_interface:\n        name: Loopback5\n      register: output\n\n    - name: DEBUG &gt;&gt; output\n      debug:\n        msg: \"{{ output }}\"\n</code></pre>"},{"location":"ansible-cisco-ios-interface/#ansible-output_1","title":"Ansible Output","text":"<p>I expect to see the configuration of just creating a loopback address. This is in fact what is seen upon executing the command.</p> <pre><code>PLAY [Switch config] ***********************************************************\n\nTASK [IOS &gt;&gt; VLAN Updates] *****************************************************\nchanged: [sw19]\n\nTASK [DEBUG &gt;&gt; output] *********************************************************\nok: [sw19] =&gt; {\n    \"msg\": {\n        \"changed\": true, \n        \"commands\": [\n            \"interface Loopback5\"\n        ], \n        \"failed\": false\n    }\n}\n\nPLAY RECAP *********************************************************************\nsw19                       : ok=2    changed=1    unreachable=0    failed=0  \n</code></pre>"},{"location":"ansible-cisco-ios-interface/#switch-post-run","title":"Switch Post Run","text":"<p>Now on the switch as expected we see another Loopback address added.</p> <pre><code>sw19#show ip int brie\nInterface              IP-Address      OK? Method Status                Protocol\nGigabitEthernet0/0     unassigned      YES unset  up                    up      \nGigabitEthernet0/1     unassigned      YES unset  administratively down down    \nGigabitEthernet0/2     unassigned      YES unset  up                    up      \nGigabitEthernet0/3     unassigned      YES unset  up                    up      \nGigabitEthernet1/0     unassigned      YES unset  up                    up      \nGigabitEthernet1/1     unassigned      YES unset  up                    up      \nGigabitEthernet1/2     unassigned      YES unset  up                    up      \nGigabitEthernet1/3     unassigned      YES unset  up                    up      \nLoopback0              10.100.100.100  YES manual up                    up      \nLoopback5              unassigned      YES unset  up                    up      \nPort-channel5          unassigned      YES unset  down                  down    \nPort-channel6          unassigned      YES unset  down                  down    \nVlan2                  172.16.1.2      YES manual up                    up   \n</code></pre>"},{"location":"ansible-cisco-ios-interface/#summary","title":"Summary","text":"<p>Earlier in the week I started using this module in a production environment. I had been using just ios_config and moving down into the interface and issuing the <code>shutdown</code> or <code>no shutdown</code> of an interface. After coming across a couple of errors I decided to try the ios_interface module for the playbook. This worked out much better. Digging through this module further with this post I am finding that ios_interface is really good for interface state of up/down and the description of the interface. So you will want to use this in conjunction with the ios_l3_interface and ios_l2_interface to get the complete interface configuration with the modules.</p>"},{"location":"ansible-cisco-ios-interface/#appendix","title":"Appendix","text":""},{"location":"ansible-cisco-ios-interface/#task-with-only-the-required-parameters-loopback10","title":"Task with only the required Parameters (Loopback10)","text":"<p>First taking a look at the play with using a Loopback10 interface. There was previously no Loopback10 interface configured. The play looks like the following:</p>"},{"location":"ansible-cisco-ios-interface/#play","title":"Play","text":"<pre><code>---\n# yamllint disable rule:truthy\n# yamllint disable rule:line-length\n- name: Switch config\n  connection: network_cli\n  hosts: sw19\n  gather_facts: no\n  become: yes\n  become_method: enable\n  tasks:\n    - name: IOS Interface &gt;&gt; Configure Loopback10\n      ios_interface:\n        name: Loopback10\n      register: output\n\n    - name: DEBUG &gt;&gt; output\n      debug:\n        msg: \"{{ output }}\"\n</code></pre>"},{"location":"ansible-cisco-ios-interface/#play-output","title":"Play Output","text":"<p>On the output front, nothing surprising. </p> <p>The commands sent to the device essentially are:</p> <p><code>config t</code> <code>interface Loopback10</code></p> <p>This creates the interface that was not there previously and does not provide any other configuration.  </p> <pre><code>PLAY [Switch config] ***********************************************************\n\nTASK [IOS &gt;&gt; VLAN Updates] *****************************************************\nchanged: [sw19]\n\nTASK [DEBUG &gt;&gt; output] *********************************************************\nok: [sw19] =&gt; {\n    \"msg\": {\n        \"changed\": true, \n        \"commands\": [\n            \"interface Loopback10\"\n        ], \n        \"failed\": false\n    }\n}\n\nPLAY RECAP *********************************************************************\nsw19                       : ok=2    changed=1    unreachable=0    failed=0   \n</code></pre>"},{"location":"ansible-cisco-ios-interface/#task-with-only-the-required-parameters-gigabitethernet01","title":"Task with only the required Parameters (GigabitEthernet0/1)","text":"<p>In this play the interface being configured will be moved from a Loopback interface to one of the physical interfaces on the device. There will once again be no parameters.</p>"},{"location":"ansible-cisco-ios-interface/#play_1","title":"Play","text":"<pre><code>---\n# yamllint disable rule:truthy\n# yamllint disable rule:line-length\n- name: Switch config\n  connection: network_cli\n  hosts: sw19\n  gather_facts: no\n  become: yes\n  become_method: enable\n  tasks:\n    - name: IOS Interface &gt;&gt; Configure Gig0/1\n      ios_interface:\n        name: GigabitEthernet0/1\n      register: output\n\n    - name: DEBUG &gt;&gt; output\n      debug:\n        msg: \"{{ output }}\"\n</code></pre>"},{"location":"ansible-cisco-ios-interface/#output","title":"Output","text":"<p>The Ansible module appears to check the running configuration as expected before stepping through. The output shows no commands being applied as the configuration on the interface already has the desired configuration (blank).</p> <p>Pre-Configuration</p> <pre><code>#show run int gig0/1\nBuilding configuration...\n\nCurrent configuration : 71 bytes\n!\ninterface GigabitEthernet0/1\n media-type rj45\n negotiation auto\nend\n</code></pre> <p>There are no pieces that need to be configured, so the output from the playbook execution is below. With no other parameters defined the module does nothing.</p> <p>Play Execution</p> <pre><code>PLAY [Switch config] ***********************************************************\n\nTASK [IOS &gt;&gt; VLAN Updates] *****************************************************\nok: [sw19]\n\nTASK [DEBUG &gt;&gt; output] *********************************************************\nok: [sw19] =&gt; {\n    \"msg\": {\n        \"changed\": false, \n        \"commands\": [], \n        \"failed\": false\n    }\n}\n\nPLAY RECAP *********************************************************************\nsw19                       : ok=2    changed=0    unreachable=0    failed=0   \n</code></pre>"},{"location":"ansible-saving-cisco-configs-ios/","title":"Ansible Saving Cisco Configs to NVRAM with Cisco Specific Modules","text":"<p>Today I'm going to take a look at a method to be able to save the configuration of a Cisco device to NVRAM (copy run start). I will be taking a look at multiple Cisco platforms to save changes done during an Ansible Playbook to NVRAM. There are options to save the configuration on every change within the modules such as ios_config or cli_config, however, this can slow down the execution of your playbook.</p> <p>First I will take a look at saving the configuration within its own task just for saving configuration. This is how I have many of my playbooks as I'm executing several tasks, breaking them out. I then have a dedicated task that will save the configuration. I do this for speed of the playbook execution. If there are multiple changes on tasks and each one of the tasks is saving the configuration, then there will be some significant time spent saving the configuration multiple times. In this I will take a look at the copy command execution, but also, the trick I like to use of the config module and just save_when parameter.</p> <p>The second methodology I will take a look at is the saving the configuration within the task itself, using the save_when parameter. This is something that I use when I have simple playbooks, with only a couple of tasks that will modify the configuration.</p> <p>We will be taking a look at how to do this with the following Cisco platforms:</p> <ul> <li>Cisco IOS</li> <li>Cisco NXOS</li> <li>Cisco WLC</li> </ul>"},{"location":"ansible-saving-cisco-configs-ios/#saving-configuration-in-one-task","title":"Saving Configuration in one task","text":""},{"location":"ansible-saving-cisco-configs-ios/#ios-nxos","title":"IOS / NXOS","text":"<p>The method I like to use to save the configuration is to use the ios_config module with the parameter of save_when set to always. No other parameter set, just the save_when. The nxos_config and ios_config modules both work in the same way. I'll show both of the outputs, but no separate write up on the NXOS side.</p> <p>To save the configuration then here is the task:</p>"},{"location":"ansible-saving-cisco-configs-ios/#playbook-definition","title":"Playbook Definition","text":"<p>IOS</p> <pre><code>---\n# yamllint disable rule:truthy\n# yamllint disable rule:line-length\n- name: Switch config\n  connection: network_cli\n  hosts: rtr02\n  gather_facts: no\n  become: yes\n  become_method: enable\n  tasks:\n    - name: IOS &gt;&gt; Save Configuration to NVRAM\n      ios_config:\n        save_when: always\n      register: output\n\n    - name: DEBUG &gt;&gt; output\n      debug:\n        msg: \"{{ output }}\"\n</code></pre> <p>NXOS </p> <pre><code>---\n# yamllint disable rule:truthy\n# yamllint disable rule:line-length\n- name: Switch config\n  connection: network_cli\n  hosts: nxos_switches\n  gather_facts: no\n  tasks:\n    - name: NXOS &gt;&gt; Save Configuration to NVRAM\n      nxos_config:\n        save_when: always\n      register: output\n\n    - name: DEBUG &gt;&gt; output\n      debug:\n        msg: \"{{ output }}\"\n</code></pre>"},{"location":"ansible-saving-cisco-configs-ios/#playbook-execution","title":"Playbook Execution","text":"<p>On the output from the playbook, you don't get a lot of feedback that the config is copied other than the task being successful. However, since I'm using a vIOS image in my lab environment the console does show GRUB messages. The second output shows the successful saving of the configuration that was done as the Ansible Playbook was being executed.</p> <p>Playbook Execution - IOS</p> <pre><code>PLAY [Switch config] ***********************************************************\n\nTASK [IOS &gt;&gt; Save Configuration to NVRAM] **************************************\nchanged: [rtr02]\n\nTASK [DEBUG &gt;&gt; output] *********************************************************\nok: [rtr02] =&gt; {\n    \"msg\": {\n        \"changed\": true, \n        \"failed\": false\n    }\n}\n\nPLAY RECAP *********************************************************************\nrtr02                      : ok=2    changed=1    unreachable=0    failed=0  \n</code></pre> <p>GRUB Output - IOS</p> <pre><code>*Mar 30 16:51:15.832: %GRUB-5-CONFIG_WRITING: GRUB configuration is being updated on disk. Please wait...\n*Mar 30 16:51:16.446: %GRUB-5-CONFIG_WRITTEN: GRUB configuration was written to disk successfully\n</code></pre> <p>NXOS Execution</p> <p>Here the output is minimal, with the output reporting success as our only method to know that the configuration was in fact saved.</p> <pre><code>PLAY [Switch config] ***********************************************************\n\nTASK [NXOS &gt;&gt; Save Configuration to NVRAM] *************************************\nchanged: [nxos01]\n\nTASK [DEBUG &gt;&gt; output] *********************************************************\nok: [nxos01] =&gt; {\n    \"msg\": {\n        \"changed\": true, \n        \"failed\": false\n    }\n}\n\nPLAY RECAP *********************************************************************\nnxos01                     : ok=2    changed=1    unreachable=0    failed=0 \n</code></pre>"},{"location":"ansible-saving-cisco-configs-ios/#method-with-ios_command","title":"Method with IOS_Command","text":"<p>Not recommended, but if you must.  </p> <p>Here there are two options for using a dedicated task for saving the configuration. The second method requires specific configuration to be added, which is something that I don't really like to have to do. You would use the configuration <code>file prompt quiet</code> within the configuratoin of the IOS device, then you can use the ios_command module to issue <code>copy run start</code>.</p>"},{"location":"ansible-saving-cisco-configs-ios/#cisco-wlc-save-configuration-aireos_command","title":"Cisco WLC - Save Configuration aireos_command","text":"<p>I have not been able to test the methodology of using save_when on the Cisco Wireless Controllers yet, but the methodology that I have used is using the command module. The difficulty on the aireos_command module is that there really isn't a methodology to handle prompts yet. This is  actually very easy to overcome however on the module using escape characters.</p> <p>Here you see a <code>\\r</code> in the middle of the output before the response of <code>y</code> for do you wish to save. You do not need one on the end as there is an implicit carriage return at the end of any line within the modules.</p> <p>Using the aireos_command module task looks like this:</p> <pre><code>- name: WLC &gt;&gt; Save Configuration\n  aireos_command:\n    commands:\n      - \"save config\\ry\"\n</code></pre>"},{"location":"ansible-saving-cisco-configs-ios/#saving-configurations-on-each-task","title":"Saving Configurations on Each Task","text":"<p>When looking at the four modules of ios_config, nxos_config, cli_config, and aireos_config you will find that there is an parameter for either save or save_when. The parameter save is something that is being deprecated and I would not recommend using this. All of these modules will flag deprecation warnings on Ansible version 2.7.</p> Module Deprecated Parameter  Move Away Current Save Parameter ios_config save (yes, no) save_when (always, never, modified, changed) nxos_config save (yes, no) save_when (always, never, modified, changed) aireos_config save (yes, no) save_when (always, never, modified, changed) <p>Note: I do not actively have a Cisco Wireless Controller available in my lab at the time of the  writing. From working in my production environment, the configuration being save follows closely to that of the ios_config or nxos_config modules.</p>"},{"location":"ansible-saving-cisco-configs-ios/#module-details-links","title":"Module Details Links","text":"<p>aireos_config ios_config nxos_config </p>"},{"location":"ansible-saving-cisco-configs-ios/#parameter-choices","title":"Parameter Choices","text":"<ul> <li>always  </li> <li>never  </li> <li>modified  </li> <li>changed  </li> </ul>"},{"location":"ansible-saving-cisco-configs-ios/#choices-detail","title":"Choices Detail","text":"<p>When changes are made to the device running-configuration, the changes are not copied to non-volatile storage by default. Using this argument will change that before. If the argument is set to always, then the running-config will always be copied to the startup-config and the modified flag will always be set to True. If the argument is set to modified, then the running-config will only be copied to the startup-config if it has changed since the last save to startup-config. If the argument is set to never, the running-config will never be copied to the startup-config. If the argument is set to changed, then the running-config will only be copied to the startup-config if the task has made a change. changed was added in Ansible 2.6.</p>"},{"location":"ansible-saving-cisco-configs-ios/#ios_config-save_when-parameter","title":"ios_config save_when Parameter","text":"<p>This is straight forward, when you want to save the configuration after a change within each task then you would set the parameter save_when to changed. This is advantages on simple playbooks. Next we will take a look at a playbook with the task set to changed.</p>"},{"location":"ansible-saving-cisco-configs-ios/#playbook-definition_1","title":"Playbook Definition","text":"<p>At the start, I have modified the hostname of the router that I'm going to be working on. I have set the hostname to <code>router02</code> which you can see with the prompt being <code>router02#</code> once entering enable mode. This playbook we will be changing the hostname of hte device to match that which is in the Ansible inventory file.</p> <p>Here is the playbook</p> <pre><code>---\n# yamllint disable rule:truthy\n# yamllint disable rule:line-length\n- name: Switch config\n  connection: network_cli\n  hosts: rtr02\n  gather_facts: no\n  become: yes\n  become_method: enable\n  tasks:\n    - name: IOS &gt;&gt; Set hostname\n      ios_config:\n        lines:\n          - hostname {{ inventory_hostname }}\n        save_when: changed\n      register: output\n\n    - name: DEBUG &gt;&gt; output\n      debug:\n        msg: \"{{ output }}\"\n</code></pre> <p>When looking at the startup configuration on the router this is what we have:</p> <pre><code>router02#show start | i hostname\nhostname router02\n</code></pre>"},{"location":"ansible-saving-cisco-configs-ios/#playbook-execution_1","title":"Playbook Execution","text":"<p>We see that the configuration was updated with the command:</p> <pre><code>hostname rtr02\n</code></pre> <pre><code>PLAY [Switch config] ***********************************************************\n\nTASK [IOS &gt;&gt; Set hostname] *****************************************************\nchanged: [rtr02]\n\nTASK [DEBUG &gt;&gt; output] *********************************************************\nok: [rtr02] =&gt; {\n    \"msg\": {\n        \"banners\": {}, \n        \"changed\": true, \n        \"commands\": [\n            \"hostname rtr02\"\n        ], \n        \"failed\": false, \n        \"updates\": [\n            \"hostname rtr02\"\n        ]\n    }\n}\n\nPLAY RECAP *********************************************************************\nrtr02                      : ok=2    changed=1    unreachable=0    failed=0   \n</code></pre> <p>After the execution we have the startup configuration with the new name, just as we expected</p> <pre><code>rtr02#show start | i hostname\nhostname rtr02\n</code></pre>"},{"location":"ansible-saving-cisco-configs-ios/#second-execution-of-the-ios_config-module","title":"Second Execution of the ios_config module","text":"<p>I'm going to run the same playbook once again, to show that the module has the smarts to not change the configuration since it is set. Note the changed output is set to false.</p> <pre><code>PLAY [Switch config] ***********************************************************\n\nTASK [IOS &gt;&gt; Set hostname] *****************************************************\nok: [rtr02]\n\nTASK [DEBUG &gt;&gt; output] *********************************************************\nok: [rtr02] =&gt; {\n    \"msg\": {\n        \"changed\": false, \n        \"failed\": false\n    }\n}\n\nPLAY RECAP *********************************************************************\nrtr02                      : ok=2    changed=0    unreachable=0    failed=0  \n</code></pre>"},{"location":"ansible-saving-cisco-configs-ios/#nxos_config-save_when-parameter","title":"nxos_config save_when Parameter","text":"<p>Similar to the ios_config here is a run through of the same set of plays this time with a NXOS device.</p>"},{"location":"ansible-saving-cisco-configs-ios/#playbook-definition_2","title":"Playbook Definition","text":"<p>Once again, I have the NXOS device hostname set to something different than we would like.</p> <p>The startup configuration has the hostname of <code>nxos_switch1</code> but the Ansible inventory has the name <code>nxos01</code> for the inventory_name. </p> <pre><code>nxos_switch1# show start | i hostname\nhostname nxos_switch1\n</code></pre> <p>THe playbook now looks like this:</p> <pre><code>---\n# yamllint disable rule:truthy\n# yamllint disable rule:line-length\n- name: Switch config\n  connection: network_cli\n  hosts: nxos_switches\n  gather_facts: no\n  tasks:\n    - name: NXOS &gt;&gt; Set hostname\n      nxos_config:\n        lines:\n          - hostname {{ inventory_hostname }}\n        save_when: changed\n      register: output\n\n    - name: DEBUG &gt;&gt; output\n      debug:\n        msg: \"{{ output }}\"\n</code></pre>"},{"location":"ansible-saving-cisco-configs-ios/#playbook-execution_2","title":"Playbook Execution","text":"<p>Just as before we have the hostname change to match that of the Ansible inventory. </p> <pre><code>PLAY [Switch config] ***********************************************************\n\nTASK [NXOS &gt;&gt; Set hostname] ****************************************************\nchanged: [nxos01]\n\nTASK [DEBUG &gt;&gt; output] *********************************************************\nok: [nxos01] =&gt; {\n    \"msg\": {\n        \"changed\": true, \n        \"commands\": [\n            \"hostname nxos01\"\n        ], \n        \"failed\": false, \n        \"updates\": [\n            \"hostname nxos01\"\n        ]\n    }\n}\n\nPLAY RECAP *********************************************************************\nnxos01                     : ok=2    changed=1    unreachable=0    failed=0   \n</code></pre> <p>The show start shows that the startup configuration was changed.</p> <pre><code>nxos01# show start | i hostname\nhostname nxos_switch1\n</code></pre>"},{"location":"ansible-saving-cisco-configs-ios/#summary","title":"Summary","text":"<p>There are multiple methods available for saving the configuration to NVRAM in the Cisco world. The first method works very well when you have a large number of tasks and always wish to have the startup configuration match that of the running configuration from playbook execution. If there are small/simple playbooks with only one or two configurations being applied, it may be worth it to have the task save the configuration. </p> <p>Hope this was helpful! </p>"},{"location":"ansible-asa-command/","title":"Ansible ASA Command Module","text":"<p>Today will be a touch shorter post, but it is good to be back at it. In this post I will be taking a quick look around at the asa_command module, as we start down the path with looking at the ASA modules in Ansible. This is spurned on a little bit by Ansible 2.8 coming out with an Object Group specific module. I will be looking into that further in a future post.</p> <p>For the set of posts regarding the ASA, we will be starting with a pretty bare configuration on the device. We will have just a management IP address and the ability to SSH to the device.</p>"},{"location":"ansible-asa-command/#module-documentation","title":"Module Documentation","text":"<p>Module documentation page can be found here.</p>"},{"location":"ansible-asa-command/#lab-configuration","title":"Lab Configuration","text":"<p>The device has bare basic configuration on it. Here we see that it has just a management IP address on it.</p> <pre><code>fw01# show int ip brie\nInterface                  IP-Address      OK? Method Status                Protocol\nGigabitEthernet0/0         unassigned      YES unset  administratively down up  \nGigabitEthernet0/1         unassigned      YES unset  administratively down up  \nGigabitEthernet0/2         unassigned      YES unset  administratively down up  \nManagement0/0              172.16.0.254    YES CONFIG up                    up \n</code></pre>"},{"location":"ansible-asa-command/#using-the-playbook","title":"Using the playbook","text":""},{"location":"ansible-asa-command/#parameters","title":"Parameters","text":"<p>There are a couple of key parameters on this module for getting started are:</p> <ul> <li>commands: A list of commands to send to the device; this can be one, or  several commands within a list</li> <li>context: used for firewalls in multi-context mode, which context do you want to run the command(s) in</li> </ul>"},{"location":"ansible-asa-command/#simple-first-playbook","title":"Simple first Playbook","text":"<p>This is a simple playbook that will issue two commands. We will access both of them in different tasks within the play. Taking a look at the play we are executing the task with two commands, a <code>show int ip brie</code> and a ping to Google DNS. </p>"},{"location":"ansible-asa-command/#playbook","title":"Playbook","text":"<pre><code>---\n# yamllint disable rule:truthy\n# yamllint disable rule:line-length\n- name: ASA Command Output\n  connection: network_cli\n  hosts: asa_firewalls\n  gather_facts: no\n  become: yes\n  become_method: enable\n  tasks:\n    - name: \"TASK 1: Read output from ASA\"\n      asa_command:\n        commands:\n          - show int ip brief\n          - ping 8.8.8.8\n      register: output\n\n    - name: \"TASK 2: Print output of show interfaces\"\n      debug:\n        msg: \"{{ output.stdout_lines.0 }}\"\n\n    - name: \"TASK 3: Print output of pinging Google DNS\"\n      debug:\n        msg: \"{{ output.stdout_lines.1 }}\"\n</code></pre>"},{"location":"ansible-asa-command/#tasks-high-level","title":"Tasks High Level","text":"<p>TASK 1 is when Ansible logs into the device and issues the two commands. TASK 2 we get the expected output of the <code>show int ip brie</code> and the commands TASK 3 we see that the device is able to successfully ping Google DNS</p> <p>These are the tasks that are to be run via the playbook broken out:</p> <pre><code>cat asa_command_demo.yml | grep TASK\n    - name: \"TASK 1: Read output from ASA\"\n    - name: \"TASK 2: Print output of show interfaces\"\n    - name: \"TASK 3: Print output of pinging Google DNS\"\n</code></pre>"},{"location":"ansible-asa-command/#playbook-run","title":"Playbook Run","text":"<p>Execution of the playbook:</p> <p>To see a video of this on Youtube - https://youtu.be/Wk-3Zg08oSw</p> <pre><code>PLAY [ASA Command Output] *********************************************************************\n\nTASK [TASK 1: Read output from ASA] ***********************************************************\nok: [asa1]\n\nTASK [TASK 2: Print output of show interfaces] ************************************************\nok: [asa1] =&gt; {\n    \"msg\": [\n        \"Interface                  IP-Address      OK? Method Status                Protocol\",\n        \"GigabitEthernet0/0         unassigned      YES unset  administratively down up  \",\n        \"GigabitEthernet0/1         unassigned      YES unset  administratively down up  \",\n        \"GigabitEthernet0/2         unassigned      YES unset  administratively down up  \",\n        \"Management0/0              172.16.0.254    YES CONFIG up                    up\"\n    ]\n}\n\nTASK [TASK 3: Print output of pinging Google DNS] *********************************************\nok: [asa1] =&gt; {\n    \"msg\": [\n        \"Type escape sequence to abort.\",\n        \"Sending 5, 100-byte ICMP Echos to 8.8.8.8, timeout is 2 seconds:\",\n        \"!!!!!\",\n        \"Success rate is 100 percent (5/5), round-trip min/avg/max = 20/104/190 ms\"\n    ]\n}\n\nPLAY RECAP ************************************************************************************\nasa1                       : ok=3    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n</code></pre>"},{"location":"ansible-asa-command/#access-multiple-commands","title":"Access Multiple Commands","text":"<p>This is another example of how to issue multiple commands against a device within a single task. For a deeper dive on that you can see an earlier post here.</p>"},{"location":"ansible-asa-command/#summary","title":"Summary","text":"<p>This is a solid starting out module for working with ASA firewalls. It does come in very handy with dealing and gathering information from the ASA firewall platform. I have used this for several things within a production environment, primarily for data gathering. Hopefully coming up I will be able to expand on this further in building out an ASA firewall.</p> <p>Hope this was helpful! </p>"},{"location":"ansible-asa-og/","title":"Ansible ASA OG Module","text":"<p>Today we are taking a look at the newest module out for Cisco ASA Ansible module - asa_og. This one is particularly exciting for the configurations that are being managed heavily with Object Groups on firewalls. I'm particularly excited to review the asa_og module, time to dig in.  </p> <p>New in this post is the finished playbooks being added to Github. I'm hoping that this may be helpful and I am uploading the contents to Github for more to be able to see and get access to if necessary. This will improve as I continue.  </p> <p>https://github.com/jvanderaa/ansible-asa_work</p> <p>Note</p> <p>When working with this module there is not an option to save the configuration available with it. Please remember this in your playbook logic. If needing to save the configuration there are options. Take a look for samples on my previous post Saving Configurations which does not include an example with ASAs yet. I will have to write a follow up post on this at which point I will update hte link and content.</p>"},{"location":"ansible-asa-og/#parameters","title":"Parameters","text":"<p>First we are going to take a look at the particular parameters to get started, and what our options are for them. The items in bold are the ones that are required by the module.</p> <ul> <li>description: Description for the object group, good for documenting the purpose</li> <li>group_object: This is a list for items within the group</li> <li>group_type: network-object, service-object, or port-object</li> <li>host_ip: List of host addresses within the object group</li> <li>ip_mask: List of IPs and masks for use in object groups</li> <li>name: Name of the object group</li> <li>port_eq: Single port for port-object</li> <li>port_range: Range for a port-object</li> <li>protocol: UDP/TCP/TCP-UDP</li> <li>service_cfg: Service object configuration protocol, direction, range or port</li> <li>state: present/absent/replace to manage the state of the object</li> </ul> <p>For the straight forward parameters this seems like it is something that will be very handy to use.  </p>"},{"location":"ansible-asa-og/#starting-lab-setup","title":"Starting Lab Setup","text":"<p>For this module we are starting with an effectively blank configuration on the firewalls. In future posts I will come back to this and show complete firewall policy management using Ansible.</p> <p>So the lab looks very much the same at the moment as some of the other posts, which is below:  </p> <p> </p> <p>Sample goals: - Create an object group for internal addresses (RFC1918) - Create an object group for external DNS services (Google DNS, Cloudflare, Quad9) - Create a service group for DNS and NTP services - Verify than a service group for just NTP_ONLY does not exist</p>"},{"location":"ansible-asa-og/#leveraging-ansible-og-for-creating-the-modules","title":"Leveraging Ansible OG for creating the modules","text":""},{"location":"ansible-asa-og/#creating-rfc1918-group","title":"Creating RFC1918 Group","text":"<p>Let's tackle the first item. First to show the configuration on the ASA firewall for the object group:</p> <pre><code>fw01# show object-group \nfw01# show run | i RFC 1918\n</code></pre> <p>We see that there is not the object group that is desired to be there.  </p> <p>We are going to modify the Playbook created in the last blog post for asa_command and change it to managing our object groups. Starting out it will look like this:</p> <pre><code>---\n# yamllint disable rule:truthy\n# yamllint disable rule:line-length\n- name: ASA OG Working\n  connection: network_cli\n  hosts: asa_firewalls\n  gather_facts: no\n  become: yes\n  become_method: enable\n\n  tasks:\n\n    - name: \"TASK 1: Set RFC1918 Object Group\"\n      asa_og:\n        name: RFC1918_Networks\n        group_type: network-object\n        state: present\n        description: RFC1918 Local Networks\n        ip_mask:\n          - 10.0.0.0 255.0.0.0\n          - 172.16.0.0 255.240.0.0\n          - 192.168.0.0 255.255.0.0\n      register: output\n\n    - name: \"TASK 2: Print output of show interfaces\"\n      debug:\n        msg: \"{{ output }}\"\n</code></pre> <p>Now to run the playbook, the expect that the object group will be on the firewall.  </p>"},{"location":"ansible-asa-og/#playbook-execution","title":"Playbook Execution","text":"<p>The output from the playbook execution gives us exactly what we were looking for. </p> <ul> <li>Task 1: Connects to the ASA and runs the commands, there is a change as the ASA did not have the object group previously</li> <li>Task 2: Output from the previous task shows the commands that were run and the fact that the device was changed.</li> </ul>"},{"location":"ansible-asa-og/#first-run","title":"First Run","text":"<pre><code>PLAY [ASA OG Working] **********************************************************\n\nTASK [TASK 1: Set RFC1918 Object Group] ****************************************\nchanged: [asa1]\n\nTASK [TASK 2: Print output of RFC1918 Object Group] ****************************\nok: [asa1] =&gt; {\n    \"msg\": {\n        \"changed\": true,\n        \"commands\": [\n            \"object-group network RFC1918_Networks\",\n            \"description RFC1918 Local Networks\",\n            \"network-object 10.0.0.0 255.0.0.0\",\n            \"network-object 172.16.0.0 255.240.0.0\",\n            \"network-object 192.168.0.0 255.255.0.0\"\n        ],\n        \"failed\": false\n    }\n}\n\nPLAY RECAP *********************************************************************\nasa1                       : ok=2    changed=1    unreachable=0    failed=0    s\nkipped=0    rescued=0    ignored=0\n</code></pre> <p>Re-running the playbook again, we see that the module is idempotent. Being that we can safely run this continuously and not have any changes unless they are necessary.</p>"},{"location":"ansible-asa-og/#second-run","title":"Second Run","text":"<pre><code>PLAY [ASA OG Working] **********************************************************\n\nTASK [TASK 1: Set RFC1918 Object Group] ****************************************\nok: [asa1]\n\nTASK [TASK 2: Print output of RFC1918 Object Group] ****************************\nok: [asa1] =&gt; {\n    \"msg\": {\n        \"changed\": false,\n        \"commands\": [],\n        \"failed\": false\n    }\n}\n\nPLAY RECAP *********************************************************************\nasa1                       : ok=2    changed=0    unreachable=0    failed=0    s\nkipped=0    rescued=0    ignored=0\n</code></pre>"},{"location":"ansible-asa-og/#adding-onto-the-previous-playbook-to-add-the-second-group","title":"Adding onto the previous playbook to add the second group","text":"<p>Continuing within this playbook we will create the second object group that will be used, the external DNS providers will be added as host objects to a new group for <code>EXTERNAL_DNS_NTP</code>. </p> <p>Let's get straight to the play update. We will create a new task for this second operation and then output the debug summary.</p>"},{"location":"ansible-asa-og/#playbook-setup-adding-host-ip-group","title":"Playbook Setup - Adding Host IP group","text":"<p>We have added the second task to the playbook here, with another debug so we can see when the changes are being made.</p> <pre><code>---\n# yamllint disable rule:truthy\n# yamllint disable rule:line-length\n- name: ASA OG Working\n  connection: network_cli\n  hosts: asa_firewalls\n  gather_facts: no\n  become: yes\n  become_method: enable\n\n  tasks:\n\n    - name: \"TASK 1: Set RFC1918 Object Group\"\n      asa_og:\n        name: RFC1918_Networks\n        group_type: network-object\n        state: present\n        description: RFC1918 Local Networks\n        ip_mask:\n          - 10.0.0.0 255.0.0.0\n          - 172.16.0.0 255.240.0.0\n          - 192.168.0.0 255.255.0.0\n      register: output\n\n    - name: \"TASK 2: Set External DNS/NTP Providers Object Group\"\n      asa_og:\n        name: EXTERNAL_DNS_NTP\n        group_type: network-object\n        state: present\n        description: External DNS Providers (CloudFlare, Google, Quad9, Umbrella)\n        host_ip:\n          - 1.1.1.1\n          - 8.8.8.8\n          - 9.9.9.9\n          - 208.67.222.222\n          - 208.67.220.220\n      register: output2     \n\n    - name: \"DEBUG 1: Print output of RFC1918 Object Group\"\n      debug:\n        msg: \"{{ output }}\"\n\n    - name: \"DEBUG 2: Print output of External DNS Group\"\n      debug:\n        msg: \"{{ output2 }}\"\n</code></pre>"},{"location":"ansible-asa-og/#playbook-execution_1","title":"Playbook Execution","text":"<ul> <li>Task 1: Comes back OK, the object group is as defined and does not need to get updated</li> <li>Task 2: Adds the second object group that we were anticipating adding to the firewall</li> <li>Debug 1: Shows that there was no change by the <code>changed</code> output being set to false</li> <li>Debug 2: Once again shows the <code>changed</code> flag is set to <code>True</code> and the commands executed on the device</li> </ul> <pre><code>PLAY [ASA OG Working] **********************************************************\n\nTASK [TASK 1: Set RFC1918 Object Group] ****************************************\nok: [asa1]\n\nTASK [TASK 2: Set External DNS/NTP Providers Object Group] *********************\nchanged: [asa1]\n\nTASK [DEBUG 1: Print output of RFC1918 Object Group] ***************************\nok: [asa1] =&gt; {\n    \"msg\": {\n        \"changed\": false,\n        \"commands\": [],\n        \"failed\": false\n    }\n}\n\nTASK [DEBUG 2: Print output of External DNS Group] *****************************\nok: [asa1] =&gt; {\n    \"msg\": {\n        \"changed\": true,\n        \"commands\": [\n            \"object-group network EXTERNAL_DNS_NTP\",\n            \"network-object host 1.1.1.1\",\n            \"network-object host 8.8.8.8\",\n            \"network-object host 9.9.9.9\",\n            \"network-object host 208.67.222.222\",\n            \"network-object host 208.67.220.220\",\n            \"description External DNS Providers (CloudFlare, Google, Quad9, Umbr\n            ella)\"\n        ],\n        \"failed\": false\n    }\n}\n\nPLAY RECAP **************************************************************************\nasa1                       : ok=4    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n</code></pre>"},{"location":"ansible-asa-og/#playbook-results-on-asa","title":"Playbook Results on ASA","text":"<p>As expected, we get the new items added to the configuration. When we look at the before and after on the configuration of the ASA we now see that we have the second object group, exactly as we expected.</p> <pre><code>fw01# show object-group\nobject-group network RFC1918_Networks\n description: RFC1918 Local Networks\n network-object 10.0.0.0 255.0.0.0\n network-object 172.16.0.0 255.240.0.0\n network-object 192.168.0.0 255.255.0.0\nfw01#\nfw01#\nfw01# \nfw01# show object-group\nobject-group network RFC1918_Networks\n description: RFC1918 Local Networks\n network-object 10.0.0.0 255.0.0.0\n network-object 172.16.0.0 255.240.0.0\n network-object 192.168.0.0 255.255.0.0\nobject-group network EXTERNAL_DNS_NTP\n description: External DNS Providers (CloudFlare, Google, Quad9, Umbrella)\n network-object host 1.1.1.1\n network-object host 8.8.8.8\n network-object host 9.9.9.9\n network-object host 208.67.222.222\n network-object host 208.67.220.220\n</code></pre>"},{"location":"ansible-asa-og/#adding-on-the-port-group","title":"Adding on the Port Group","text":"<p>Now we need to complete the setup by adding a port group to the playbook so when a policy is built that the hosts can communicate on the specific ports. To start off the policy will use only UDP ports 53 (DNS) and 123 (NTP).</p> <p>Yes DNS is also on TCP/53, but for this we will stick to only the UDP side for non large requests.</p> <p>Issue: When working on this I came across an issue with the asa_og module and my particular setup (Python3.7.2) with respects to Ansible 2.8. The concatenation engine would error out combining strings (the actual commands) and integers. The work around on this that you see in the playbook is that the ports are surrounded by quotes. This makes them strings instead of integers and the module works. I have opened an issue on github for this. https://github.com/ansible/ansible/issues/58258 if you wish to check on the status.</p>"},{"location":"ansible-asa-og/#playbook-adding-in-port-object-group-creation","title":"Playbook - Adding in port-object group creation","text":"<p>As expected there is a third task now that will be for creating the port object. From the module parameters we are now using parameters of <code>protocol</code> and <code>port_eq</code>. These are expected parameters for creating a port group.</p>"},{"location":"ansible-asa-og/#playbook-task-design-port-group","title":"Playbook Task Design - Port Group","text":"<p>Here is the task that is added with the group-type set to <code>port-object</code>:  </p> <pre><code>    - name: \"TASK 3: Add Port Group\"\n      asa_og:\n        name: SVC_OBJ_DNS_NTP\n        group_type: port-object\n        state: present\n        description: DNS and NTP ports\n        protocol: udp\n        port_eq:\n          - 53\n          - 123\n      register: output3\n</code></pre> <p>This brings the full playbook to looking like this:  </p> <pre><code>---\n# yamllint disable rule:truthy\n# yamllint disable rule:line-length\n- name: ASA OG Working\n  connection: network_cli\n  hosts: asa_firewalls\n  gather_facts: no\n  become: yes\n  become_method: enable\n\n  tasks:\n\n    - name: \"TASK 1: Set RFC1918 Object Group\"\n      asa_og:\n        name: RFC1918_Networks\n        group_type: network-object\n        state: present\n        description: RFC1918 Local Networks\n        ip_mask:\n          - 10.0.0.0 255.0.0.0\n          - 172.16.0.0 255.240.0.0\n          - 192.168.0.0 255.255.0.0\n      register: output\n\n    - name: \"TASK 2: Set External DNS/NTP Providers Object Group\"\n      asa_og:\n        name: EXTERNAL_DNS_NTP\n        group_type: network-object\n        state: present\n        description: External DNS Providers (CloudFlare, Google, Quad9, Umbrella)\n        host_ip:\n          - 1.1.1.1\n          - 8.8.8.8\n          - 9.9.9.9\n          - 208.67.222.222\n          - 208.67.220.220\n      register: output2\n\n    - name: \"TASK 3: Add Port Group\"\n      asa_og:\n        name: SVC_OBJ_DNS_NTP\n        group_type: port-object\n        state: present\n        description: DNS and NTP ports\n        protocol: udp\n        port_eq:\n          - 53\n          - 123\n      register: output3\n\n    - name: \"DEBUG 1: Print output of RFC1918 Object Group\"\n      debug:\n        msg: \"{{ output }}\"\n\n    - name: \"DEBUG 2: Print output of External DNS Group\"\n      debug:\n        msg: \"{{ output2 }}\"\n\n    - name: \"DEBUG 3: Print output of adding Port Group\"\n      debug:\n        msg: \"{{ output3 }}\"\n\n...\n</code></pre>"},{"location":"ansible-asa-og/#playbook-execution_2","title":"Playbook Execution","text":"<p>Task 1: Reports OK, as there are no changes here - as expected Task 2: Also reports OK, as there should be no changes - as expected Task 3: Creates the port-object to be used Debug 1: Shows no changes Debug 2: Shows no changes Debug 3: Shows that the changed flag is set to <code>True</code> and that the changes sent to the device creates the port object</p> <pre><code>PLAY [ASA OG Working] **********************************************************\n\nTASK [TASK 1: Set RFC1918 Object Group] ****************************************\nok: [asa1]\n\nTASK [TASK 2: Set External DNS/NTP Providers Object Group] *********************\nok: [asa1]\n\nTASK [TASK 3: Add Port Group] **************************************************\nchanged: [asa1]\n\nTASK [DEBUG 1: Print output of RFC1918 Object Group] ***************************\nok: [asa1] =&gt; {\n    \"msg\": {\n        \"changed\": false,\n        \"commands\": [],\n        \"failed\": false\n    }\n}\n\nTASK [DEBUG 2: Print output of External DNS Group] *****************************\nok: [asa1] =&gt; {\n    \"msg\": {\n        \"changed\": false,\n        \"commands\": [],\n        \"failed\": false\n    }\n}\n\nTASK [DEBUG 3: Print output of adding Port Group] ******************************\nok: [asa1] =&gt; {\n    \"msg\": {\n        \"changed\": true,\n        \"commands\": [\n            \"object-group service SVC_OBJ_DNS_NTP udp\",\n            \"port-object eq 53\",\n            \"port-object eq 123\",\n            \"description DNS and NTP ports\"\n        ],\n        \"failed\": false\n    }\n}\n\nPLAY RECAP *********************************************************************\nasa1                       : ok=6    changed=1    unreachable=0    failed=0    s\nkipped=0    rescued=0    ignored=0\n</code></pre>"},{"location":"ansible-asa-og/#changes-on-the-asa","title":"Changes on the ASA","text":"<p>And as we are use to seeing, we see the update on the ASA itself:</p> <pre><code>fw01# show object-group\nobject-group network RFC1918_Networks\n description: RFC1918 Local Networks\n network-object 10.0.0.0 255.0.0.0\n network-object 172.16.0.0 255.240.0.0\n network-object 192.168.0.0 255.255.0.0\nobject-group network EXTERNAL_DNS_NTP\n description: External DNS Providers (CloudFlare, Google, Quad9, Umbrella)\n network-object host 1.1.1.1\n network-object host 8.8.8.8\n network-object host 9.9.9.9\n network-object host 208.67.222.222\n network-object host 208.67.220.220\nfw01#\nfw01#\nfw01# show object-group\nobject-group network RFC1918_Networks\n description: RFC1918 Local Networks\n network-object 10.0.0.0 255.0.0.0\n network-object 172.16.0.0 255.240.0.0\n network-object 192.168.0.0 255.255.0.0\nobject-group network EXTERNAL_DNS_NTP\n description: External DNS Providers (CloudFlare, Google, Quad9, Umbrella)\n network-object host 1.1.1.1\n network-object host 8.8.8.8\n network-object host 9.9.9.9\n network-object host 208.67.222.222\n network-object host 208.67.220.220\nobject-group service SVC_OBJ_DNS_NTP udp\n description: DNS and NTP ports\n port-object eq domain\n port-object eq ntp\n</code></pre>"},{"location":"ansible-asa-og/#removing-groups","title":"Removing groups","text":"<p>The last thing to demo is the <code>state: absent</code> of the module. What I have seen in testing at the moment is that the module does not delete the group all together, but removes the object members. Let's take a look at this in action.</p>"},{"location":"ansible-asa-og/#setup-deletion","title":"Setup - Deletion","text":"<p>First I went ahead and created a new group (using Ansible of course). This is the old group for DNS that is no longer being used. So we should clean that up of course. This module only deletes items from within the object-group, it will NOT remove an entire object-group.  </p> <p>The firewall configuration has the following for object groups:  </p> <pre><code>object-group network RFC1918_Networks\n description: RFC1918 Local Networks\n network-object 10.0.0.0 255.0.0.0\n network-object 172.16.0.0 255.240.0.0\n network-object 192.168.0.0 255.255.0.0\nobject-group network EXTERNAL_DNS_NTP\n description: External DNS Providers (CloudFlare, Google, Quad9, Umbrella)\n network-object host 1.1.1.1\n network-object host 8.8.8.8\n network-object host 9.9.9.9\n network-object host 208.67.222.222\n network-object host 208.67.220.220\nobject-group service SVC_OBJ_DNS_NTP udp\n description: DNS and NTP ports\n port-object eq domain\n port-object eq ntp\nobject-group service DNS_ONLY udp\n description: DNS ports\n port-object eq domain\n</code></pre>"},{"location":"ansible-asa-og/#task-created-absent-state","title":"Task Created - Absent state","text":"<p>Here is the task with the state changed from <code>present</code> to <code>absent</code>:  </p> <pre><code>    - name: \"TASK 4: Remove Extra Group\"\n      asa_og:\n        name: DNS_ONLY\n        group_type: port-object\n        state: absent\n        protocol: udp\n        port_eq:\n          - domain\n      register: output4\n</code></pre>"},{"location":"ansible-asa-og/#playbook-execution-absent-state-for-an-object-group","title":"Playbook Execution - Absent state for an object group","text":"<p>Task 1 - 3: These are the idempotent adds. There is not any changes being made, so these remain OK. Task 4: Removes the particular item from within a group. So you will need to call out all of the objects that you want to have missing from here. The next task will take a look at another helpful state of replace.  </p> <pre><code>PLAY [ASA OG Working] **********************************************************\n\nTASK [TASK 1: Set RFC1918 Object Group] ****************************************\nok: [asa1]\n\nTASK [TASK 2: Set External DNS/NTP Providers Object Group] *********************\nok: [asa1]\n\nTASK [TASK 3: Add Port Group] **************************************************\nok: [asa1]\n\nTASK [TASK 4: Remove Extra Group] **********************************************\nchanged: [asa1]\n\nTASK [DEBUG 1: Print output of RFC1918 Object Group] ***************************\nok: [asa1] =&gt; {\n    \"msg\": {\n        \"changed\": false,\n        \"commands\": [],\n        \"failed\": false\n    }\n}\n\nTASK [DEBUG 2: Print output of External DNS Group] *****************************\nok: [asa1] =&gt; {\n    \"msg\": {\n        \"changed\": false,\n        \"commands\": [],\n        \"failed\": false\n    }\n}\n\nTASK [DEBUG 3: Print output of adding Port Group] ******************************\nok: [asa1] =&gt; {\n    \"msg\": {\n        \"changed\": false,\n        \"commands\": [],\n        \"failed\": false\n    }\n}\n\nTASK [DEBUG 4: Print output of removing extra Port Group] **********************\nok: [asa1] =&gt; {\n    \"msg\": {\n        \"changed\": true,\n        \"commands\": [\n            \"object-group service DNS_ONLY udp\",\n            \"no port-object eq domain\"\n        ],\n        \"failed\": false\n    }\n}\n\nPLAY RECAP *********************************************************************\nasa1                       : ok=8    changed=1    unreachable=0    failed=0    s\nkipped=0    rescued=0    ignored=0\n</code></pre>"},{"location":"ansible-asa-og/#firewall-after","title":"Firewall After","text":"<p>Here we see that the port object that we asked to remove is gone. If there were other object items in the object-group they would still remain.</p> <pre><code>fw01# show object-group\nobject-group network RFC1918_Networks\n description: RFC1918 Local Networks\n network-object 10.0.0.0 255.0.0.0\n network-object 172.16.0.0 255.240.0.0\n network-object 192.168.0.0 255.255.0.0\nobject-group network EXTERNAL_DNS_NTP\n description: External DNS Providers (CloudFlare, Google, Quad9, Umbrella)\n network-object host 1.1.1.1\n network-object host 8.8.8.8\n network-object host 9.9.9.9\n network-object host 208.67.222.222\n network-object host 208.67.220.220\nobject-group service SVC_OBJ_DNS_NTP udp\n description: DNS and NTP ports\n port-object eq domain\n port-object eq ntp\nobject-group service DNS_ONLY udp\n description: DNS ports\n port-object eq domain\nfw01#\nfw01# ! AFTER THE CHANGE\nfw01# show object-group\nobject-group network RFC1918_Networks\n description: RFC1918 Local Networks\n network-object 10.0.0.0 255.0.0.0\n network-object 172.16.0.0 255.240.0.0\n network-object 192.168.0.0 255.255.0.0\nobject-group network EXTERNAL_DNS_NTP\n description: External DNS Providers (CloudFlare, Google, Quad9, Umbrella)\n network-object host 1.1.1.1\n network-object host 8.8.8.8\n network-object host 9.9.9.9\n network-object host 208.67.222.222\n network-object host 208.67.220.220\nobject-group service SVC_OBJ_DNS_NTP udp\n description: DNS and NTP ports\n port-object eq domain\n port-object eq ntp\nobject-group service DNS_ONLY udp\n description: DNS ports\n</code></pre>"},{"location":"ansible-asa-og/#state-replace","title":"State: Replace","text":"<p>Originally this was not on the radar to include in this post, but I have found it very helpful. What <code>replace</code> will do for you is allow you to set this as the \"standard\". So if there are extraneous items in the object group <code>replace</code> will remove anything extra. If there are any items missing from the object group it will add them in.  </p>"},{"location":"ansible-asa-og/#tasks-replace","title":"Tasks - Replace","text":"<p>I'm going to modify Task 3 from earlier to add some extra ports to the port-group and change domain to dns even though domain is the proper ASA shorthand for port 53.  The two tasks are now this:</p> <pre><code>    - name: \"TASK 3: Add Port Group\"\n      asa_og:\n        name: SVC_OBJ_DNS_NTP\n        group_type: port-object\n        state: present\n        description: DNS and NTP ports\n        protocol: udp\n        port_eq:\n          - \"ntp\"\n          - \"dns\"\n          - \"5353\"\n          - \"553\"\n          - \"353\"\n      register: output3\n\n    - name: \"TASK 5: Fix the DNS Port Group\"\n      asa_og:\n        name: SVC_OBJ_DNS_NTP\n        group_type: port-object\n        state: replace\n        description: DNS and NTP ports\n        protocol: udp\n        port_eq:\n          - \"domain\"\n          - \"ntp\"\n      register: output5\n</code></pre>"},{"location":"ansible-asa-og/#replace-task-execution","title":"Replace Task Execution","text":"<p>Let's get right to looking at the execution based on the summary above.  </p>"},{"location":"ansible-asa-og/#firewall-before","title":"Firewall Before","text":"<p>Here we see that the object group SVC_OBJ_DNS_NTP has a lot more entries than one should expect.  </p> <pre><code>fw01# show object-group\nobject-group network RFC1918_Networks\n description: RFC1918 Local Networks\n network-object 10.0.0.0 255.0.0.0\n network-object 172.16.0.0 255.240.0.0\n network-object 192.168.0.0 255.255.0.0\nobject-group network EXTERNAL_DNS_NTP\n description: External DNS Providers (CloudFlare, Google, Quad9, Umbrella)\n network-object host 1.1.1.1\n network-object host 8.8.8.8\n network-object host 9.9.9.9\n network-object host 208.67.222.222\n network-object host 208.67.220.220\nobject-group service SVC_OBJ_DNS_NTP udp\n description: DNS and NTP ports\n port-object eq domain\n port-object eq ntp\n port-object eq dnsix\n port-object eq 5353\n port-object eq 553\n port-object eq 353\n</code></pre>"},{"location":"ansible-asa-og/#replace-full-playbook","title":"Replace - Full Playbook","text":"<p>The full playbook with all of the debugs and the tasks.  </p> <pre><code>---\n# yamllint disable rule:truthy\n# yamllint disable rule:line-length\n- name: ASA OG Working\n  connection: network_cli\n  hosts: asa_firewalls\n  gather_facts: no\n  become: yes\n  become_method: enable\n\n  tasks:\n\n    - name: \"TASK 1: Set RFC1918 Object Group\"\n      asa_og:\n        name: RFC1918_Networks\n        group_type: network-object\n        state: present\n        description: RFC1918 Local Networks\n        ip_mask:\n          - 10.0.0.0 255.0.0.0\n          - 172.16.0.0 255.240.0.0\n          - 192.168.0.0 255.255.0.0\n      register: output\n\n    - name: \"TASK 2: Set External DNS/NTP Providers Object Group\"\n      asa_og:\n        name: EXTERNAL_DNS_NTP\n        group_type: network-object\n        state: present\n        description: External DNS Providers (CloudFlare, Google, Quad9, Umbrella)\n        host_ip:\n          - 1.1.1.1\n          - 8.8.8.8\n          - 9.9.9.9\n          - 208.67.222.222\n          - 208.67.220.220\n      register: output2\n\n    - name: \"TASK 3: Add Port Group\"\n      asa_og:\n        name: SVC_OBJ_DNS_NTP\n        group_type: port-object\n        state: present\n        description: DNS and NTP ports\n        protocol: udp\n        port_eq:\n          - \"ntp\"\n          - \"dnsix\"\n          - \"5353\"\n          - \"553\"\n          - \"353\"\n      register: output3\n\n    - name: \"TASK 4: Remove Extra Group\"\n      asa_og:\n        name: DNS_ONLY\n        group_type: port-object\n        state: absent\n        protocol: udp\n        port_eq:\n          - domain\n      register: output4\n\n    - name: \"TASK 5: Fix the DNS Port Group\"\n      asa_og:\n        name: SVC_OBJ_DNS_NTP\n        group_type: port-object\n        state: replace\n        description: DNS and NTP ports\n        protocol: udp\n        port_eq:\n          - \"domain\"\n          - \"ntp\"\n      register: output5\n\n    - name: \"DEBUG 1: Print output of RFC1918 Object Group\"\n      debug:\n        msg: \"{{ output }}\"\n\n    - name: \"DEBUG 2: Print output of External DNS Group\"\n      debug:\n        msg: \"{{ output2 }}\"\n\n    - name: \"DEBUG 3: Print output of adding Port Group\"\n      debug:\n        msg: \"{{ output3 }}\"\n\n    - name: \"DEBUG 4: Print output of removing extra Port Group\"\n      debug:\n        msg: \"{{ output4 }}\"\n\n    - name: \"DEBUG 5: Print output of Port Group Replace\"\n      debug:\n        msg: \"{{ output5 }}\"\n...\n</code></pre>"},{"location":"ansible-asa-og/#execution","title":"Execution","text":"<p>Task 1-4: All check out OK, there are no changes being made Task 5: This is that additional task to fix the item properly Debug 1-4: All show no changes Debug 5: Shows that the changes were made, including add <code>domain</code> and removing a bunch of extra items  </p> <pre><code>PLAY [ASA OG Working] **********************************************************\n\nTASK [TASK 1: Set RFC1918 Object Group] ****************************************\nok: [asa1]\n\nTASK [TASK 2: Set External DNS/NTP Providers Object Group] *********************\nok: [asa1]\n\nTASK [TASK 3: Add Port Group] **************************************************\nok: [asa1]\n\nTASK [TASK 4: Remove Extra Group] **********************************************\nok: [asa1]\n\nTASK [TASK 5: Fix the DNS Port Group] ******************************************\nchanged: [asa1]\n\nTASK [DEBUG 1: Print output of RFC1918 Object Group] ***************************\nok: [asa1] =&gt; {\n    \"msg\": {\n        \"changed\": false,\n        \"commands\": [],\n        \"failed\": false\n    }\n}\n\nTASK [DEBUG 2: Print output of External DNS Group] *****************************\nok: [asa1] =&gt; {\n    \"msg\": {\n        \"changed\": false,\n        \"commands\": [],\n        \"failed\": false\n    }\n}\n\nTASK [DEBUG 3: Print output of adding Port Group] ******************************\nok: [asa1] =&gt; {\n    \"msg\": {\n        \"changed\": false,\n        \"commands\": [],\n        \"failed\": false\n    }\n}\n\nTASK [DEBUG 4: Print output of removing extra Port Group] **********************\nok: [asa1] =&gt; {\n    \"msg\": {\n        \"changed\": false,\n        \"commands\": [],\n        \"failed\": false\n    }\n}\n\nTASK [DEBUG 5: Print output of Port Group Replace] *****************************\nok: [asa1] =&gt; {\n    \"msg\": {\n        \"changed\": true,\n        \"commands\": [\n            \"object-group service SVC_OBJ_DNS_NTP udp\",\n            \"port-object eq domain\",\n            \"no port-object eq 553\",\n            \"no port-object eq 353\",\n            \"no port-object eq 5353\",\n            \"no port-object eq dnsix\"\n        ],\n        \"failed\": false\n    }\n}\n\nPLAY RECAP *********************************************************************\nasa1                       : ok=10   changed=1    unreachable=0    failed=0    s\nkipped=0    rescued=0    ignored=0\n</code></pre>"},{"location":"ansible-asa-og/#summary","title":"Summary","text":"<p>This module is a terrific module if you are asked to manage ASA policy. This is very complete and should be part of your toolset for managing ASA devices. I do foresee a significant amount of use out of the <code>state: replace</code> setup in getting object groups to a declared state.  </p> <p>Again, very important as well, do not forget to save your configurations at the end if making changes.  </p> <p>I hope that this has been informative!  </p>"},{"location":"ansible-ios-bgp-module/","title":"Ansible IOS BGP Module","text":"<p>In this post I'm going to be taking a deeper dive into the new in Ansible 2.8 IOS BGP module. This may be one of the more complex modules to date and I'll try to make it as simple as possible.  </p> <p>For a reminder about the BGP protocol is that this is the predominate protocol that runs the Internet. It is used to peer up with other companies and is what helps to make the Internet great. This is a very powerful protocol, and has been expanded to support many things. This is also a protocol that is heavily used in modern data centers.  </p> <p>On this module there are a TON of parameters (OK - 47 parameters). That is going to be too many to list out. If you want to take a look at each and every one of the parameters (which I do recommend doing at times, or at least going to the  examples) check out the link above that takes you to the Ansible documentation.  </p> <p>Let's dive on in.</p> <p>Note: In this I will be working with a \"fixed version\" of the <code>ios_bgp</code> module. In working on the post it was found that <code>next-hop-self</code> was not getting applied when used in the module. This is fixed in a coming release of Ansible. As of 2.8.1 this is still broken. See https://github.com/ansible/ansible/pull/58789 for more information.  </p>","tags":["ansible","cisco","ios_bgp"]},{"location":"ansible-ios-bgp-module/#observations","title":"Observations","text":"<p>A couple of general observations and my take on the module before getting into the lab and demo portions. This module is a great start on simplifying what can be a very complex configuration with BGP. By its nature BGP has a deep and complex configuration because of how flexible and how much has been stuffed into the BGP protocol. It's being used within Data Centers of single tenants! This is not going to be a post about BGP however - you can find plenty of those elsewhere that are more in depth at this point.  </p> <p>This module gets the basics spot on. I'm going to look to leverage this wherever I can. That said however, there are still a few pieces that I haven't been able to figure out how to do with this, and first comes the ISP world. Where there are multiple VRFs configured within BGP. I'm hopeful that this can be expanded in the future to support VRF configuration as well.  </p> <p>All that said, this is a complex module, and has a lot of great standardization to it. Take a look at the module definition in the Ansible docs.</p> <p>A very impressive part to this as well is that the redistribution from multiple protocols is covered within the module. Route maps can be applied on redistributions as well as the network advertisements. It's going to continue to improve!  </p>","tags":["ansible","cisco","ios_bgp"]},{"location":"ansible-ios-bgp-module/#lab-setup","title":"Lab Setup","text":"","tags":["ansible","cisco","ios_bgp"]},{"location":"ansible-ios-bgp-module/#lab-devices","title":"Lab Devices","text":"<p>For this module in particular I went ahead and designed a new lab so we can dig deep into the setup of various methods of BGP. First we are having R2 as the edge of the lab, heading out Gig0/1 towards the Internet. R2 is acting as a single router within an ISP in this instance. R1 is the edge of the virtualized environment which allows me to leverage Ansible from my machine as the control machine. R3 and R4 will be on the edge of the enterprise network, with R5 originating some routes via EIGRP to the routers on R3 and R4.  </p> <p> </p>","tags":["ansible","cisco","ios_bgp"]},{"location":"ansible-ios-bgp-module/#networks","title":"Networks","text":"<p>Routes being advertised by R5 are two /25 networks out of the 203.0.113.0/24 network. All of the addressing in the \"production\" area of this enterprise are using RFC5737 address space. These are:</p> <ul> <li>198.51.100.0/24</li> <li>203.0.113.0/24</li> <li>192.0.2.0/24</li> </ul>","tags":["ansible","cisco","ios_bgp"]},{"location":"ansible-ios-bgp-module/#scenario","title":"Scenario","text":"<p>For demonstration purposes the configuration will be getting done on only R3 from a text perspective. There will however be the modules on the Github page, and in a follow on subsequent video demonstration of the playbook.</p> <pre><code>- name: \"PLAY 1: Get Configuration Backup to verify connectivity\"\n    - name: \"TASK 1: Verify Config Backup\"\n- name: \"PLAY 2: Setup R2\"\n    - name: \"TASK 1: Setup eBGP Peers\"\n- name: \"PLAY 3: Setup R4\"\n    - name: \"TASK 1: Setup BGP Peers\"\n</code></pre> <p>This is going to walk through getting R3 to eBGP peer with R2 as a 3rd party connection, and to R4 as an iBGP peer for internal BGP. This will not work with the internal routing protocols. We assume that these are already all set to go.  </p>","tags":["ansible","cisco","ios_bgp"]},{"location":"ansible-ios-bgp-module/#initial-routing-configuration","title":"Initial Routing Configuration","text":"<p>This is more just to show where we are and that there is nothing configured for BGP on R3.</p> <pre><code>R3#show run | sec bgp\nR3#\n</code></pre>","tags":["ansible","cisco","ios_bgp"]},{"location":"ansible-ios-bgp-module/#first-bgp-neighbor","title":"First BGP Neighbor","text":"<p>The first BGP neighbor we should bring up is within the same AS. Let's make sure that we are able to get BGP going to that within your same autonomous system and control before bringing up an exterior peer. In this example we will be building a BGP peer to 198.51.100.2 with the AS65500.  </p>","tags":["ansible","cisco","ios_bgp"]},{"location":"ansible-ios-bgp-module/#playbook-start-add-ibgp-peer","title":"Playbook Start - Add iBGP peer","text":"<p>Getting started with building the internal BGP connection the task does get a touch lengthy, so leveraging copy and paste and finding the fields with the help of the module documentation this is the play to build that iBGP neighbor:</p> <pre><code>- name: \"PLAY 1: Setup iBGP Peer to R4\"\n  connection: network_cli\n  hosts: r3\n  become: yes\n  become_method: enable\n  tasks:\n    - name: \"TASK 1: Setup iBGP Peer\"\n      ios_bgp:\n        config:\n          bgp_as: 65500\n          router_id: 10.0.0.3\n          log_neighbor_changes: true\n          neighbors:\n            - neighbor: 198.51.100.2\n              remote_as: 65500\n              activate: true\n              timers:\n                keepalive: 15\n                holdtime: 45\n                min_neighbor_holdtime: 5\n              description: R4\n          networks:\n            - prefix: 198.51.100.0\n              masklen: 24\n            - prefix: 203.0.113.0\n              masklen: 24\n          address_family:\n            - afi: ipv4\n              safi: unicast\n              neighbors:\n                - neighbor: 198.51.100.2\n                  activate: yes\n                  next_hop_self: yes\n        operation: merge\n      register: ibgp_peer1\n\n    - name: \"TASK 2: Debug output\"\n      debug:\n        msg: \"{{ ibgp_peer1 }}\"\n</code></pre> <p>Lines 10, 11, 12 are very common on the BGP configuration. Let's walk through some of these.</p> <p>Line 10: Sets the locally running BGP AS on the router Line 11: Sets the router-id to be used for the BGP process Line 12: Sets logging of neighbor changes to true Lines 13-21: Sets configuration of neighbor detail, outside of the address family Lines 22-26: Are used for what networks we want to advertise from BGP Lines 27-33: Used for BGP address family configuration updates Lines 34-36: Identify our neighbor within the address family Line 37: The operation style from Merge, Replace, Override or Delete  </p>","tags":["ansible","cisco","ios_bgp"]},{"location":"ansible-ios-bgp-module/#playbook-ibgp-execution","title":"Playbook iBGP - Execution","text":"<pre><code>- name: \"PLAY 1: Setup iBGP Peer to R4\"\n    - name: \"TASK 1: Setup iBGP Peer\"\n    - name: \"TASK 2: Debug output\"\n</code></pre> <p>There are 2 tasks so we can see the output. The first task is to setup an iBGP peer. We get to see the output on the second task.  </p> <p>Task 2 output has all of the router configurations that are going to be applied. Before the change there are no neighbors established. On the console there is an immediate neighbor established on the iBGP side of things with this configuration.</p> <pre><code>PLAY [PLAY 1: Setup iBGP Peer to R4] *******************************************\n\nTASK [TASK 1: Setup iBGP Peer] *************************************************\nchanged: [r3]\n\nTASK [TASK 2: Debug output] ****************************************************\nok: [r3] =&gt; {\n    \"msg\": {\n        \"changed\": true,\n        \"commands\": [\n            \"router bgp 65500\",\n            \"bgp router-id 10.0.0.3\",\n            \"bgp log-neighbor-changes\",\n            \"neighbor 198.51.100.2 remote-as 65500\",\n            \"neighbor 198.51.100.2 timers 15 45 5\",\n            \"neighbor 198.51.100.2 description R4\",\n            \"network 198.51.100.0 mask 255.255.255.0\",\n            \"network 203.0.113.0 mask 255.255.255.128\",\n            \"address-family ipv4\",\n            \"no auto-summary\",\n            \"neighbor 198.51.100.2 activate\",\n            \"neighbor 198.51.100.2 next-hop-self\",\n            \"exit-address-family\",\n            \"exit\"\n        ],\n        \"failed\": false\n    }\n}\n\nPLAY RECAP *********************************************************************\nr3                         : ok=2    changed=1    unreachable=0    failed=0    s\nkipped=0    rescued=0    ignored=0\n</code></pre> <p>Pretty straight to the point, that we have a complete BGP configuration getting deployed. A second run of the playbook should be idempotent, however, when executing the <code>show run</code> to get the configuration of the device and the network is subnetted on its proper class boundary the Ansible playbook will re-execute the command. </p> <p>In working on this I have opened up a bug report on the module to see if this can be made idempotent. See Github Ansible Issue #59083</p>","tags":["ansible","cisco","ios_bgp"]},{"location":"ansible-ios-bgp-module/#second-neighbor-the-isp-connection","title":"Second neighbor - the ISP connection","text":"<p>Let's get to adding the second BGP connection. We will add a few more pieces of information onto the single task of creating a full BGP configuration.  </p> <p>First let's take a look at the BGP table on the router at this time, there is only one neighbor:  </p> <pre><code>BGP router identifier 10.0.0.3, local AS number 65500\nBGP table version is 37404, main routing table version 37404\n11 network entries using 1584 bytes of memory\n12 path entries using 960 bytes of memory\n6/6 BGP path/bestpath attribute entries using 912 bytes of memory\n1 BGP AS-PATH entries using 24 bytes of memory\n0 BGP route-map cache entries using 0 bytes of memory\n0 BGP filter-list cache entries using 0 bytes of memory\nBGP using 3480 total bytes of memory\nBGP activity 18/6 prefixes, 18708/18696 paths, scan interval 60 secs\n\nNeighbor        V           AS MsgRcvd MsgSent   TblVer  InQ OutQ Up/Down  State/PfxRcd\n198.51.100.2    4        65500   37400      63    37404    0    0 00:13:39       11\n</code></pre> <p>THe playbook is now:</p> <pre><code>---\n# yamllint disable rule:truthy\n# yamllint disable rule:line-length\n- name: \"PLAY 1: Setup iBGP Peer to R4\"\n  connection: network_cli\n  hosts: r3\n  become: yes\n  become_method: enable\n  tasks:\n    - name: \"TASK 1: Setup iBGP Peer\"\n      ios_bgp:\n        config:\n          bgp_as: 65500\n          router_id: 10.0.0.3\n          log_neighbor_changes: true\n          neighbors:\n            - neighbor: 198.51.100.2\n              remote_as: 65500\n              timers:\n                keepalive: 15\n                holdtime: 45\n                min_neighbor_holdtime: 5\n              description: R4\n            - neighbor: 192.0.2.1\n              remote_as: 65510\n              timers:\n                keepalive: 15\n                holdtime: 45\n                min_neighbor_holdtime: 5\n              description: ISP Neighbor 1\n          networks:\n            - prefix: 198.51.100.0\n              masklen: 24\n            - prefix: 203.0.113.0\n              masklen: 25\n            - prefix: 203.0.113.128\n              masklen: 25\n          address_family:\n            - afi: ipv4\n              safi: unicast\n              auto_summary: no\n              neighbors:\n                - neighbor: 198.51.100.2\n                  activate: yes\n                  next_hop_self: yes\n                - neighbor: 192.0.2.1\n                  activate: yes\n        operation: merge\n      register: bgp_setup\n\n    - name: \"SUMMARY TASK: Debug output\"\n      debug:\n        msg:\n          - \"{{ bgp_setup }}\"\n...\n</code></pre>","tags":["ansible","cisco","ios_bgp"]},{"location":"ansible-ios-bgp-module/#adding-second-neighbor-execution","title":"Adding Second Neighbor - Execution","text":"<p>The execution of the playbook is straight forward again. As expected a second neighbor statement is created with the <code>remote-as</code>, <code>timers</code>, and <code>description</code>. The module also activates the neighbor.  </p> <pre><code>PLAY [PLAY 1: Setup iBGP Peer to R4] *******************************************\n\nTASK [TASK 1: Setup iBGP Peer] *************************************************\nchanged: [r3]\n\nTASK [SUMMARY TASK: Debug output] **********************************************\nok: [r3] =&gt; {\n    \"msg\": [\n        {\n            \"changed\": true,\n            \"commands\": [\n                \"router bgp 65500\",\n                \"neighbor 192.0.2.1 remote-as 65510\",\n                \"neighbor 192.0.2.1 timers 15 45 5\",\n                \"neighbor 192.0.2.1 description ISP Neighbor 1\",\n                \"network 198.51.100.0 mask 255.255.255.0\",\n                \"network 203.0.113.128 mask 255.255.255.128\",\n                \"address-family ipv4\",\n                \"no auto-summary\",\n                \"neighbor 192.0.2.1 activate\",\n                \"exit-address-family\",\n                \"exit\"\n            ],\n            \"failed\": false\n        }\n    ]\n}\n\nPLAY RECAP *********************************************************************\nr3                         : ok=2    changed=1    unreachable=0    failed=0    s\nkipped=0    rescued=0    ignored=0\n</code></pre> <p>Taking a look at the BGP table, we now have 2 neighbors formed instead of just the one.  </p> <pre><code>BGP router identifier 10.0.0.3, local AS number 65500\nBGP table version is 58006, main routing table version 58006\n11 network entries using 1584 bytes of memory\n19 path entries using 1520 bytes of memory\n9/6 BGP path/bestpath attribute entries using 1368 bytes of memory\n1 BGP AS-PATH entries using 24 bytes of memory\n0 BGP route-map cache entries using 0 bytes of memory\n0 BGP filter-list cache entries using 0 bytes of memory\nBGP using 4496 total bytes of memory\nBGP activity 21/9 prefixes, 29013/28994 paths, scan interval 60 secs\n\nNeighbor        V           AS MsgRcvd MsgSent   TblVer  InQ OutQ Up/Down  State/PfxRcd\n192.0.2.1       4        65510      31      30    56929    0    0 00:03:26        6\n198.51.100.2    4        65500   57996      99    58006    0    0 00:21:06       11\n</code></pre>","tags":["ansible","cisco","ios_bgp"]},{"location":"ansible-ios-bgp-module/#summary","title":"Summary","text":"<p>The module library keeps expanding. Originally I was taken back on the number of different modules being created that had a specialty to it. Take a look at the number of modules available for Ansible 2.9 and NXOS! I now see the benefit, and this module is a great addition the IOS module family. There are still areas that aren't covered that may be better suited to be done with a Jinja template, but this is a great start on the BGP world for IOS.  </p> <p>Hope that this has helped someone along the way!</p>","tags":["ansible","cisco","ios_bgp"]},{"location":"eveng-for-autoamtion-practice-and-testing/","title":"EVE-NG for Automation Practice and Testing","text":"<p>As I restarted looking at how I'm continuing my education on the Network Automation and certification realm I asked the question \"How are you simulating your network environment?\" At the same time there has been thought on the idea of leveraging cloud resources to gain experience there.  </p> <p>First requirement for me is that whatever tool/simulation set that I use it has to work. That being said, I need to be able to generate configurations, connect devices to each other, and have packets flow through the simulated network, just like any other network.</p> <p>Second requirement is that I desire the solution to be economical. As a budget for this there wasn't a lot of money left to be throwing around.  </p> <p>Asking around, the third softer requirement is the solution should have a GUI of some sorts to make things work quickly so you aren't fussing around with creating your own middleware solution.  </p> <p>My answer then to this at this point in time (2019-08-04) is EVE-NG. There is a strong possibility of this changing in the near future based on what I saw at 2019 Cisco Live to Cisco's VIRL, but at the moment, EVE-NG and GNS3 both meet the requirements.</p> <p>If you are looking for the part about how I get at devices in the EVE-NG network jump down to \"EVE-NG for Automation Practice\".  </p> <p>This is not going to be a post on getting started on using the solutions. This post assumes that you are up and running with EVE locally on your network already. There are links further down that do help though for getting connectivity.</p>"},{"location":"eveng-for-autoamtion-practice-and-testing/#requirements-for-automation","title":"Requirements for automation","text":"<ul> <li>Must be able to simulate larger networks</li> <li>Must be able to SSH to the devices directly for automation (Not just click on and get a console window)</li> </ul>"},{"location":"eveng-for-autoamtion-practice-and-testing/#evaluation-in-my-mind-no-formal-written-down","title":"Evaluation (In my mind, no formal written down)","text":"<p>First when looking at the cloud side of things for running EVE-NG, I had built out an instance of EVE-NG in Google Cloud with the help of @showipintbri's article on EVE-NG in the Cloud. As I looked at what I had already done with a bare metal host it appeared that I would need to create a VPN to be able to get at the network behind the cloud of an EVE-NG. Looking at the pricing on a VPN tunnel per minute/hour with Google Cloud, I made the evaluation that doing this in the cloud would not be economical.  </p> <p>GNS3 has been a solid main stay for some time in the Network simulation world. There is nothing wrong with it. I have been successful in reaching into the GNS3 simulated world from a real network. There are some instructions on the web about how to do so. I had basically followed this instruction set (that has been removed, using the way back machine to get the old post) -  Connect GNS3 to the Internet.  </p> <p>For me to get started quickly, I had recently installed an EVE-NG bare metal installation. That is the route that I have chosen at the moment to get started quickly. For instructions on doing a bare metal installation of EVE-NG I followed the online docs located on the EVE-NG main page -  EVE-NG Bare Metal Install.</p>"},{"location":"eveng-for-autoamtion-practice-and-testing/#eve-ng-for-automation-practice-and-testing","title":"EVE-NG for Automation Practice and Testing","text":"<p>So now how do we get access to the network? First within EVE-NG I Add a New Network to the project. I make sure that it is set to:</p> <ul> <li>Number of Networks to build: 1</li> <li>Name/Prefix: Internet (Be creative if you wish)</li> <li>Type: bridge</li> </ul> <p> The type of bridge is what we are looking for to enable the connectivity.  </p>"},{"location":"eveng-for-autoamtion-practice-and-testing/#connecting-your-router","title":"Connecting your router","text":"<p>First thing you need to do within EVE-NG is to add a router and connect it to your Outside network. I've done so as shown here:</p> <p> </p> <p>Then the configuration on this Cisco edge device I have configured the following on the interface that has been connected to the outside.</p> <pre><code>interface GigabitEthernet0/0\n ip address dhcp\n duplex full\n speed 1000\n media-type rj45\nend\n</code></pre> <p>This allows the device to come online and get a network address. If you wanted to prescribe what address it is in a static fashion, that is something you can do too. I like to use DHCP to verify that the device is in fact connected to the home network.  </p>"},{"location":"eveng-for-autoamtion-practice-and-testing/#routing","title":"Routing","text":"<p>For connectivity this is where I like to start with OSPF to peer with my home firewall. Why? First, because the firewall at home supports OSPF and why not use routing! Secondly it is more practice with OSPF. Much of the time in my career has been spent at places where EIGRP is the predominant IGP. It always helps to continue to gain experience. This is where you could use a static route for your device to send the routes back into your EVE-NG environment. You'll want to make sure that you have routes for the networks that are in the network and that they do not overlap with your existing home network.  </p>"},{"location":"eveng-for-autoamtion-practice-and-testing/#test-connections","title":"Test Connections","text":"<p>Once connected, I make sure that I make sure that I'm able to connect successfully with SSH before starting the Ansible work. You could create and get started with a playbook and test, but I found it is easier to verify that you have SSH connectivity natively to your devices just like you would with a non-virtualized network.  </p>"},{"location":"eveng-for-autoamtion-practice-and-testing/#automation-testing","title":"Automation Testing","text":"<p>Now that there is connectivity to the devices on the box that you can SSH to each device from your home network, you are able to do testing of various playbooks. Start with some simple show commands using the ios_command or the newer cli_command module. And then debug it. Head on over to the post my earlier post Ansible - working with command output for some samples on getting started with Cisco devices.  </p> <p>Hope this is something that is helpful for you at home already running EVE locally!</p>"},{"location":"ansible-cli-vs-ios-command/","title":"Ansible differences between ios command and cli command","text":"<p>In an earlier post I covered the differences between <code>ios_config</code> and  <code>cli_config</code>. However I did not cover what the difference was between <code>ios_command</code> and <code>cli_command</code>. Most of the items covered there remain the same. So this will be a post that mostly gets straight to it and sees what the difference is.  </p> <p>A reminder that I am also putting playbooks used here out on Github. You can find this at: https://github.com/jvanderaa/ansible-using_ios</p>","tags":["ansible","cisco","ios_command","cli_command"]},{"location":"ansible-cli-vs-ios-command/#differences","title":"Differences","text":"<p>First, for the <code>cli_commands</code> module, you must be using a connection method of <code>network_cli</code>. You should not use <code>connection: local</code> for this module. Note that the <code>cli_command</code> can also be used with multiple device types, including multiple vendors. Take a look at the <code>cli_command</code> documentation page that there is a link at the bottom of the post.</p>","tags":["ansible","cisco","ios_command","cli_command"]},{"location":"ansible-cli-vs-ios-command/#parameters","title":"Parameters","text":"<p>As in the config modules, the first difference is how you pass what you wish to have executed. With <code>cli_command</code> you are sending a single string, just one command. This is under the command parameter. With <code>ios_command</code> you get to send a list of commands send with the commands parameter. This can be handy in some times to execute a whole bunch of commands in one task to a device.</p>","tags":["ansible","cisco","ios_command","cli_command"]},{"location":"ansible-cli-vs-ios-command/#output","title":"Output","text":"<p>The second major difference according to the documentation between <code>cli_command</code> and <code>ios_command</code> is the return format. Assuming a single command on the <code>ios_command</code> side of things is sent, here are the returns from the module:</p>","tags":["ansible","cisco","ios_command","cli_command"]},{"location":"ansible-cli-vs-ios-command/#ios_command-returns","title":"ios_command returns","text":"<ul> <li>failed_conditions</li> <li>stdout</li> <li>stdout_lines</li> </ul>","tags":["ansible","cisco","ios_command","cli_command"]},{"location":"ansible-cli-vs-ios-command/#cli_command-returns","title":"cli_command returns","text":"<ul> <li>json</li> <li>stdout</li> </ul>","tags":["ansible","cisco","ios_command","cli_command"]},{"location":"ansible-cli-vs-ios-command/#analysis-of-returns","title":"Analysis of returns","text":"<p>The first thing about the stdout_lines output is that it makes it very human readable what the output of the command is. If you are working on something programmatically speaking, you will likely only want to use stdout.  </p> <p>Next we see that cli_command has a json return, which is going to provide more structured feedback from the command.  </p> <p>Both have in common the stdout return, however, the data type is very different. Since <code>cli_command</code> sent only a single string, the return is a single string. On <code>ios_command</code> this is a list of responses. Even if you sent a single command, it comes back as a list that is only one item. So you will need to access <code>variable_name.stdout.0</code> (or <code>variable_name.stdout[0]</code>) to get at the command output.  </p> <p>Let's get to taking a look at the output.</p>","tags":["ansible","cisco","ios_command","cli_command"]},{"location":"ansible-cli-vs-ios-command/#demo-of-commands","title":"Demo of commands","text":"<p>Let's take a look at how the responses look with just a single command first. I have a preference of taking a look at NTP associations lately.</p>","tags":["ansible","cisco","ios_command","cli_command"]},{"location":"ansible-cli-vs-ios-command/#ios_command-single-command","title":"ios_command - single command","text":"<p>Here is what the task portion of the playbook looks like with a single command.  </p> <pre><code>tasks:\n  - name: \"TASK 1: Get NTP Associations\"\n    ios_command:\n      commands:\n        - show ntp associations\n    register: command_output\n\n  - name: \"TASK 2: Debug output\"\n    debug:\n      msg: \"{{ command_output }}\"\n\n\n```}\n\nThe output from this is as follows assuming an NTP association to the cloudflare NTP servers:  \n\n```yaml linenums=\"1\"\n\n\nPLAY [PLAY 1: Using ios_command for a single command] **********************************************\n\nTASK [TASK 1: Get NTP Associations] ****************************************************************\nok: [r1]\n\nTASK [TASK 2: Debug output] ************************************************************************\nok: [r1] =&gt; {\n    \"msg\": {\n        \"changed\": false,\n        \"failed\": false,\n        \"stdout\": [\n            \"address         ref clock       st   when   poll reach  delay  offset   disp\\n*~162.159.200.123 10.72.8.95       3     14     64     7 21.111  72.531  0.746\\n * sys.peer, # selected, + candidate, - outlyer, x falseticker, ~ configured\"\n        ],\n        \"stdout_lines\": [\n            [\n                \"address         ref clock       st   when   poll reach  delay  offset   disp\",\n                \"*~162.159.200.123 10.72.8.95       3     14     64     7 21.111  72.531  0.746\",\n                \" * sys.peer, # selected, + candidate, - outlyer, x falseticker, ~ configured\"\n            ]\n        ]\n    }\n}\n\nPLAY RECAP ************************************************************************************************************\nr1                         : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n</code></pre> <p></p> <p>There are four items returned, with the first two primarily being \"standard\" Ansible returns for <code>changed</code> and <code>failed</code>. There is then:</p> <ul> <li><code>stdout</code>: List of outputs, so when there are multiple commands.</li> <li><code>stdout_lines</code>: List of lists, the inner list is the commands printed line by line, which makes it more human readable. The outer list is like that of stdout, that is for each command run, including if there is only a single command. Look at the end of line 11, and line 13. This shows that there is in fact a list [] of responses to parse through.  </li> </ul>","tags":["ansible","cisco","ios_command","cli_command"]},{"location":"ansible-cli-vs-ios-command/#cli_command-single-command","title":"cli_command - single command","text":"<p>The tasks on the <code>cli_command</code> looks pretty similar. However there a few differences. First the value of <code>command:</code> is a string, this you will see by not having a <code>-</code> in the line. I'm also going to use quotes around to demonstrate this.</p> <pre><code>tasks:\n  - name: \"TASK 1: Get NTP Associations\"\n    cli_command:\n      command: \"show ntp associations\"\n    register: command_output\n\n  - name: \"TASK 2: Debug output\"\n    debug:\n      msg: \"{{ command_output }}\"\n</code></pre> <p>The command output looks awfully similar now in recent versions of Ansible.</p> <pre><code>PLAY [PLAY 1: Using cli_command for a single command] ****************************************************************************\n\nTASK [TASK 1: Get NTP Associations] **********************************************************************************************\nok: [r1]\n\nTASK [TASK 2: Debug output] ******************************************************************************************************\nok: [r1] =&gt; {\n    \"msg\": {\n        \"changed\": false,\n        \"failed\": false,\n        \"stdout\": \"address         ref clock       st   when   poll reach  delay  offset   disp\\n*~162.159.200.123 10.72.8.95       3     50     64   377 16.772  28.507  5.117\\n * sys.peer, # selected, + candidate, - outlyer, x falseticker, ~ configured\",\n        \"stdout_lines\": [\n            \"address         ref clock       st   when   poll reach  delay  offset   disp\",\n            \"*~162.159.200.123 10.72.8.95       3     50     64   377 16.772  28.507  5.117\",\n            \" * sys.peer, # selected, + candidate, - outlyer, x falseticker, ~ configured\"\n        ]\n    }\n}\n\nPLAY RECAP ***********************************************************************************************************************\nr1                         : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n</code></pre> <p></p> <p>The big difference here is that the stdout part of the response is of type string, and not of a type list like <code>ios_command</code>. Take a look at line number 11 where stdout is. Immediately following the colon is a double quote, indicating that this is a string. So if you are doing work on this variable, you will need to take string actions.</p>","tags":["ansible","cisco","ios_command","cli_command"]},{"location":"ansible-cli-vs-ios-command/#ios_comamnd-multiple-commands","title":"ios_comamnd - multiple commands","text":"<p>The \"bonus\" of the ios_command module is that you can run multiple commands within a single task. As I type that out, it seems against the idea of individual task execution, to do 2 or more things in a single task. But that is what the module allows us in this instance. Let's take a look at this playbook to verify NTP information and then get the time from the device.</p> <p>This could be a part of the <code>ios_command</code> history as well. When <code>ios_command</code> was written each individual task would start a new connection to IOS devices. So to preserve the number of logins required it would be good to be able to execute multiple lines.</p> <p>There is now a single task, but there are two commands in the commands section. These will be run and saved to a variable named <code>command_output</code>.</p> <pre><code>- name: \"TASK 1: Get NTP Associations\"\n  ios_command:\n    commands:\n      - show ntp associations\n      - show clock\n  register: command_output\n\n- name: \"TASK 2: Debug output\"\n  debug:\n    msg: \"{{ command_output }}\"\n</code></pre> <p>Let's take a look at how this looks now:</p> <pre><code>PLAY [PLAY 1: Using ios_command for a single command] ****************************************************************************\n\nTASK [TASK 1: Get NTP Associations] **********************************************************************************************\nok: [r1]\n\nTASK [TASK 2: Debug output] ******************************************************************************************************\nok: [r1] =&gt; {\n    \"msg\": {\n        \"changed\": false,\n        \"failed\": false,\n        \"stdout\": [\n            \"address         ref clock       st   when   poll reach  delay  offset   disp\\n*~162.159.200.123 10.72.8.95       3    262    512   377 16.551  65.112  0.097\\n * sys.peer, # selected, + candidate, - outlyer, x falseticker, ~ configured\",\n            \"21:57:28.776 UTC Sun Oct 20 2019\"\n        ],\n        \"stdout_lines\": [\n            [\n                \"address         ref clock       st   when   poll reach  delay  offset   disp\",\n                \"*~162.159.200.123 10.72.8.95       3    262    512   377 16.551  65.112  0.097\",\n                \" * sys.peer, # selected, + candidate, - outlyer, x falseticker, ~ configured\"\n            ],\n            [\n                \"21:57:28.776 UTC Sun Oct 20 2019\"\n            ]\n        ]\n    }\n}\n\nPLAY RECAP ***********************************************************************************************************************\nr1                         : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n</code></pre> <p></p> <p>This is where you start to see that there are multiple list items in the response. Taking a look at line number 11 we still have the <code>[</code> of the list showing at the end, then line 12 ends in a comma, indicating the next list item. Line 13 ends the list. This repeats on the stdout_lines as well.  </p> <p>If you wanted to get at just the time of the device in this instance, this is how you would do a debug task for it:</p> <pre><code>- name: \"Debug time\"\n  debug:\n    msg: \"{{ command_output.stdout[1] }}\"\n</code></pre> <p>You see that you need to call the variable name, then the return value that you are looking for -  stdout. Then you need the list position on the response that corresponds to where it was called on the <code>ios_command</code> module.  </p>","tags":["ansible","cisco","ios_command","cli_command"]},{"location":"ansible-cli-vs-ios-command/#cli_command-multiple-commands","title":"cli_command - multiple commands","text":"<p>To do the same multiple commands on the <code>cli_command</code> front, you will want to use a loop. Here I prefer to use the <code>with_items</code> loop. You will see several more key/value pairs on the variable when using a loop, so let's take a look below:</p> <pre><code>- name: \"TASK 1: Get NTP Associations\"\n  cli_command:\n    command: \"{{ item }}\"\n  register: command_output\n  with_items:\n    - \"show ntp associations\"\n    - \"show clock\"\n\n- name: \"TASK 2: Debug command output\"\n  debug:\n    msg: \"{{ command_output }}\"\n</code></pre> <p>The output:</p> <pre><code>PLAY [PLAY 1: Using cli_command for a single command] ****************************************************************************\n\nTASK [TASK 1: Get NTP Associations] **********************************************************************************************\nok: [r1] =&gt; (item=show ntp associations)\nok: [r1] =&gt; (item=show clock)\n\nTASK [TASK 2: Debug command output] **********************************************************************************************\nok: [r1] =&gt; {\n    \"msg\": {\n        \"changed\": false,\n        \"msg\": \"All items completed\",\n        \"results\": [\n            {\n                \"ansible_loop_var\": \"item\",\n                \"changed\": false,\n                \"failed\": false,\n                \"invocation\": {\n                    \"module_args\": {\n                        \"answer\": null,\n                        \"check_all\": false,\n                        \"command\": \"show ntp associations\",\n                        \"prompt\": null,\n                        \"sendonly\": false\n                    }\n                },\n                \"item\": \"show ntp associations\",\n                \"stdout\": \"address         ref clock       st   when   poll reach  delay  offset   disp\\n*~162.159.200.123 10.72.8.95       3     64    128   377 16.354 -25.131  2.283\\n * sys.peer, # selected, + candidate, - outlyer, x falseticker, ~ configured\",\n                \"stdout_lines\": [\n                    \"address         ref clock       st   when   poll reach  delay  offset   disp\",\n                    \"*~162.159.200.123 10.72.8.95       3     64    128   377 16.354 -25.131  2.283\",\n                    \" * sys.peer, # selected, + candidate, - outlyer, x falseticker, ~ configured\"\n                ]\n            },\n            {\n                \"ansible_loop_var\": \"item\",\n                \"changed\": false,\n                \"failed\": false,\n                \"invocation\": {\n                    \"module_args\": {\n                        \"answer\": null,\n                        \"check_all\": false,\n                        \"command\": \"show clock\",\n                        \"prompt\": null,\n                        \"sendonly\": false\n                    }\n                },\n                \"item\": \"show clock\",\n                \"stdout\": \"02:19:22.598 UTC Mon Oct 21 2019\",\n                \"stdout_lines\": [\n                    \"02:19:22.598 UTC Mon Oct 21 2019\"\n                ]\n            }\n        ]\n    }\n}\n\nPLAY RECAP ***********************************************************************************************************************\nr1                         : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n</code></pre> <p></p> <p>In this execution we now have to get at the information within the results section. You do however also get the command in the output, as well as some other module arguments, which can be handy! To get at the results from <code>show ntp associations</code> you will need to use <code>command_output.results[0].stdout</code> and <code>command_output.results[1].stdout</code> to get at the results of <code>show clock</code>. </p>","tags":["ansible","cisco","ios_command","cli_command"]},{"location":"ansible-cli-vs-ios-command/#summary","title":"Summary","text":"<p>I hope this has been valuable to you as a reader. With <code>cli_command</code> still relatively new, having been released in Ansible 2.7, I expect that it will continue to evolve. Take a look at the docs pages for these here:</p> <p>cli_command ios_command </p>","tags":["ansible","cisco","ios_command","cli_command"]},{"location":"disney_plus_streaming/","title":"Disney Plus Streaming Bandwidth","text":"<p>This will be a brief departure from the automation focused attention that I have been giving to this blog over the past year or so. This week in the United States was the launch of Disney+ streaming service. I have subscribed to it at this point and have found some interesting data based on SNMP polling my network.  </p> <p>This post is about the bandwidth that I am seeing used, not about anything about the service, or if another service is better. I don't have the time for that at this time. This is just about what was an unexpected jump in the bandwidth usage with the new application. But I am very much OK with that as my subscription level is taking care of that.</p>","tags":["homenet","disney","streaming"]},{"location":"disney_plus_streaming/#streaming-setup","title":"Streaming Setup","text":"<p>So what this is going to show is numbers with just a single device. So this isn't a full across the board deep test. But it is something to get some numbers out there. My household streaming consists of primarily an Apple TV Full HD (not the 4k one) or an iPad mini streaming.  </p> <p>My broadband provider is a cable service provider that speed tests have shown consistent speeds at around 200 Mbps down, 11 Mbps up.</p>","tags":["homenet","disney","streaming"]},{"location":"disney_plus_streaming/#streaming-bandwidth-number-historically","title":"Streaming Bandwidth Number - Historically","text":"<p>Historically I've always maintained that based on a Netflix stream, or use of PlayStation Vue that an HD stream would use somewhere between 3-5 Mbps of bandwidth on a broadband network. I'll show where that still remains true on the bandwidth graph. That has held true so far.</p>","tags":["homenet","disney","streaming"]},{"location":"disney_plus_streaming/#disney-streaming","title":"Disney+ Streaming","text":"<p>So what have I seen at this point from just 2 days of use of Disney+? It uses much more bandwidth than I originally expected, in fact, it has been over double that of Netflix streams. So much more so that the 95th percentile on the home network with just a single stream running has consistently run up to 11 Mbps for my download.</p>","tags":["homenet","disney","streaming"]},{"location":"disney_plus_streaming/#quality","title":"Quality?","text":"<p>From the picture that I have seen, it does look very crisp and clear. Makes sense that there are more bits coming across the wire. I would say that at least in my experience there is a correlation of quality to the amount of bits coming across.  </p> <p>The Netflix subscription is the basic HD level. I have not done that study of what that actually means if that is 720p or 1080p that is coming across. I also do not know what compression there is within the applications.  </p>","tags":["homenet","disney","streaming"]},{"location":"disney_plus_streaming/#graph-to-show","title":"Graph To Show","text":"<p>Here is the bandwidth graphs. I will try to pretty up the graphics a little more in a future post.</p> <p> </p> <p>The two spikes up to the red line (95th percentile) on the top side of 0 Mbps are the two times that Disney+ was in use. Definitely some good amount of usage there. If you see the other couple of early morning spikes, that is Netflix to an iPad in use. There are some other general streams of data that happen throughout the day as the family jumps in and out of some other streaming services. But nothing close to the Disney+ numbers.  </p>","tags":["homenet","disney","streaming"]},{"location":"disney_plus_streaming/#summary","title":"Summary","text":"<p>There has been some negative press around the large launch, although I have not had the experiences. Thus far things have been good, we see that there is more bandwidth utilized from Disney+ launch. At this point a kudos to Disney's CDN providers as well that have been able to push out this kind of data rates. I assume that others are likely having the same bandwidth utilization with the service.  </p> <p>I hope this helps!</p> <p>I have it on my radar to move to a more modern network graphing setup as well, I'm just not to that point in the home environment yet.</p>","tags":["homenet","disney","streaming"]},{"location":"ansible-cisco-ios-interfaces-module/","title":"Ansible Cisco ios_interfaces module","text":"<p>This has become a post about the ios_interfaces module with documentation that can be found Ansible ios_interfaces doc. Originally I was going to write about the deprecations for just the Cisco IOS modules. Then as I investigated further, I had found that there are many more modules that are being deprecated. In this post I will take a closer look at the differences between the <code>ios_interface</code> and <code>ios_vlan</code> modules that I had written posts on last year and what their new counter parts look like. And in the end the post had quite a bit of good detail about the module. I think you will like what is here.  </p> <p>Previous Posts</p> <ul> <li>ios_interface</li> <li>ios_vlan</li> </ul>","tags":["blog","ansible","cisco","deprecation"]},{"location":"ansible-cisco-ios-interfaces-module/#module-deprecations","title":"Module Deprecations","text":"<p>In addition to the Cisco \"legacy\" interface and vlan modules that are being deprecated, the Ansible generic modules are being deprecated as well. These include <code>net_interface</code>, <code>net_linkagg</code>, <code>net_l2/l3_interface</code>, and <code>net_l2/l3_vlan</code>.  </p> <p>Not to be outdone, not only are the Cisco and Ansible generic modules being deprecated, so are each of the same set of modules for Juniper, EOS, VYOS, NXOS, IOSXR, and Netvisor.  </p> <p>In all by doing a browser search for <code>(D)</code> on the Ansible 2.9 Network Modules page I come across a total of 77 different modules that are in the process of being deprecated. That is quite a bit, so make sure that you are taking a look at your playbooks to look for this.  </p> <p>One of the downsides I see coming out of this module change is the change from <code>ios_interface</code> to <code>ios_interfaces</code>. This is such a subtle difference between the two. You are going to need to pay extra attention to it. In fact I was taken back when I first saw the parameters of the module being posted on the Network To Code Public Slack that I had to take a second look. I thought that someone was way off on the parameters they were using. Then I saw the deprecations and new modules.</p>","tags":["blog","ansible","cisco","deprecation"]},{"location":"ansible-cisco-ios-interfaces-module/#panos-module-deprecation","title":"PANOS Module Deprecation","text":"<p>I did also observe an interesting (in my mind) planned deprecation. All of the PANOS modules are planned to be deprecated in favor of using community drivers from Ansible Galaxy. Why is this interesting? Well, this is the start of the move to move modules out of Ansible Core and start using Ansible Galaxy to distribute the modules that you need. More on that to come in another post.</p>","tags":["blog","ansible","cisco","deprecation"]},{"location":"ansible-cisco-ios-interfaces-module/#differences","title":"Differences","text":"<p>There are quite a bit of differences in the modules. The first level parameters for _interfaces across vendors has been reduced to just two, config and state*.  </p> <p>A second difference that I'm observing is the lack of the ability to save the config when change. This means that you as the playbook creator will need to take action such as notifying a handler or running a save config when there is a change. You can do this, or have a task executed when there is a change.</p>","tags":["blog","ansible","cisco","deprecation"]},{"location":"ansible-cisco-ios-interfaces-module/#config","title":"Config","text":"<p>All of the bulk of the configuration has moved into the config parameter. Within Cisco <code>ios_interfaces</code> you now have the options to configure the description, duplex, enabled, mtu, name*, and speed. These were previously first level parameters within the module definition. You can find the module definition at Ansible Docs ios_interfaces.</p>","tags":["blog","ansible","cisco","deprecation"]},{"location":"ansible-cisco-ios-interfaces-module/#state","title":"State","text":"<p>The state is now referencing the configuration of the interface. It does not state whether or not the interface is enabled or disabled. That is controlled by the enabled sub-parameter of config. The options for state include merged (default setting), replaced, overridden, and deleted.</p>","tags":["blog","ansible","cisco","deprecation"]},{"location":"ansible-cisco-ios-interfaces-module/#state-merged","title":"State: Merged","text":"<p>This looks to take whatever is already in the interface configuration and adding/replacing based on what the module parameters that are configured. So if you have an interface that has just the speed configured and you just have a task that configures the duplex, you will have speed and duplex configured.  </p> <p>With the merged module, you will run the commands that are defined, not worrying about the defaults. For this demo, I have set the MTU to 1450, which is different than the default of 1500 for this device type. You will see that in other states, that Ansible will change the configuration. Where as with the merged type, you will run the commands seen in the module parameters, not worrying about the other parameters not provided.</p> <pre><code>  tasks:\n    - name: \"TASK 1: IOS &gt;&gt; Set some interfaces with merge\"\n        ios_interfaces:\n        state: replaced\n        config:\n            - name: GigabitEthernet0/3\n            enabled: yes\n            description: \"Configured by Ansible\"\n        register: ios_interface_output\n\n    - name: \"TASK 2: SYS &gt;&gt; DEBUG OUTPUT\"\n        debug:\n        msg: \"{{ ios_interface_output }}\"\n</code></pre> <pre><code>\"commands\": [\n    \"interface GigabitEthernet0/3\",\n    \"description Configured by Ansible\"\n</code></pre>","tags":["blog","ansible","cisco","deprecation"]},{"location":"ansible-cisco-ios-interfaces-module/#state-replaced","title":"State: Replaced","text":"<p>When using replaced, the entire interface will be configured with what is set in the module. If you set duplex only, you will only get the duplex of the interface set.  </p> <p>In this example I will only be looking to enable the interface and change the description. There will be no changes to the other interfaces defined.  </p> <pre><code>  tasks:\n    - name: \"TASK 1: IOS &gt;&gt; Set some interfaces with merge\"\n      ios_interfaces:\n        state: replaced\n        config:\n          - name: GigabitEthernet0/3\n            enabled: yes\n            description: \"Configured by Ansible\"\n      register: ios_interface_output\n\n    - name: \"TASK 2: SYS &gt;&gt; DEBUG OUTPUT\"\n      debug:\n        msg: \"{{ ios_interface_output }}\"\n</code></pre> <p>The output (shown in full here only, will skip the \"after\" and \"before\" keys in subsequent examples) shows that the only changes being made are to <code>GigabitEthernet0/3</code>. This is similar to what we will see in the overridden section. Overridden will configure every interface on the device vs replaced looks to only be handling the interface defined within the config parameter.</p> <pre><code>    \"msg\": {\n        \"after\": [\n            {\n                \"enabled\": true,\n                \"name\": \"loopback0\"\n            },\n            {\n                \"duplex\": \"auto\",\n                \"enabled\": true,\n                \"name\": \"GigabitEthernet0/0\",\n                \"speed\": \"auto\"\n            },\n            {\n                \"description\": \"MANAGEMENT\",\n                \"duplex\": \"auto\",\n                \"enabled\": true,\n                \"name\": \"GigabitEthernet0/1\",\n                \"speed\": \"auto\"\n            },\n            {\n                \"description\": \"PRODUCTION\",\n                \"duplex\": \"auto\",\n                \"enabled\": true,\n                \"mtu\": 1400,\n                \"name\": \"GigabitEthernet0/2\",\n                \"speed\": \"auto\"\n            },\n            {\n                \"description\": \"Configured by Ansible\",\n                \"duplex\": \"auto\",\n                \"enabled\": true,\n                \"name\": \"GigabitEthernet0/3\",\n                \"speed\": \"auto\"\n            }\n        ],\n        \"before\": [\n            {\n                \"enabled\": true,\n                \"name\": \"loopback0\"\n            },\n            {\n                \"duplex\": \"auto\",\n                \"enabled\": true,\n                \"name\": \"GigabitEthernet0/0\",\n                \"speed\": \"auto\"\n            },\n            {\n                \"description\": \"MANAGEMENT\",\n                \"duplex\": \"auto\",\n                \"enabled\": true,\n                \"name\": \"GigabitEthernet0/1\",\n                \"speed\": \"auto\"\n            },\n            {\n                \"description\": \"PRODUCTION\",\n                \"duplex\": \"auto\",\n                \"enabled\": true,\n                \"mtu\": 1400,\n                \"name\": \"GigabitEthernet0/2\",\n                \"speed\": \"auto\"\n            },\n            {\n                \"description\": \"BEFORE ANSIBLE\",\n                \"duplex\": \"auto\",\n                \"enabled\": true,\n                \"mtu\": 1450,\n                \"name\": \"GigabitEthernet0/3\",\n                \"speed\": \"auto\"\n            }\n        ],\n        \"changed\": true,\n        \"commands\": [\n            \"interface GigabitEthernet0/3\",\n            \"no mtu\",\n            \"description Configured by Ansible\"\n</code></pre>","tags":["blog","ansible","cisco","deprecation"]},{"location":"ansible-cisco-ios-interfaces-module/#state-overridden","title":"State: Overridden","text":"<p>This one was not completely obvious to me when originally looking at. But as I tested, I have now come to find that this state is something to be VERY cautious with. In the testing, I had the following configuration on the devices:</p> <pre><code>interface GigabitEthernet0/1\n description MANAGEMENT\n OUTPUT OMITTED\n!\ninterface GigabitEthernet0/2\n description PRODUCTION\n</code></pre> <p>You can see that there are configurations applied to the description. I then created these playbook tasks to test the <code>Overridden</code> setting.</p> <pre><code>  tasks:\n    - name: \"TASK 1: IOS &gt;&gt; Set some interfaces with merge\"\n      ios_interfaces:\n        state: overridden\n        config:\n          - name: GigabitEthernet0/3\n            enabled: yes\n            description: \"Configured by Ansible\"\n      register: ios_interface_output\n\n    - name: \"TASK 2: SYS &gt;&gt; DEBUG OUTPUT\"\n      debug:\n        msg: \"{{ ios_interface_output }}\"\n</code></pre> <p>The output shows that Ansible is going to erase the description lines:</p> <pre><code>\"commands\": [\n    \"interface GigabitEthernet0/1\",\n    \"no description\",\n    \"interface GigabitEthernet0/2\",\n    \"no description\"\n</code></pre> <p>This indicates that Ansible will default settings that are not specifically defined within the module.</p> <p>The next test I changed the MTU on <code>GigabitEthernet0/1</code> and <code>GigabitEthernet0/2</code> to some random 14xx MTUs. I've left everything else the same as the play above with no changing fo the MTU, just setting the description on a single interface. The results from running that playbook now show that the interface MTU is reset to default since it was not statically defined in the task.</p> <pre><code>    \"commands\": [\n    \"interface GigabitEthernet0/1\",\n    \"no mtu\",\n    \"interface GigabitEthernet0/2\",\n    \"no mtu\",\n    \"interface GigabitEthernet0/3\",\n    \"no mtu\"\n],\n</code></pre>","tags":["blog","ansible","cisco","deprecation"]},{"location":"ansible-cisco-ios-interfaces-module/#state-overridden-loop","title":"State: Overridden - Loop","text":"<p>So what does this look like if we wanted to loop over a set of interfaces? Would there be special considerations made for the module? The answer is no. If you attempted to loop over a module that is using the state of overridden, then you are going to default the other interfaces. Given the following task:</p> <pre><code>  tasks:\n    - name: \"TASK 1: IOS &gt;&gt; Set some interfaces with merge\"\n      ios_interfaces:\n        state: overridden\n        config:\n          - name: \"{{ item }}\"\n            enabled: yes\n            description: \"Configured by Ansible\"\n      register: ios_interface_output\n      loop:\n        - \"GigabitEthernet0/2\"\n        - \"GigabitEthernet0/3\"\n      loop_control:\n        loop_var: item\n\n    - name: \"TASK 2: SYS &gt;&gt; DEBUG OUTPUT\"\n      debug:\n        msg: \"{{ ios_interface_output }}\"\n</code></pre> <p>You will have two loops. The first time through the loop when the loop_var is <code>GigabitEthernet0/2</code> you will have the other interfaces all defaulted, except <code>GigabitEthernet0/2</code> will have the state enabled and the description set to <code>Configured by Ansible</code>. The rest of the interface descriptions will be removed. MTU all set to default, and so on.  </p> <p>The second time through the loop the module will configure <code>GigabitEthernet0/3</code> with each of the interface configurations. Defaulting the rest of the interfaces on teh device. So the description at the end of this execution for <code>GigabitEthernet0/2</code> will be blank. Even though it was configured on the first loop through.</p> <p>To handle this you will need to define the interfaces within the context of the config. Here is the new configuration of the tasks:</p> <pre><code>  tasks:\n    - name: \"TASK 1: IOS &gt;&gt; Set some interfaces with merge\"\n      ios_interfaces:\n        state: overridden\n        config:\n          - name: \"GigabitEthernet0/2\"\n            enabled: yes\n            description: \"Configured by Ansible\"\n          - name: \"GigabitEthernet0/3\"\n            enabled: yes\n            description: \"Configured by Ansible\"\n      register: ios_interface_output\n\n    - name: \"TASK 2: SYS &gt;&gt; DEBUG OUTPUT\"\n      debug:\n        msg: \"{{ ios_interface_output }}\"\n</code></pre> <p>With having multiple interfaces defined under the config as another list item you are able to get both of the interfaces configured with what you are looking to do.</p> <pre><code>        \"after\": [\n            {\n                \"enabled\": true,\n                \"name\": \"loopback0\"\n            },\n            {\n                \"duplex\": \"auto\",\n                \"enabled\": true,\n                \"name\": \"GigabitEthernet0/0\",\n                \"speed\": \"auto\"\n            },\n            {\n                \"duplex\": \"auto\",\n                \"enabled\": true,\n                \"name\": \"GigabitEthernet0/1\",\n                \"speed\": \"auto\"\n            },\n            {\n                \"description\": \"Configured by Ansible\",\n                \"duplex\": \"auto\",\n                \"enabled\": true,\n                \"name\": \"GigabitEthernet0/2\",\n                \"speed\": \"auto\"\n            },\n            {\n                \"description\": \"Configured by Ansible\",\n                \"duplex\": \"auto\",\n                \"enabled\": true,\n                \"name\": \"GigabitEthernet0/3\",\n                \"speed\": \"auto\"\n            }\n        ],\n        \"before\": [\n            {\n                \"enabled\": true,\n                \"name\": \"loopback0\"\n            },\n            {\n                \"duplex\": \"auto\",\n                \"enabled\": true,\n                \"name\": \"GigabitEthernet0/0\",\n                \"speed\": \"auto\"\n            },\n            {\n                \"duplex\": \"auto\",\n                \"enabled\": true,\n                \"name\": \"GigabitEthernet0/1\",\n                \"speed\": \"auto\"\n            },\n            {\n                \"description\": \"BEFORE ANSIBLE\",\n                \"duplex\": \"auto\",\n                \"enabled\": true,\n                \"name\": \"GigabitEthernet0/2\",\n                \"speed\": \"auto\"\n            },\n            {\n                \"description\": \"BEFORE ANSIBLE\",\n                \"duplex\": \"auto\",\n                \"enabled\": true,\n                \"name\": \"GigabitEthernet0/3\",\n                \"speed\": \"auto\"\n            }\n        ],\n        \"changed\": true,\n        \"commands\": [\n            \"interface GigabitEthernet0/2\",\n            \"description Configured by Ansible\",\n            \"interface GigabitEthernet0/3\",\n            \"description Configured by Ansible\"\n</code></pre> <p>You now see within the commands key that both of hte interfaces are configured by Ansible. At this point with both interfaces being defined in the config parameter, you get both of the interfaces configured. To do this more programmatically  you would need to create the list ahead of time and feed the list of interfaces with their state into the module. This may be a future blog post.</p>","tags":["blog","ansible","cisco","deprecation"]},{"location":"ansible-cisco-ios-interfaces-module/#state-deleted","title":"State: Deleted","text":"<p>The task looks straight to the point for the deleted status. As expected. In this you define which interface name is to have the configuration removed, and then the module will default all of the sub-parameters of description, duplex, enabled, mtu, and speed. Note that the enabled default in the module is currently (2020-01-26) set to enabled.</p> <pre><code>  tasks:\n    - name: \"TASK 1: IOS &gt;&gt; Set some interfaces with merge\"\n      ios_interfaces:\n        state: deleted\n        config:\n            - name: GigabitEthernet0/3\n        register: ios_interface_output\n\n    - name: \"TASK 2: SYS &gt;&gt; DEBUG OUTPUT\"\n      debug:\n        msg: \"{{ ios_interface_output }}\"\n</code></pre> <p>When the configuration previously had an interface description and an MTU set, the following is the commands that are executed on just the single interface that is defined in the task:</p> <pre><code>\"commands\": [\n    \"interface GigabitEthernet0/3\",\n    \"no description\",\n    \"no mtu\"\n</code></pre>","tags":["blog","ansible","cisco","deprecation"]},{"location":"ansible-cisco-ios-user/","title":"Ansible Cisco IOS User Module","text":"<p>In this post I will be taking a look at some of the usability setup of managing Cisco IOS devices with the Ansible Cisco IOS User Module. This can be very helpful for setting up managed user accounts on systems, or the backup user accounts when you have TACACS or RADIUS setup.</p> <p>The module documentation overall looks complete from what I have done for user account management on devices in the past. There are a couple of interesting parameters available, that I may not get to completely on this post. There is support for aggregate, meaning that you can generate the configuration for multiple user accounts and pass it in as one. You can set a password in clear text that gets encrypted when on the device, or you can set a hashed_password with the type of hash and its corresponding value. And as expected with a module for setting user accounts you can also set the privilege level for which the user account uses.</p>","tags":["ansible","cisco","ios_user","netdevops","network automation"]},{"location":"ansible-cisco-ios-user/#ssh-before-setting-up-ssh-keys","title":"SSH Before Setting Up SSH Keys","text":"<p>You have probably seen this before, but for completeness sake I did get the output of the SSH login banner. This has the default lab setup on the device. So we do get a banner, but I'm getting prompted for a Password as well.</p> <pre><code>ssh rtr-1\nThe authenticity of host 'rtr-1 (10.250.0.167)' can't be established.\nRSA key fingerprint is SHA256:iyEgRBFlLhkW+Z2OOYWPvrjuzhTVY9wULmoHkWYgbrw.\nAre you sure you want to continue connecting (yes/no)? yes\nWarning: Permanently added 'rtr-1' (RSA) to the list of known hosts.\nWarning: the RSA host key for 'rtr-1' differs from the key for the IP address '10.250.0.167'\nOffending key for IP in /Users/joshv/.ssh/known_hosts:170\nAre you sure you want to continue connecting (yes/no)? yes\n\n**************************************************************************\n* IOSv is strictly limited to use for evaluation, demonstration and IOS  *\n* education. IOSv is provided as-is and is not supported by Cisco's      *\n* Technical Advisory Center. Any use or disclosure, in whole or in part, *\n* of the IOSv Software or Documentation to any third party for any       *\n* purposes is expressly prohibited except as otherwise authorized by     *\n* Cisco in writing.                                                      *\n**************************************************************************Password:\n</code></pre>","tags":["ansible","cisco","ios_user","netdevops","network automation"]},{"location":"ansible-cisco-ios-user/#adding-ssh-key-users","title":"Adding SSH Key Users","text":"<p>Copying from the example on the module definition, I went ahead and created a playbook that will create an account on the same device but with my local computer account. Here is the playbook:</p> <pre><code>---\n- name: \"PLAY 1: WORKING WITH IOS USER MODULE\"\n  hosts: cisco_routers\n  connection: network_cli\n  tasks:\n    - name: \"TASK 1: Add local username with SSH Key\"\n      ios_user:\n        name: joshv\n        nopassword: True\n        sshkey: \"{{ lookup('file', '~/.ssh/id_rsa.pub') }}\"\n        state: absent\n\n    - name: \"FINAL TASK: Save Config\"\n      ios_config:\n        save_when: always\n</code></pre> <p>It is a single play playbook, with 2 tasks. Task 1 will add the local id_rsa public key to the IOS device. The final task is a play to save the configuration.</p> <pre><code>ansible-playbook working_with_ios_user-1.yml\n\nPLAY [PLAY 1: WORKING WITH IOS USER MODULE] ****************************************************\n\nTASK [TASK 1: Add local username with SSH Key] *************************************************\n[WARNING]: Module did not set no_log for update_password\n[WARNING]: Module did not set no_log for password_type\nchanged: [r1]\n\nTASK [debug] ***********************************************************************************\nok: [r1] =&gt; {\n    \"msg\": {\n        \"ansible_facts\": {\n            \"discovered_interpreter_python\": \"/usr/bin/python\"\n        },\n        \"changed\": true,\n        \"commands\": [\n            \"ip ssh pubkey-chain\",\n            \"username joshv\",\n            \"key-hash ssh-rsa &lt;hash_masked&gt; joshv@&lt;adevice&gt;\",\n            \"exit\",\n            \"exit\",\n            \"username joshv nopassword\"\n        ],\n        \"failed\": false,\n        \"warnings\": [\n            \"Module did not set no_log for update_password\",\n            \"Module did not set no_log for password_type\"\n        ]\n    }\n}\n\nTASK [FINAL TASK: Save Config] *****************************************************************\nchanged: [r1]\n\nPLAY RECAP *************************************************************************************\nr1                         : ok=3    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n</code></pre> <p></p> <p>On execution one can see that the commands pushed in the debug task including setting up an IP SSH keypair, setting a username of joshv, and setting the key hash. Then Ansible exits to what is expected to be the first level of config mode and sets username <code>joshv</code> without a password.  </p> <p>Execution is pretty much what we would expect of adding a username to the device. Taking a look at if we get prompted when connecting to the device is a no, I do not.</p> <pre><code>$ ssh rtr-1\n\n**************************************************************************\n* IOSv is strictly limited to use for evaluation, demonstration and IOS  *\n* education. IOSv is provided as-is and is not supported by Cisco's      *\n* Technical Advisory Center. Any use or disclosure, in whole or in part, *\n* of the IOSv Software or Documentation to any third party for any       *\n* purposes is expressly prohibited except as otherwise authorized by     *\n* Cisco in writing.                                                      *\n**************************************************************************\n**************************************************************************\n* IOSv is strictly limited to use for evaluation, demonstration and IOS  *\n* education. IOSv is provided as-is and is not supported by Cisco's      *\n* Technical Advisory Center. Any use or disclosure, in whole or in part, *\n* of the IOSv Software or Documentation to any third party for any       *\n* purposes is expressly prohibited except as otherwise authorized by     *\n* Cisco in writing.                                                      *\n**************************************************************************\nrtr-1#\n</code></pre> <p>Taking a look at the configuration in the router, it looks exactly as we would expect. There are only two users configured. The first being the one that Ansible uses to connect to this device. The second being the one we just reconfigured.</p> <pre><code>rtr-1#show run | i username\nusername cisco secret 5 $1$GNTQ$RpNy.E9LZMzgrOz/g2pYJ.\nusername joshv nopassword\n  username joshv\n</code></pre> <p>On the output you see that there is the username <code>joshv</code> multiple times. One is in the generic username section that was created with the command <code>username joshv nopassword</code> and then another time that is within the public key section of the SSH configuration.  </p>","tags":["ansible","cisco","ios_user","netdevops","network automation"]},{"location":"ansible-cisco-ios-user/#removing-ssh-key-user","title":"Removing SSH Key User","text":"<p>To go along with creating an user on the device, I have created the playbook to remove the same user from the device. This is as simple as changing the state from <code>present</code> to <code>absent</code>. This will remove all of what was created on the device.</p> <pre><code>---\n- name: \"PLAY 1: WORKING WITH IOS USER MODULE\"\n  hosts: cisco_routers\n  connection: network_cli\n  tasks:\n    - name: \"TASK 1: Remove local username with SSH Key\"\n      ios_user:\n        name: joshv\n        nopassword: True\n        sshkey: \"{{ lookup('file', '~/.ssh/id_rsa.pub') }}\"\n        state: absent\n      register: config_output\n\n    - debug:\n        msg: \"{{ config_output }}\"\n\n    - name: \"FINAL TASK: Save Config\"\n      ios_config:\n        save_when: always\n</code></pre> <p>Execution looks extremely similar. Here it is:</p> <pre><code>PLAY [PLAY 1: WORKING WITH IOS USER MODULE] ****************************************************\n\nTASK [TASK 1: Remove local username with SSH Key] **********************************************\n[WARNING]: Module did not set no_log for update_password\n[WARNING]: Module did not set no_log for password_type\nchanged: [r1]\n\nTASK [debug] ***********************************************************************************\nok: [r1] =&gt; {\n    \"msg\": {\n        \"ansible_facts\": {\n            \"discovered_interpreter_python\": \"/usr/bin/python\"\n        },\n        \"changed\": true,\n        \"commands\": [\n            \"ip ssh pubkey-chain\",\n            \"no username joshv\",\n            \"exit\"\n        ],\n        \"failed\": false,\n        \"warnings\": [\n            \"Module did not set no_log for update_password\",\n            \"Module did not set no_log for password_type\"\n        ]\n    }\n}\n\nTASK [FINAL TASK: Save Config] *****************************************************************\nchanged: [r1]\n\nPLAY RECAP *************************************************************************************\nr1                         : ok=3    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n</code></pre> <p>In writing of this I did find what I would consider a bug within Ansilbe's ios_user. If you use an SSH Key with the credential, you will need to remove the user account with running the same taskk 2 times. This is filed under issue https://github.com/ansible/ansible/issues/68238</p> <p></p> <p>Executing the module a second time you get the full removal of the user account.</p> <pre><code>PLAY [PLAY 1: WORKING WITH IOS USER MODULE] ****************************************************\n\nTASK [TASK 1: Remove local username with SSH Key] **********************************************\n[WARNING]: Module did not set no_log for update_password\n[WARNING]: Module did not set no_log for password_type\nchanged: [r1]\n\nTASK [debug] ***********************************************************************************\nok: [r1] =&gt; {\n    \"msg\": {\n        \"ansible_facts\": {\n            \"discovered_interpreter_python\": \"/usr/bin/python\"\n        },\n        \"changed\": true,\n        \"commands\": [\n            {\n                \"answer\": \"y\",\n                \"command\": \"no username joshv\",\n                \"newline\": false,\n                \"prompt\": \"This operation will remove all username related configurations with same name\"\n            }\n        ],\n        \"failed\": false,\n        \"warnings\": [\n            \"Module did not set no_log for update_password\",\n            \"Module did not set no_log for password_type\"\n        ]\n    }\n}\n\nTASK [FINAL TASK: Save Config] *****************************************************************\nchanged: [r1]\n\nPLAY RECAP *************************************************************************************\nr1                         : ok=3    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n</code></pre>","tags":["ansible","cisco","ios_user","netdevops","network automation"]},{"location":"ansible-cisco-ios-user/#setting-username-and-password-no-key","title":"Setting Username and Password - No Key","text":"<p>Now that I have gone through the use of creating a SSH Key user, let's take a look at setting an user account on the device with a credential. I've created a local environmental variable named <code>NEW_PASSWORD</code> that has the credential that I wish to set the username to. This could be any lookup that gets a password, such as a lookup to a password manager.</p> <pre><code>---\n- name: \"PLAY 1: WORKING WITH IOS USER MODULE\"\n  hosts: cisco_routers\n  connection: network_cli\n  tasks:\n    - name: \"TASK 1: Add local username with SSH Key\"\n      ios_user:\n        name: josh2\n        configured_password: \"{{ lookup('env', 'NEW_PASSWORD') }}\"\n        state: present\n        privilege: 15\n      register: config_output\n\n    - debug:\n        msg: \"{{ config_output }}\"\n\n    - name: \"FINAL TASK: Save Config\"\n      ios_config:\n        save_when: always\n</code></pre> <p>The output on this particular setup is not idempotent. Each time the play will be run it will set a new username and password on the device due to the checking of the running configuration. You will need to add some additional logic to your playbook to have the task only executed when a condition is met.  </p> <p>Here is the execution. Note that Ansible masks the password being set.</p> <pre><code>PLAY [PLAY 1: WORKING WITH IOS USER MODULE] ****************************************************\n\nTASK [TASK 1: Add local username with SSH Key] *************************************************\n[WARNING]: Module did not set no_log for update_password\n[WARNING]: Module did not set no_log for password_type\nchanged: [r1]\n\nTASK [debug] ***********************************************************************************\nok: [r1] =&gt; {\n    \"msg\": {\n        \"ansible_facts\": {\n            \"discovered_interpreter_python\": \"/usr/bin/python\"\n        },\n        \"changed\": true,\n        \"commands\": [\n            \"username josh2 secret ********\"\n        ],\n        \"failed\": false,\n        \"warnings\": [\n            \"Module did not set no_log for update_password\",\n            \"Module did not set no_log for password_type\"\n        ]\n    }\n}\n\nTASK [FINAL TASK: Save Config] *****************************************************************\nchanged: [r1]\n\nPLAY RECAP *************************************************************************************\nr1                         : ok=3    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n</code></pre> <p></p>","tags":["ansible","cisco","ios_user","netdevops","network automation"]},{"location":"ansible-cisco-ios-user/#summary","title":"Summary","text":"<p>From the examples that I have given, hopefully this will help to see what you could do in your own environment. Need to regularly rotate an offline access password? A playbook may be a way that is low impact to get you on your way for automating the management of your Cisco IOS devices.  </p> <p>I also started with the use of SSH keys as well as this may be an under utilized method to log into devices. This sets up and uses a known cryptographic key set for authentication. Please check with the team/individuals responsible for security before implementing.  </p> <p>As always, I hope this has helped!</p> <p>I've added the Playbooks executed within this post to my collection of examples on Github at https://github.com/jvanderaa/ansible-using_ios.</p>","tags":["ansible","cisco","ios_user","netdevops","network automation"]},{"location":"apple_automator/","title":"Using Apple Automator to Open Projects","text":"<p>Today I'm going to walk through the newest part of my personal workflow for working with projects. Straight to the point, this is going to be using Apple Automator to quickly open your project that you wish to work on within VS Code, and presumably PyCharm as well.  </p>","tags":["vscode","mac","productivity"]},{"location":"apple_automator/#problem","title":"Problem","text":"<p>So what is the problem that I'm trying to solve? I am one that generally likes the workspace concept within VS Code, but I don't like having to maintain workspace files. I have found them a little difficult to maintain and keep organized. To that end I have found that there is an option to install VS Code shortcut into your OS path from the command pallet (cmd-P), and <code>path</code>.</p> <p></p> <p>Once this is installed, you can issue at a terminal (or iTerm2) prompt the command <code>code .</code> and this will bring up VS Code from the folder that you are currently working on.  </p> <p>This sounds great, what is the problem? Well, I tend to open a lot of iTerm2 tabs just to open up and go into VS Code. While working in VS Code, I then use the terminal that is baked into VS Code as my terminal. So I have a window that is open unnecessarily.</p>","tags":["vscode","mac","productivity"]},{"location":"apple_automator/#my-solution-apple-script-and-automator","title":"My Solution - Apple Script and Automator","text":"<p>So I was reminded about Apple Automator for some particular reason and I thought this would be a great solution to opening VS Code directly to my folder that I want to work on and be able to close VS Code windows whenever I was done working in a folder. So for me I use a specific directory on my Mac to have all of my projects in them. This is a typical directory structure:</p> <pre><code>projects\n\u251c\u2500\u2500 project1\n\u251c\u2500\u2500 project2\n</code></pre>","tags":["vscode","mac","productivity"]},{"location":"apple_automator/#automator-flow","title":"Automator Flow","text":"<p>The flow is going to have a prompt come to the top when a command shortcut is executed. So -&gt;:</p> <pre><code>command-shortcut -&gt; Select project -&gt; Open VS Code\n</code></pre>","tags":["vscode","mac","productivity"]},{"location":"apple_automator/#start-a-new-quick-action","title":"Start a New Quick Action","text":"<p>When creating a new document in your Automator, make sure to select the type as Quick Action. This type is needed in order to use the Keyboard shortcut later.</p>","tags":["vscode","mac","productivity"]},{"location":"apple_automator/#first-step-run-applescript","title":"First Step: Run AppleScript","text":"<p>I looked at a couple of options here to get the prompt to display. From what I could tell there was options in both AppleScript and JavaScript. I choose to stick with AppleScript in the current iteration for ease of access to the file system. I will have to do additional research, but that would be for a learning thing, not necessarily for productivity gains. I worked through a few links and eventually came up with the following AppleScript:</p> <pre><code>on run {input, parameters}\n    # Define the folder that is being used as project directory. This is the only thing that needs to be set\n    set projectFolder to \"/Users/joshv/projects/\"\n    # Use the finder application to get the list of all the folders inside of the folder\n    tell application \"Finder\"\n        set fileList to get name of folders of folder (projectFolder as POSIX file)\n    end tell\n    # Create a prompt\n    choose from list fileList with prompt \"Which project?\"\n    # Create a string of projectFolder with result of response\n    set resultString to projectFolder &amp; result as string\n    # Return the path\n    return the resultString as string\nend run\n</code></pre> <p>The only parameter that needs to be changed is the path to the directory on the third line, to match what is your own project directory.</p>","tags":["vscode","mac","productivity"]},{"location":"apple_automator/#second-step-run-shell-script","title":"Second Step: Run Shell Script","text":"<p>I then took the easy way out at the moment to execute an application. I will have to work to add this all into one AppleScript or JavaScript execution in the future, but for now I know this works. It is a short command, first I needed to find out where the <code>code</code> application that was mentioned above is stored. So I did the command <code>which code</code> to get my path to the code shortcut. The final result for me is:</p> <pre><code>/usr/local/bin/code $1\n</code></pre> <p>This takes an argument that is passed in from the AppleScript and passes it into the shell script. The next change I needed to do on the Run Shell Script module of Automator was to set Pass input to as arguments to pass it in as an argument.  </p> <p>This now looks like the following from an Automator application:  </p> <p></p>","tags":["vscode","mac","productivity"]},{"location":"apple_automator/#keyboard-shortcuts","title":"Keyboard Shortcuts","text":"<p>Last thing to do is to assign a keyboard shortcut to your automation.</p> <ol> <li>Open Keyboard preferences</li> <li>Select Shortcuts</li> <li>Select Services on the left</li> <li>Scroll to the section General</li> <li>Find the name of your document that you created in Automator</li> <li>Assign a keyboard shortcut, I'm using <code>CMD-Shift-'</code> for mine</li> </ol> <p> </p> <p>Now when I select the keyboard shortcut, I get a visual prompt of the folders in the project folder specified. And when I select the project and OK I am then taken to either a new VS Code window based in that directory or the already opened VS Code window for that project folder.  </p>","tags":["vscode","mac","productivity"]},{"location":"apple_automator/#summary","title":"Summary","text":"<p>Not all automation tools have to be Python, Ansible, or other modern language. You can use tools that are provided that may feel old to help you in your every day work. Hopefully this comes in handy for you as well! Let me know. In the end, if this helps great, otherwise this is good documentation for myself if needed in the future!  </p> <p>Thanks,</p> <p>-Josh</p>","tags":["vscode","mac","productivity"]},{"location":"docker_for_automation_environment_ansible_210/","title":"Docker for Automation Environment - Ansible 2.10","text":"<p>Docker is a terrific solution for making a consistent working environment. It's been about a year or so since I built my very first own Docker container. I had always known why you use a container, but was always intimidated too much so to even get started. I am glad that I did get started and am off on my journey of using Docker containers. Let me jump into the problem and why? Couple the recent experiences with Docker, and the upcoming move to slim down Ansible and install Collections for most Network Automation modules, I thought it would be a good thing to get a write up done.</p>","tags":["docker","ansible","netdevops"]},{"location":"docker_for_automation_environment_ansible_210/#problem","title":"Problem","text":"<p>From a Network Automation standpoint, there is much change still occurring in the tooling ecosystem. First that January 1, 2020 marked the end of support for Python 2.7. Yet there are still many setups that require Python 2.7.  </p> <p>Upcoming Ansible is changing the behavior from a full batteries included for Network Automation tooling, moving over to a base package where you install Collections on top of it. This is going to be, in my opinion, a second driver for really digging into using containers for your enterprise automation envrionment.</p>","tags":["docker","ansible","netdevops"]},{"location":"docker_for_automation_environment_ansible_210/#why-is-it-a-solution","title":"Why is it a Solution?","text":"","tags":["docker","ansible","netdevops"]},{"location":"docker_for_automation_environment_ansible_210/#ansible-experience","title":"Ansible Experience","text":"<p>This is a solution because it helps that you do your development work within a container. When you run the command <code>ansible-playbook</code> you are getting the <code>ansible-playbook</code> executable that is built into the container image. If outside of a container, there are several things that could happen. You may install some things with Python 2, could with Python 3, which version of Python 3? Which version of Python does the executables associated with Ansible reference? There are several methods to get multiple versions of Python to be executing on your system. Installing Ansible dependencies into the wrong Python PIP can definitely hamper it.</p>","tags":["docker","ansible","netdevops"]},{"location":"docker_for_automation_environment_ansible_210/#i-dont-use-ansible-why-then","title":"I Don't Use Ansible, Why Then?","text":"<p>This is an answer still for both Ansible and Python a like. One, if you mess up an installation you just rebuild the container image. Yes, it is shorter to just delete a virtual environment as well. When you go to install the Python application into the customer environment, you get portability, as you bring your own Python installation with the container. As long as the customer supports containers it adds portability. No more differing sets of instructions. Just a simple, docker command or update of a docker-compose file and away you go.  </p>","tags":["docker","ansible","netdevops"]},{"location":"docker_for_automation_environment_ansible_210/#portability","title":"Portability","text":"<p>One of the tenants of the 12-factor app for developing modern apps is to ask the question if you could open source your project tomorrow. If developing in virtual environments you will still need to have a setup instruction set that could miss something that you just have in your environment. With a container you bring a blank OS to the table inside of the container. You install explicitly everything you need to get the app up and running.</p>","tags":["docker","ansible","netdevops"]},{"location":"docker_for_automation_environment_ansible_210/#rhel-vs-ubuntu-vs-macos-vs-any-other-linux-os","title":"RHEL vs Ubuntu vs MacOS vs Any Other Linux OS","text":"<p>The next thing about containers if you are going to interact with a file system in any way is that you get consistency for your app. No longer do you need to understand how to interact with MacOS with this command, and CentOS is this command, and Ubuntu is this other command. You choose the base OS image, and interact with it as such.</p>","tags":["docker","ansible","netdevops"]},{"location":"docker_for_automation_environment_ansible_210/#getting-started","title":"Getting Started","text":"<p>My methodology for getting started with Docker was to read some getting started guides, and adapt them for my Network Automation flavor. I'm going to try to walk you along so you can learn from some of the things I learned along the way, and improve upon them.  </p> <p>I started off with installing Docker Desktop on my Mac. The best way to get started is to install Docker onto your OS of choice. The installation guide is https://docs.docker.com/get-docker/. There is also a HomeBrew package available for installing Docker as well.  </p>","tags":["docker","ansible","netdevops"]},{"location":"docker_for_automation_environment_ansible_210/#docker-labs-on-the-internet","title":"Docker Labs on the Internet","text":"<p>There are several resources available on the Internet for you to get experience with Docker.</p> <ul> <li>Cisco DevNet Learning Labs</li> <li>Play with Docker Labs </li> </ul> <p>I encourage you to take a look at these if you do not want to get started on your own in your own machine. The following examples will be done on your local machine.</p>","tags":["docker","ansible","netdevops"]},{"location":"docker_for_automation_environment_ansible_210/#writing-a-dockerfile","title":"Writing A Dockerfile","text":"<p>A Dockerfile is the set of instructions of how to build your container. So if you want to follow along, create a directory anywhere that is accessible via command line called <code>docker_test</code>. Take the following and paste it into a file called <code>Dockerfile</code>.  </p> <pre><code>FROM python:3.8.3-slim-buster\n\nRUN apt-get update &amp;&amp; apt-get install sshpass vim -y\n\nCOPY . /local\nWORKDIR /local\n\nRUN pip install -r requirements.txt\nRUN ansible-galaxy collection install -r requirements.yml\n</code></pre> <p>What this is doing:</p> Line Number Outline 1 Selecting the base image, this can be found on https://hub.docker.com, searching Python 3 Installing <code>sshpass</code> and <code>vim</code> via apt. Updating the apt repo list first. Being a slim image, VIM is not pre-installed 5 Copies everything in the local directory into a directory /local 6 Changes the working directory, similar to <code>cd</code> in many OSes 8 Installs Python packages from the local <code>requirements.txt</code> file that was copied on line 5 9 Uses <code>ansible-galaxy</code> to install Galaxy collections from the local <code>requirements.yml</code> file <p>To have this work right, lets use this as an opportunity to test out installing the new version of Ansible into a container, by pip installing <code>ansible-base</code>.  </p> <p>In order to build this container, you will need to have a requirements file ready to go for both Python and Ansible Galaxy. So to this effort, here are those two files with a few packages:</p>","tags":["docker","ansible","netdevops"]},{"location":"docker_for_automation_environment_ansible_210/#requirementstxt","title":"requirements.txt","text":"<pre><code>ansible-base\n</code></pre> <p>The only Python package going to install this for is <code>ansible-base</code> which will install the current beta versions of Ansible Base 2.10.</p>","tags":["docker","ansible","netdevops"]},{"location":"docker_for_automation_environment_ansible_210/#requirementsyml","title":"requirements.yml","text":"<pre><code>---\n# Collection Installations\ncollections:\n  - name: cisco.asa\n    version: 1.0.0\n</code></pre> <p>This has a few more keys to the requirements.yml file that Ansible Galaxy will install with. The YML file first has a key of <code>collections</code>. This is because you can use the file to install both roles and collections, not just collections. Here this will install the 1.0.0 release of the Ansible modules for the Cisco ASA platform. You will likely need to add additional roles to here. To get the name you leverage https://galaxy.ansible.com to search and find the modules that you would install.</p>","tags":["docker","ansible","netdevops"]},{"location":"docker_for_automation_environment_ansible_210/#helper","title":"Helper","text":"<p>The last piece I'd like to provide some info on as well is a helper file. Many of the *nix systems have Make available to them. If unable to use Make on your filesystem, I will suggest to take a look at the Python package invoke. Invoke does have more flexibility than make as it is written in Python, but for this demo I want to use Make so you can see the command line commands that are used.  </p> <p>The arguments for Make are made available by defining information in a <code>Makefile</code>. Here is the Makefile that I will be using for this:</p> <pre><code>IMG_NAME=jvanderaa/network_automation\nIMG_VERSION=2.0-rc2\n.DEFAULT_GOAL := cli\n\n.PHONY: build\nbuild:\n    docker build -t $(IMG_NAME):$(IMG_VERSION) . \n\n.PHONY: cli\ncli:\n    docker run -it \\\n        -v $(shell pwd):/local \\\n        -w /local \\\n        $(IMG_NAME):$(IMG_VERSION) bash\n</code></pre> Line Number Action 1 Defining a variable for image name, here <code>jvanderaa/network_automation</code> 2 Defining a variable for IMG_VERSION so you can version your Docker containers 3 Setting a default goal so you can just type make and that is what will be done 5 .PHONY is saying that this is a phony file that is upcoming. It is best practice to include but not required 6 The key build: is what will be executed with the <code>make build</code> command 7 The actual command, it is tabbed in. You MUST NOT use spaces and MUST use tabs with Make 9 Definition of .PHONY for CLI 10 Defining a key of cli for <code>make cli</code> or just <code>make</code> due to the default goal defined 11 Start of the Docker run command, which includes mapping the local directory into the container directory so you can make live updates, changing the working directory, and launching bash","tags":["docker","ansible","netdevops"]},{"location":"docker_for_automation_environment_ansible_210/#building-the-container","title":"Building The Container","text":"<p>First in my container image list I have no containers (I just pruned them all, and they are now all deleted with the command <code>docker system prune -a</code>):</p> <pre><code>$ docker image ls       \nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\n</code></pre> <p>With no containers, I execute the command from the <code>Makefile</code> of <code>make build</code>. This will download all of the layers, run the apt installations, pip install, and galaxy install. The output is below with many of the lines removed.</p> <pre><code>docker build -t jvanderaa/network_automation:2.0-rc2 . \nSending build context to Docker daemon   5.12kB\nStep 1/6 : FROM python:3.8.3-slim-buster\n3.8.3-slim-buster: Pulling from library/python\n8559a31e96f4: Pull complete \n62e60f3ef11e: Pull complete \n93c8ae153782: Pull complete \nea222f757df7: Pull complete \ne97d3933bbbe: Pull complete \nDigest: sha256:938fd520a888e9dbac3de374b8ba495cc50fe96440030264a40f733052001895\nStatus: Downloaded newer image for python:3.8.3-slim-buster\n ---&gt; 9d84edf35a0a\nStep 2/6 : RUN apt-get update &amp;&amp; apt-get install sshpass vim -y\n ---&gt; Running in 66e37abb454a\n\n\n[ I REMOVED A BUNCH OF LINES HERE]\n\nStep 6/6 : RUN ansible-galaxy collection install -r requirements.yml\n ---&gt; Running in 94ca0bb3f3c9\nStarting galaxy collection install process\nProcess install dependency map\nStarting collection install process\nInstalling 'cisco.asa:1.0.0' to '/root/.ansible/collections/ansible_collections/cisco/asa'\nInstalling 'ansible.netcommon:1.0.0' to '/root/.ansible/collections/ansible_collections/ansible/netcommon'\nRemoving intermediate container 94ca0bb3f3c9\n ---&gt; 0805ddb1719f\nSuccessfully built 0805ddb1719f\nSuccessfully tagged jvanderaa/network_automation:2.0-rc2\n</code></pre> <p>If you have followed along, congratulations, you have created your first Docker Image. This is what will be used to create additional containers, which are a copy of the image, but a whole separate container.  </p> <p>Because of the tags, I prefer to use the <code>Makefile</code> to execute my containers as well. Now I just go to the command line within the container by issuing <code>make cli</code> command. This will then take me to the root user prompt of my conatiner.</p> <pre><code>$ make cli  \ndocker run -it \\\n                -v /Users/joshv/projects/docker_test:/local \\\n                -w /local \\\n                jvanderaa/network_automation:2.0-rc2 bash\nroot@58a4ea071203:/local# ansible-galaxy collection list\n\n# /root/.ansible/collections/ansible_collections\nCollection        Version\n----------------- -------\nansible.netcommon 1.0.0  \ncisco.asa         1.0.0  \n</code></pre> <p>Inside of the container I execute the command <code>ansible-galaxy collection list</code>. This now shows me that there are two collections installed:</p> <ul> <li>ansible.netcommon</li> <li>cisco.asa</li> </ul>","tags":["docker","ansible","netdevops"]},{"location":"docker_for_automation_environment_ansible_210/#summary","title":"Summary","text":"<p>With the container built, and a Dockerfile in place. I can now upload the Dockerfile along with the rest of the project file to Git. This coupled with the Makefile will help to build any system quickly. No more trying to find a place to host the Docker image (like Docker Hub), how to install into the proper Python executable any modules that may be needed (like pandevice for PANOS modules) or other SDKs that are helpers to the Ansible Collections being created.  </p> <p>Hopefully this has been helpful. Take a look at the links!</p> <p>Thanks,</p> <p>-Josh</p>","tags":["docker","ansible","netdevops"]},{"location":"ansible_for_enterprise/","title":"Ansible for Enterprise","text":"<p>One of the appealing features that I have towards working with Ansible is that it is able to automate components across the entire Enterprise IT stacks. Rather than having to stitch together your network, server, and desktop automation tools, there is at least one automation tool that will work with just about your entire IT stack. In this I will take a high level overview of some of the features that are there for you to explore.</p>","tags":["ansible","enterprise design"]},{"location":"ansible_for_enterprise/#ansible-for-network-automation","title":"Ansible for Network Automation","text":"<p>The first area that I will be brief on is from the network side of things. I am a long time network engineer and that is close to my heart.  </p> <p>Ansible is agentless and uses SSH as it's communication path. That leads well to interacting with some of the more legacy network devices. Ansible also supports using the newer tooling of APIs from devices, so until all of your entire Enterprise IT infrastructure supports API calls for automation, Ansible can definitely fit the bill.  </p> <p>The other interesting shift in the modules for networking is the move towards helping with intent based configuration. The newer modules being written by the Ansible team have an absolute intent configuration to them. This being that you need to send through your entire defined state to the modules, or else they will be seen as intended to have a blank configuration. The modules will then configure the devices as such. To see more on that look at my post on the interfaces module.</p> <p>If you are running an OS in your network that is Linux based, then you are in luck as well! Continue to the next section about Linux automation.</p>","tags":["ansible","enterprise design"]},{"location":"ansible_for_enterprise/#ansible-for-linux-server-automation","title":"Ansible for Linux Server Automation","text":"<p>This is the original purpose of Ansible. It was built to automate Linux systems. Many of the core modules that will be part of the Ansible base moving forward are modules that you use to manage Linux systems. This is an absolute fit for the market. The times that I have written Ansible Playbooks for Linux OS it has been a joy to work with and works very smoothly.</p>","tags":["ansible","enterprise design"]},{"location":"ansible_for_enterprise/#ansible-for-docker","title":"Ansible for Docker","text":"<p>Ansible is able to automate your Docker environment as well. With support for both Docker containers and docker-compose functionality. This will help you through your life cycle of Docker containers. Although it does not get to the level of what Kubernetes will do from an orchestration level without some level of effort.</p>","tags":["ansible","enterprise design"]},{"location":"ansible_for_enterprise/#ansible-for-windows","title":"Ansible for Windows","text":"<p>Ansible for Windows is a thing! Although I do think it takes a little more effort to get off the ground than even the network side. You need to enable WinRM on the Windows host for the functionality to work. After that under the hood instead of using Python Ansible is leveraging PowerShell code to interact with Windows OS. So yes, you can automate Windows devices.</p>","tags":["ansible","enterprise design"]},{"location":"ansible_for_enterprise/#ansible-for-macos","title":"Ansible for MacOS","text":"<p>Being a *nix operating system, you can manage your Mac deployment with Ansible. Now you just need to make sure the hosts are online when executing. So there isn't an out of the box check in agent within Ansible. That's what makes Ansible awesome for networking is that it is agentless. Take a look at Ansible for your Macs.</p>","tags":["ansible","enterprise design"]},{"location":"ansible_for_enterprise/#ansible-for-cloud","title":"Ansible for Cloud","text":"<p>I will need to find the link again, but Ansible is one of the largest percentage increase in tools to manage your cloud environments. There are quite the number of modules available for the leading public cloud (and private) providers. If there isn't a specific module, one characteristic of a good cloud environment these days is the ability to have a REST API. With Ansible you can leverage the URI module for this.  </p> <p>On the cloud module front, take a look at the table below. This is the number of modules that there are within Ansible for managing their cloud environment. In my opinion, that is quite a bit and can get you what you need.</p> Cloud Module Count AWS 45+ Azure 169 Oracle Cloud 30 \u201cservices\u201d Google 153 Digital Ocean 22 Rackspace 26 Avi Networks 65 VMWare 140+","tags":["ansible","enterprise design"]},{"location":"ansible_for_enterprise/#downside-of-ansible","title":"Downside of Ansible","text":"<p>The biggest downside to leveraging Ansible would be the timeliness of execution. If you are looking for speed on execution, then Ansible would either need some tweaks (such as installing mitogen).  </p> <p>When it comes to automation, in my book the first gain is not speed. Speed is a by product of not having to do rework and to move verification into an automated state. The real gain is by having a consistent environment. Then moving your operations into Playbooks, which many may have heard of a \"runbook\" which defines the process. Now your process is defined into a system that actually does something.</p>","tags":["ansible","enterprise design"]},{"location":"ansible_for_enterprise/#summary","title":"Summary","text":"<p>In my opinion Ansible is still a right tool for the job when it comes to automating both your network environments, and your entire enterprise IT stack. I encourage you to take a look, evaluate other options as well. I hope this summary may be helpful. Let me know your thoughts in the comments or on Twitter/LinkedIn.</p> <p>Josh</p>","tags":["ansible","enterprise design"]},{"location":"practicing_with_gns3/","title":"Practicing Network Automation with GNS3","text":"<p>In an earlier post I took a look at how to setup EVE-NG to get access to virtualized network devices and topologies. This post is going to take a look at how to setup GNS3 systems to allow access.</p> <p>In the overall topology that is a \"home\" network sits a device that supports a routing protocol, usually either OSPF or BGP. What is known to work at an inexpensive price point is the Ubiquiti EdgeRouter X. </p>","tags":["ansible","cisco","network simulator","gns3"]},{"location":"practicing_with_gns3/#gns3-setup","title":"GNS3 Setup","text":"<p>This post is not a post on how to setup GNS3, it is meant to help you start to access devices. This tutorial is running a GNS3 VM on a remote host. Take a look at the GNS3 docs on how to install GNS3 specifically.</p>","tags":["ansible","cisco","network simulator","gns3"]},{"location":"practicing_with_gns3/#gns3-configuration","title":"GNS3 Configuration","text":"<p>The topology item used to create the connection to the production network is the device type of Cloud. Add a cloud with the general connection and the device will have connectivity to your local network.</p> <p>Next setup a router (in this instance using a Cisco vIOS image - licensed item). Connect that device to the cloud that was added to the topology. In this particular setup, DHCP is being used. </p> <p></p>","tags":["ansible","cisco","network simulator","gns3"]},{"location":"practicing_with_gns3/#router-configuration","title":"Router Configuration","text":"<pre><code>rtr-edge#show run interface GigabitEthernet0/0\nBuilding configuration...\n\nCurrent configuration : 110 bytes\n!\ninterface GigabitEthernet0/0\n ip address dhcp\n duplex auto\n speed auto\n media-type rj45\n no cdp enable\nend\n</code></pre> <p>With the device getting an address, the device also gets a floating default static route imported to match the DNS request:</p> <pre><code>rtr-edge#show ip route\nCodes: L - local, C - connected, S - static, R - RIP, M - mobile, B - BGP\n       D - EIGRP, EX - EIGRP external, O - OSPF, IA - OSPF inter area \n       N1 - OSPF NSSA external type 1, N2 - OSPF NSSA external type 2\n       E1 - OSPF external type 1, E2 - OSPF external type 2\n       i - IS-IS, su - IS-IS summary, L1 - IS-IS level-1, L2 - IS-IS level-2\n       ia - IS-IS inter area, * - candidate default, U - per-user static route\n       o - ODR, P - periodic downloaded static route, H - NHRP, l - LISP\n       a - application route\n       + - replicated route, % - next hop override, p - overrides from PfR\n\nGateway of last resort is 192.0.2.1 to network 0.0.0.0\n\nS*    0.0.0.0/0 [254/0] via 192.0.2.1\n      192.0.2.0/24 is variably subnetted, 2 subnets, 2 masks\nC        192.0.2.0/24 is directly connected, GigabitEthernet0/0\nL        192.0.2.163/32 is directly connected, GigabitEthernet0/0\n</code></pre> <p>Verify that you have access to the Internet by using ICMP to test.</p> <pre><code>rtr-edge#ping 1.1.1.1\nType escape sequence to abort.\nSending 5, 100-byte ICMP Echos to 1.1.1.1, timeout is 2 seconds:\n!!!!!\nSuccess rate is 100 percent (5/5), round-trip min/avg/max = 13/16/18 ms\n</code></pre>","tags":["ansible","cisco","network simulator","gns3"]},{"location":"practicing_with_gns3/#setup-automation","title":"Setup Automation","text":"<p>By using OSPF you are able to setup networks and advertise them back into your \"production\"/\"home\" network. With the network being advertised you can then setup your hosts with addressing that would have access from the network.</p> <p>It is recommended that you test SSH/API connectivity into the GNS3 environment manually.</p>","tags":["ansible","cisco","network simulator","gns3"]},{"location":"practicing_with_gns3/#summary","title":"Summary","text":"<p>With the tools of GNS3, EVE-NG, Cisco CML, and VRNetLab you have significant choice in looking at tools that will help you to level up your skills in Network Automation.</p> <p>Hope that this may help you out in some way!</p> <p></p> <p>Josh</p>","tags":["ansible","cisco","network simulator","gns3"]},{"location":"blog_update_v2/","title":"2020 Blog Update","text":"<p>I've changed a few things on the site. Sorry about that! URLs have changed. Over the past week or so I have been working through making some what originally were small updates to the blog, that turned into a little too much effort. I was hoping to add a little bit of polish to the site while keeping the content in place. Earlier in 2020, maybe even back in 2019 I had become aware of Hashnode from the posts of David Flores - aka NetPanda who is on the Hashnode side at https://davidban77.hashnode.dev/. I liked many things that the blogging site has to offer. From a very quick up and running, to having a strong start of a community.</p>","tags":["blog","hashnode","jekyll","hugo"]},{"location":"blog_update_v2/#hashnode-trial","title":"Hashnode Trial","text":"<p>I decided to move the blog to Hashnode as a let's get started. I found it is easy to move the site as they already supported Markdown, which is what I write my blogs in already. The only downside that I originally saw as that there were no line numberings on the code blocks. I can live without that, but I still did desire it. I added my domain name and made the necessary DNS updates and it started to work out.</p>","tags":["blog","hashnode","jekyll","hugo"]},{"location":"blog_update_v2/#downsides-of-moving-to-hashnode","title":"Downsides of Moving to Hashnode","text":"<p>In the process I found that my first downside was that I lost some of my search engine rankings. One of the more popular over time posts was now gone from the search list. This is better as now my posts with Network to Code and some more recent post updates are showing up at the top of the list. Just wanted to make sure that was known.  </p> <p>An unexpected downside of the move was that Hashnode appeared to enable HSTS on the domain josh-v.com. The implications of this is that on my development hosts that get the domain extension josh-v.com are then also expecting to be HTTPS. And the browsers automatically forwarded to the HTTPS domain. So I just went ahead and moved them to a new domain. Nothing that I couldn't overcome, but it took a short bit to figure that out.</p>","tags":["blog","hashnode","jekyll","hugo"]},{"location":"blog_update_v2/#move-away-from-hashnode","title":"Move Away from Hashnode","text":"<p>In the end I found that when my domain was moved over there was not a way to customize the RSS feed. I chose to move back for that reason. I also want to be able to have a little more control on my domain about which URLs are HTTPS and which ones are not. And there are some other implications for the future as well. I moved back to the GitLab Pages that was still in place. This migration was an easy undo action.</p>","tags":["blog","hashnode","jekyll","hugo"]},{"location":"blog_update_v2/#hugo-evaluation","title":"Hugo Evaluation","text":"<p>Hugo has a tremendous upside to it as the static site generator platform. It is written in Go, which helps with its speed, and can help you work with GoLang learning. There is an awesome centralized theme gallery for you to view possible themes to apply, and their features/code. It is extremely flexible.  </p> <p>The one thing that I could not get to work out right was the image sizes on the site. When I tested locally and on GitHub pages (note that my main site is hosted on GitLab pages) the images would not resize to match the article. I found several posts that indicated to make a shortcode and then call the image that way, however that didn't seem to resolve my issues. So after a couple of evenings of attempting to figure this out that is when I turned back to check into the Jekyll themes arena.</p>","tags":["blog","hashnode","jekyll","hugo"]},{"location":"blog_update_v2/#jekyll-themes","title":"Jekyll Themes","text":"<p>The one downside is there is not a great system for themes like Hugo has, but there were some theme galleries around. I decided first to take a look at Minimal Mistakes to see if there was something that I could do. And there was. There had been many updates to the theme, and it has had an extreme amount of flexibility and capabilities added on. I got the theme up and running and then tested out what my existing blog posts would look like. They looked terrific, and just what I was looking for. I had a few small tweaks to make. I also needed to update the GitLab CI process for the new theme version. This has been a pleasant experience thus far. The couple of downsides that I see are that I need to research how to add a copy button to the code snippets where I would like them. I can handle that. And then the deployment length has some added pieces due ot the Ruby install process. I can live with this for now and have some paths forward to take.  </p> <p>I love the new layouts in it. I like the table of contents options that will be on most if not all of my posts. I have a few tweaks on some presentations which shouldn't be too hard.  </p> <p>The only downside thus far to the move to the newer version (and maybe this was my fault from 2+ years ago) was that my posts now have new URLs. So if I was counting on someone bookmarking a post, these have now changed. I hope that these URLs will now remain in the future as they seem consistent on platforms from Hashnode, Hugo, and Jekyll now.  </p> <p>So for now, I am staying on Jekyll. </p>","tags":["blog","hashnode","jekyll","hugo"]},{"location":"blog_update_v2/#python-pelican","title":"Python - Pelican","text":"<p>I'd love to look at moving this blog to being a Pelican theme. However, when I researched some of the features I was looking for they just didn't seem to have it yet. I know Python the best and can contribute at times, but now is not the time for me to be contributing to this. I have other priorities that I know I won't be able to tackle this type of adventure at this time. Maybe some point in the future.</p>","tags":["blog","hashnode","jekyll","hugo"]},{"location":"blog_update_v2/#hashnode-good-platform","title":"Hashnode - Good Platform","text":"<p>I did a small amount of negative points around Hashnode. There is a TON of good things about the platform. I do intend to keep an eye on how the platform moves on. I have opened several feature requests to hopefully get the last pieces that I would need to move over to them. There are several awesome things that are going on within the platform:</p> <ul> <li>Using modern techniques, writing within Markdown and providing a modern browser interface is good</li> <li>Easy, no research needed features</li> <li>Image resizing! This is why I didn't move to Hugo</li> <li>Nice layouts pre-built</li> </ul> <p>I absolutely would look at Hashnode if I were starting out on my blog. There is a great amount of features available. I had the opportunity to stick with something else.</p>","tags":["blog","hashnode","jekyll","hugo"]},{"location":"blog_update_v2/#tldr","title":"TLDR","text":"<ol> <li>Moved to Hashnode, had some quirks that I didn't like</li> <li>Moved back to GitLab Pages!</li> <li>Tried hugo, but couldn't get images to work right</li> <li>Moved back to Jekyll with an updated theme and here we are!</li> </ol>","tags":["blog","hashnode","jekyll","hugo"]},{"location":"homeassistant-prometheus/","title":"Home Assistant Prometheus Exporting Setup","text":"<p>There does not appear to be a complete set of documentation pieces available for setting up Prometheus on the Home Assistant platform. This post will take you along on my journey of setting up the Home Assistant to get metrics from it. The link for the documentation is a good start at getting Prometheus installed. https://www.home-assistant.io/integrations/prometheus/</p>"},{"location":"homeassistant-prometheus/#starting-prometheus","title":"Starting Prometheus","text":"<ol> <li>Edit your <code>configuration.yaml</code> file</li> <li>Add in a key of <code>prometheus:</code> </li> <li>Add in any parameters you may need, but just they key alone is enough to start the exporter</li> </ol> <p>Once you have started the exporter, I was still getting a 404 not found. So I did restart the Home Assistant.</p>"},{"location":"homeassistant-prometheus/#installation-of-prometheus-endpoint","title":"Installation of Prometheus Endpoint","text":"<p>The first thing that is different from most of the times that I have used Prometheus is that this implementation puts the information behind an authorization page. This is not so bad, but it definitely threw me for a loop for a short bit. There are a few options that I looked at for getting past the authentication issue</p>"},{"location":"homeassistant-prometheus/#legacy-tokens","title":"Legacy Tokens","text":"<p>The first thing I looked at was the legacy tokens. But quickly moved beyond this as Legacy wording is key. It is going away in the future and since this is a new setup, I didn't want to use anything legacy.</p>"},{"location":"homeassistant-prometheus/#trusted-networks-authentication-providers","title":"Trusted Networks &amp; Authentication Providers","text":"<p>I started to take a look at auth_providers and specifically the Trusted Networks aspect. I would love to be able to see what the Prometheus HTTP page looks like. However, making changes to the authentication mechanisms seemed like overkill for what I was looking to do.</p> <p>Continuing to look at the Prometheus example:</p> <pre><code># Example Prometheus scrape_configs entry\n  - job_name: 'hass'\n    scrape_interval: 60s\n    metrics_path: /api/prometheus\n\n    # Legacy api password\n    params:\n      api_password: ['PASSWORD']\n\n    # Long-Lived Access Token\n    bearer_token: 'your.longlived.token'\n\n    scheme: https\n    static_configs:\n      - targets: ['HOSTNAME:8123']\n</code></pre> <p>I kept finding the comment of \"Long-Lived Access Token\". This seems ideal for what we would want in an API based application. </p>"},{"location":"homeassistant-prometheus/#long-lived-access-token","title":"Long Lived Access Token","text":"<p>This seems like the ideal state to get into. Setting this token is outlined https://developers.home-assistant.io/docs/auth_api/#long-lived-access-token. To get at the profile page for your user:</p> <ol> <li>Select your username on the lower left</li> <li>Scroll to the very bottom</li> <li>Select Create Token</li> <li>Save this token to your password manager \u2192 It will go away from sight</li> </ol>"},{"location":"homeassistant-prometheus/#setup-prometheus","title":"Setup Prometheus","text":"<p>The final configuration that I used for the Prometheus scraping was set:</p> <pre><code>  - job_name: 'homeassistant'\n    scrape_interval: 60s\n    metrics_path: /api/prometheus\n    bearer_token: &lt;token&gt;\n    static_configs:\n      - targets:\n        - \"192.0.2.10:8123\"\n</code></pre> <p>Substitute \"192.0.2.10\" with the IP address/name of your home assistant host.</p>"},{"location":"homeassistant-prometheus/#verify-metrics","title":"Verify Metrics","text":"<p>Once the scraping has been setup, go to your Prometheus end point, then search. My devices started showing up immediately with the measurement prefix of <code>hass</code> when there was nothing more specifically setup.</p>"},{"location":"homeassistant-prometheus/#summary","title":"Summary","text":"<p>Gathering metrics about your Home Automation platform is a fun thing to do, and to continue to gather experience on how to work with the modern metrics tooling. Whether the database is Prometheus or InfluxDB or other option, this can be helpful. This walked through looking at a couple of options for setting up Prometheus. Then once Prometheus was setup on the Home Assistant side, went and setup the Prometheus scraping configuration. Finally how to verify that you are getting appropriate metrics ingested.  </p> <p>Note that I look to have a post in the future on why Home Assistant, where I have installed Home Assistant (there is some thought on this), and perhaps some of my graphs that I have being stored in Grafana at this time. I also hope to go through and setup the same for an InfluxDB instance as well.  </p> <p>Hope this has helped. Leave a comment or give a thumbs up or down!  </p> <p>Josh</p>"},{"location":"netbox_ansible_allocate_prefix_ipaddress/","title":"Ansible + NetBox: Getting Next Prefix / IP","text":"<p>This originates from a conversation had on Twitter about how to get the IP Prefix information from an IPAM tool, specifically   NetBox using Ansible. There are a couple of methodologies to go through, and I had originally started down the path of using the URI module. Which could be done. The more elegant solution is to use the NetBox Ansible Collections to handle the logic for you! Let\u2019s take a look.</p> <p>Thank you to @ttl255 for the inspiration to the journey with the Collection!</p> <p>Note</p> <p>This post was created when NetBox was an open source project used often in my automation framework. I have moved on to using Nautobot due to the project vision and providing a methodology that will drive network automation forward further. You may want to take a look at it yourself.</p> <p>The final playbook will be posted at the very bottom.</p>"},{"location":"netbox_ansible_allocate_prefix_ipaddress/#setup","title":"Setup","text":"<p>The NetBox environment for this is NetBox 2.9.9. I have not tested with previous versions, but believe that this will work with 2.8.x as well. The Ansible execution environment is Ansible 2.9.15. This is making use of the Ansible NetBox Collections using the FQCN for the NetBox modules and NOT the core modules.  </p> <p>The first thing to note with this is that there are two variables in the environment to help this. They are the URL and TOKEN. This is good practice to help pass this through without much changes. The primary prefix at the top of the code should likely also move into the environment so that it can be changed.</p> <pre><code>NETBOX_URL\nNETBOX_TOKEN\n</code></pre> <p>The lab device that will be having the configuration updated is the edge router of my GNS3 lab. This had one more interface available on it for me to change and rather than change things up significantly, I just used this device.</p>"},{"location":"netbox_ansible_allocate_prefix_ipaddress/#scenario","title":"Scenario","text":"<p>To get the next available Prefix from NetBox, and assign the IP address to the interface on the router. Success criteria for this scenario include:  </p> <ol> <li>Allocate a /24 network prefix within NetBox for use</li> <li>Allocate the first usable (192.0.2.0/24 would be 192.0.2.1) as allocated within NetBox</li> <li>Add the IP address configuration to the router interface GigabitEthernet0/3</li> <li>Add the network to area 0 of the OSPF configuration</li> </ol> <p>The device has already been created in NetBox with all of the necessary interfaces. A separate post will be created around adding devices to NetBox. To get started on this I suggest taking a look at a YouTube video that I did for the Ansible Minneapolis Meetup - https://www.youtube.com/watch?v=GyQf5F0gr3w and the corresponding [GitHub repo]</p>"},{"location":"netbox_ansible_allocate_prefix_ipaddress/#netbox-prefix-allocation","title":"NetBox Prefix Allocation","text":"<p>Here are the start the NetBox prefix allocation only has a single prefix defined at the start. Only 10.21.0.0/16, which is going to be the parent prefix.</p> <p></p> <p>We can see that there are no children prefixes and the current allocation is 0.</p> <p></p>"},{"location":"netbox_ansible_allocate_prefix_ipaddress/#creating-a-prefix-within-netbox","title":"Creating a Prefix within NetBox","text":"<p>The first step is to assign another prefix within NetBox. To do this the following task is used:</p> <pre><code>- name: \u201c10 - GET NEW PREFIX FROM NETBOX {{ primary_prefix }}\u201d\n  netbox.netbox.netbox_prefix:\n    netbox_url: \u201c{{ lookup(\u2018env\u2019, \u2018NETBOX_URL\u2019) }}\u201d\n    netbox_token: \u201c{{ lookup(\u2018env\u2019, \u2018NETBOX_TOKEN\u2019) }}\u201d\n      data:\n        parent: \u201c{{ primary_prefix }}\u201d\n        prefix_length: 24\n      state: present\n      first_available: yes\n  register: prefix_info\n</code></pre> <p>This task is going to take from the parent prefix and allocate a prefix of length 24. This states to take the first available prefix. Executing the playbook we are building with the <code>-vv</code> option and the <code>stdout_callback=yaml</code> in the ansible.cfg file you can see the output:</p> <pre><code>changed: [rtr-edge] =&gt; changed=true \n  msg: prefix 10.21.5.0/24 created\n  prefix:\n    created: '2020-11-22'\n    custom_fields: {}\n    description: ''\n    family: 4\n    id: 25\n    is_pool: false\n    last_updated: '2020-11-22T15:57:13.641224Z'\n    prefix: 10.21.5.0/24\n    role: null\n    site: null\n    status: active\n    tags: []\n    tenant: null\n    url: http://netbox.josh-v.com/api/ipam/prefixes/25/\n    vlan: null\n    vrf: null\n</code></pre> <p>This response when registered will provide with the Prefix ID, prefix itself, and any additional items that may have been set for your NetBox environment. After running this a few times and this demo being the sixth execution this is now what the NetBox environment looks like for prefixes:</p> <p></p>"},{"location":"netbox_ansible_allocate_prefix_ipaddress/#ip-address-allocation","title":"IP Address Allocation","text":"<p>After getting the first task to allocate the prefix, next up is to assign the IP address from the prefix. This task allocates an IP address from the prefix that was just previously allocated.</p> <pre><code>- name: \"20 - ALLOCATE IP ADDRESS FOR THE ROUTER INTERFACE\"\n  netbox.netbox.netbox_ip_address:\n    netbox_url: \"{{ lookup('env', 'NETBOX_URL') }}\"\n    netbox_token: \"{{ lookup('env', 'NETBOX_TOKEN') }}\"\n    data:\n      prefix: \"{{ prefix_info['prefix']['prefix'] }}\"\n    state: new\n  register: ip_address_info\n</code></pre> <p>On line 6 you see that the prefix gathered is mentioned via the variable. This is taken from the output that was seen from the NetBox Prefix allocation.</p> <p>This then looks like this for the output:</p> <pre><code>changed: [rtr-edge] =&gt; changed=true \n  ip_address:\n    address: 10.21.5.1/24\n    assigned_object: null\n    assigned_object_id: null\n    assigned_object_type: null\n    created: '2020-11-22'\n    custom_fields: {}\n    description: ''\n    dns_name: ''\n    family: 4\n    id: 23\n    last_updated: '2020-11-22T15:57:14.833379Z'\n    nat_inside: null\n    nat_outside: null\n    role: null\n    status: active\n    tags: []\n    tenant: null\n    url: http://netbox.josh-v.com/api/ipam/ip-addresses/23/\n    vrf: null\n  msg: ip_address 10.21.5.1/24 created\n</code></pre> <p>Taking a look at the NetBox Prefix View for the 10.21.5.0/24 network this is what you see:</p> <p></p> <p>You can see that there is a single IP address allocated.</p>"},{"location":"netbox_ansible_allocate_prefix_ipaddress/#variable-shortening","title":"Variable Shortening","text":"<p>The next task in the Playbook is to shorten some of the variables. This is purely for visualization purposes. In order to not have long lines in the coming tasks, the following was done to create shorter line lengths:</p> <pre><code>- name: \"30 - SET FACTS TO ASSIGN IP ADDRESS TO CISCO IOS ROUTER\"\n  set_fact:\n    ip_address: \"{{ ip_address_info['ip_address']['address'] | ipaddr('ip')  }}\"\n    netmask: \"{{ ip_address_info['ip_address']['address'] | ipaddr('netmask') }}\"\n</code></pre>"},{"location":"netbox_ansible_allocate_prefix_ipaddress/#apply-the-cisco-configuration","title":"Apply the Cisco Configuration","text":"<p>Now that there is an IP address and prefix available, and assigned within NetBox, the next step is to add the configuration to the device. Since this is primarily a focus on the NetBox side of things this will be short.</p> <pre><code>    # DEPLOY THE INFORMATION TO THE ROUTER\n    - name: \"100 - ADD IP ADDRESS INFORMATION TO THE ROUTER\"\n      ios_config:\n        parents: \"interface GigabitEthernet0/3\"\n        lines:\n          - \"ip address {{ ip_address }} {{ netmask }}\"\n        save_when: changed\n\n    - name: \"110 - ADD ROUTING CONFIGURATION\"\n      ios_config:\n        parents: \"router ospf 1\"\n        lines:\n          - \"network {{ ip_address_info['ip_address']['address'] | ipaddr('network') }} {{ netmask }} area 0\"\n        save_when: changed\n</code></pre> <p>Lines 2-7 are the applying of the configuration to the interface to be used. Lines 9-14 are used to add the network statement to OSPF for the prefix. With this done the interface is now configured and routing is setup.</p> <pre><code>TASK [100 - ADD IP ADDRESS INFORMATION TO THE ROUTER] ****************************************************************************************************************\nchanged: [rtr-edge] =&gt; changed=true \n  ansible_facts:\n    discovered_interpreter_python: /usr/bin/python\n  banners: {}\n  commands:\n  - interface GigabitEthernet0/3\n  - ip address 10.21.5.1 255.255.255.0\n  updates:\n  - interface GigabitEthernet0/3\n  - ip address 10.21.5.1 255.255.255.0\n\nTASK [110 - ADD ROUTING CONFIGURATION] *******************************************************************************************************************************\nchanged: [rtr-edge] =&gt; changed=true \n  banners: {}\n  commands:\n  - router ospf 1\n  - network 10.21.5.0 255.255.255.0 area 0\n  updates:\n  - router ospf 1\n  - network 10.21.5.0 255.255.255.0 area 0\n</code></pre>"},{"location":"netbox_ansible_allocate_prefix_ipaddress/#production-ready","title":"Production Ready","text":"<p>This is a quick demo and has some hand holding that needs to be done for it. There does need to be some Atomic handling added yet to make this a rock solid playbook. In a future post I will also cover how to simplify the save_when feature to help speed things up as well. This right now will save the configuration on each change. This should get simplified down to a single save execution.</p>"},{"location":"netbox_ansible_allocate_prefix_ipaddress/#final-playbook","title":"Final Playbook","text":"<p>Here is what the final playbook looks like at the moment, again not completely production ready, but is a good starting point.</p> <pre><code>---\n- name: \"PLAY 1 - ASSIGN PREFIXES FOR HOST\"\n  gather_facts: no\n  connection: network_cli\n  hosts: rtr-edge\n  vars:\n    primary_prefix: \"10.21.0.0/16\"\n  tasks:\n    - name: \"LOCALHOST BLOCK\"\n      delegate_to: localhost\n      block:\n        - name: \"10 - GET NEW PREFIX FROM NETBOX {{ primary_prefix }}\"\n          netbox.netbox.netbox_prefix:\n            netbox_url: \"{{ lookup('env', 'NETBOX_URL') }}\"\n            netbox_token: \"{{ lookup('env', 'NETBOX_TOKEN') }}\"\n            data:\n              parent: \"{{ primary_prefix }}\"\n              prefix_length: 24\n            state: present\n            first_available: yes\n          register: prefix_info\n\n        - name: \"20 - ALLOCATE IP ADDRESS FOR THE ROUTER INTERFACE\"\n          netbox.netbox.netbox_ip_address:\n            netbox_url: \"{{ lookup('env', 'NETBOX_URL') }}\"\n            netbox_token: \"{{ lookup('env', 'NETBOX_TOKEN') }}\"\n            data:\n              prefix: \"{{ prefix_info['prefix']['prefix'] }}\"\n            state: new\n          register: ip_address_info\n\n        - name: \"30 - SET FACTS TO ASSIGN IP ADDRESS TO CISCO IOS ROUTER\"\n          set_fact:\n            ip_address: \"{{ ip_address_info['ip_address']['address'] | ipaddr('ip')  }}\"\n            netmask: \"{{ ip_address_info['ip_address']['address'] | ipaddr('netmask') }}\"\n\n    # DEPLOY THE INFORMATION TO THE ROUTER\n    - name: \"100 - ADD IP ADDRESS INFORMATION TO THE ROUTER\"\n      ios_config:\n        parents: \"interface GigabitEthernet0/3\"\n        lines:\n          - \"ip address {{ ip_address }} {{ netmask }}\"\n        save_when: changed\n\n    - name: \"110 - ADD ROUTING CONFIGURATION\"\n      ios_config:\n        parents: \"router ospf 1\"\n        lines:\n          - \"network {{ ip_address_info['ip_address']['address'] | ipaddr('network') }} {{ netmask }} area 0\"\n        save_when: changed\n</code></pre>"},{"location":"jekyll-url-redirection/","title":"Jekyll - Adding a URL Redirection","text":"<p>Recently I had some discussions with Nick Russo on some URL redirection changes he was making for his content. I'm not going to take any of his thunder of what he is doing, and that is quite awesome. I decided that I wanted to take a look at that as well within my domain/blog using the Jekyll approach. This is going to be my short post regarding the steps I took to add the URL redirection setup to my personal blog page - josh-v.com.  </p>","tags":["blog","jekyll"]},{"location":"jekyll-url-redirection/#first-step-research","title":"First Step - Research","text":"<p>Not being a native Ruby/gem person myself (Python + Ansible), the first thing I did was what anyone should do, see if there is prior art. So I did a search on your favorite search tool and there are a few references. Terrific this should be able to be done.  </p> <p>The first page that came up was https://github.com/hlaueriksson/jekyll-url-shortener. It itself was a little bit tougher for me to decipher but enough to get started. What really helped was the blog post that accompanied the page, which was the third result in the search - https://conductofcode.io/post/introducing-jekyll-url-shortener/.  </p>","tags":["blog","jekyll"]},{"location":"jekyll-url-redirection/#second-step-testing","title":"Second Step - Testing","text":"<p>Next up was to generate some test code on my local Docker container that I could test the Jekyll blog out. This has been a terrific help. I had some issues getting Jekyll installed on my Mac, so I built a container to handle the testing and that has been working great. I sense a future post here.  </p>","tags":["blog","jekyll"]},{"location":"jekyll-url-redirection/#redirect-page","title":"Redirect Page","text":"<p>First thing was to create a redirect page with front matter. I decided the best thing to start with was a redirect to my employer's page. So I put this in:</p> <pre><code>---\npermalink: /ntc/\nredirect_to: https://www.networktocode.com/\n---\n</code></pre> <p>Once in I attempted to load the page to see if it would redirect. No go. That is where the second URL, the one referenced in the references section comes into play. I found that there were an additional two configuration items needed to help the Jekyll pages handle the redirection.</p>","tags":["blog","jekyll"]},{"location":"jekyll-url-redirection/#additional-packages","title":"Additional Packages","text":"<p>The first thing was to add the package to the gemspec file. I went and found where the Jekyll plugins were referenced and found them inside the <code>minimal-mistakes-jekyll.gemspec</code> file. The blog theme I am using is Minimal Mistakes. So I added the following configuration line to the gemspec:</p> <pre><code>spec.add_runtime_dependency \"jekyll-redirect-from\", \"~&gt; 0.1\"\n</code></pre> <p>I then added the line to the plugins section of the <code>_config.yml</code> file (there are more plugins than just what's listed here):</p> <pre><code>plugins:\n  - jekyll-redirect-from\n</code></pre> <p>Once these two updates were made I was successfully getting redirects from my http://localhost:4000/ntc/ URL to https://www.networktocode.com.</p>","tags":["blog","jekyll"]},{"location":"jekyll-url-redirection/#production","title":"Production","text":"<p>Next up was to put into production the changes. So I took the files over from the test instance into the GitLab project that I host this on. The same changes were made to the system and all is set to go.</p>","tags":["blog","jekyll"]},{"location":"jekyll-url-redirection/#next-up","title":"Next Up","text":"<p>There are several improvements that I'm looking to improve upon now that I'm all set in this current blog environment. First is to merge the development environment with my regular blog site. This should not be too bad, but as I was writing this post I realized the importance of this. The reasoning behind why I got in this state is because I was testing different blog platforms (Hugo vs Hashnode vs Jekyll vs Pelican).  </p> <p>I also plan to re-evaluate if Disqus is the proper platform for my commenting. There are some other options out there, and I need to take a look for how low volume the blog is.  </p> <p>The last feature that I am looking that would have come in handy on this post is the copy of code snippets feature. That looks a bit more involved however. There are posts on how to do it, but I just need to take a little bit of time to test it out.  </p>","tags":["blog","jekyll"]},{"location":"jekyll-url-redirection/#summary","title":"Summary","text":"<p>In the end it was not too difficult to add a redirect URL page to my Jekyll blog. I plan to have a few links made available, and it is really to just help maintain a list of redirects to helpful content elsewhere and shorten URLs that become lengthy in social posting. Let me know your thoughts below, or if there is another method that I am missing. I may also start to look at some shorter domains, although two characters is probably the best I would be able to squeak out.</p>","tags":["blog","jekyll"]},{"location":"jekyll-url-redirection/#resources","title":"Resources","text":"<p>https://conductofcode.io/post/introducing-jekyll-url-shortener/</p>","tags":["blog","jekyll"]},{"location":"collection_install/","title":"NetBox Ansible Collection: Installation","text":"<p>This is the first post as I start to look at the NetBox Ansible Collection. This is an impressive collection with modules for several of the NetBox applications, a query plugin, and an inventory plugin. This will take a deeper dive into several of the components of the inventory plugin, but not all of the options. The documentation for all of the collection can be found at:</p> <ul> <li>ReadTheDocs: https://netbox-ansible-collection.readthedocs.io/en/latest/</li> <li>Galaxy Page: https://galaxy.ansible.com/netbox/netbox </li> </ul> <p>Note</p> <p>This post was created when NetBox was an open source project used often in my automation framework. I have moved on to using Nautobot due to the project vision and providing a methodology that will drive network automation forward further. You may want to take a look at it yourself.</p> <p>This post is going to give information on how to install the collection as it may be applicable to every post in the series (as they get posted).</p> <p>(Update 2020-12-05) The corresponding YouTube video is here:</p>"},{"location":"collection_install/#installation","title":"Installation","text":"<p>Installation is done via Ansible Galaxy. It is recommended to have the latest version of the collection when working on it as there are updates happening routinely. There is a Python requirement with the modules of the pynetbox package.  </p> <p>It does not matter which order you install these in, you just need to install both before you start using the module.</p>"},{"location":"collection_install/#installation-pynetbox","title":"Installation - pynetbox","text":"<p>To install you execute the following to get the latest version of pynetbox:</p> <pre><code>pip install pynetbox --upgrade\n</code></pre>"},{"location":"collection_install/#installation-netbox-collection","title":"Installation - NetBox Collection","text":"<p>The collection is installed via Ansible Galaxy as a primary method to install. You can also install the collection manually from GitHub, but the galaxy method is the preferred method.</p> <pre><code>ansible-galaxy collection install netbox.netbox --force\n</code></pre> <p>The addition of <code>--force</code> will have Ansible Galaxy install the latest version on top of what you may already have. If you already have a version of the collection installed, Galaxy will not overwrite what you already have.</p>"},{"location":"collection_install/#verification-of-installation","title":"Verification of Installation","text":"<p>Once you have run the steps there are many ways to verify that the installation is completed successfully for the Python package. The one that I like to use is to execute a <code>pip freeze | grep &lt;package_name&gt;</code>. The execution looks like this on the current date:</p> <pre><code>pip3 freeze | grep pynetbox    \npynetbox==5.1.0\n</code></pre> <p>To verify that you have installed the NetBox Ansible Collection, you can execute the Ansible Doc command to get the current documentation. This is done as followed with the netbox_device module to verify that the docs load:</p> <pre><code>ansible-doc netbox.netbox.netbox_device\n</code></pre> <p>If the module is not installed properly you will see, with a key in on the first line</p> <p><pre><code>[WARNING]: module netbox.netbox.netbox_inventory not found in:\n~/.local/lib/python3.7/site-packages/ansible/modules\n</code></pre> The output when I sent the stdout to a file is:</p> <pre><code>&gt; NETBOX.NETBOX.NETBOX_DEVICE    (/Users/joshvanderaa/.ansible/collections/ansible_collections/netbox/netbox/plugins/modules/netbox_device.py)\n\n        Creates, updates or removes devices from Netbox\n\nOPTIONS (= is mandatory):\n\n= data\n        Defines the device configuration\n\n        type: dict\n\n        SUBOPTIONS:\n\n        - asset_tag\n            Asset tag that is associated to the device\n            [Default: (null)]\n            type: str\n\n        - cluster\n            Cluster that the device will be assigned to\n            [Default: (null)]\n            type: raw\n\n        - comments\n            Comments that may include additional information in\n            regards to the device\n            [Default: (null)]\n            type: str\n\n        - custom_fields\n            must exist in Netbox\n            [Default: (null)]\n            type: dict\n\n        - device_role\n            Required if `state=present' and the device does not exist\n            yet\n            [Default: (null)]\n            type: raw\n\n        - device_type\n            Required if `state=present' and the device does not exist\n            yet\n            [Default: (null)]\n            type: raw\n\n        - face\n            Required if `rack' is defined\n            (Choices: Front, front, Rear, rear)[Default: (null)]\n            type: str\n\n        - local_context_data\n            Arbitrary JSON data to define the devices configuration\n            variables.\n            [Default: (null)]\n            type: dict\n\n        = name\n            The name of the device\n\n            type: str\n\n        - platform\n            The platform of the device\n            [Default: (null)]\n            type: raw\n\n        - position\n            The position of the device in the rack defined above\n            [Default: (null)]\n            type: int\n\n        - primary_ip4\n            Primary IPv4 address assigned to the device\n            [Default: (null)]\n            type: raw\n\n        - primary_ip6\n            Primary IPv6 address assigned to the device\n            [Default: (null)]\n            type: raw\n\n        - rack\n            The name of the rack to assign the device to\n            [Default: (null)]\n            type: raw\n\n        - serial\n            Serial number of the device\n            [Default: (null)]\n            type: str\n\n        - site\n            Required if `state=present' and the device does not exist\n            yet\n            [Default: (null)]\n            type: raw\n\n        - status\n            The status of the device\n            [Default: (null)]\n            type: raw\n\n        - tags\n            Any tags that the device may need to be associated with\n            [Default: (null)]\n            type: list\n\n        - tenant\n            The tenant that the device will be assigned to\n            [Default: (null)]\n            type: raw\n\n        - vc_position\n            Position in the assigned virtual chassis\n            [Default: (null)]\n            type: int\n\n        - vc_priority\n            Priority in the assigned virtual chassis\n            [Default: (null)]\n            type: int\n\n        - virtual_chassis\n            Virtual chassis the device will be assigned to\n            [Default: (null)]\n            type: raw\n\n= netbox_token\n        The token created within Netbox to authorize API access\n\n        type: str\n\n= netbox_url\n        URL of the Netbox instance resolvable by Ansible control host\n\n        type: str\n\n- query_params\n        This can be used to override the specified values in\n        ALLOWED_QUERY_PARAMS that is defined\n        in plugins/module_utils/netbox_utils.py and provides control\n        to users on what may make\n        an object unique in their environment.\n        [Default: (null)]\n        elements: str\n        type: list\n\n- state\n        Use `present' or `absent' for adding or removing.\n        (Choices: absent, present)[Default: present]\n        type: str\n\n- validate_certs\n        If `no', SSL certificates will not be validated. This should\n        only be used on personally controlled sites using self-signed\n        certificates.\n        [Default: True]\n        type: raw\n\n\nNOTES:\n      * Tags should be defined as a YAML list\n      * This should be ran with connection `local' and hosts\n        `localhost'\n\n\nREQUIREMENTS:  pynetbox\n\nAUTHOR: Mikhail Yohman (@FragmentedPacket), David Gomez (@amb1s1)\n\nMETADATA:\n  metadata_version: '1.1'\n  status:\n  - preview\n  supported_by: community\n\n\nVERSION_ADDED_COLLECTION: netbox.netbox\n\nEXAMPLES:\n\n- name: \"Test Netbox modules\"\n  connection: local\n  hosts: localhost\n  gather_facts: False\n\n  tasks:\n    - name: Create device within Netbox with only required information\n      netbox_device:\n        netbox_url: http://netbox.local\n        netbox_token: thisIsMyToken\n        data:\n          name: Test Device\n          device_type: C9410R\n          device_role: Core Switch\n          site: Main\n        state: present\n\n    - name: Create device within Netbox with empty string name to generate UUID\n      netbox_device:\n        netbox_url: http://netbox.local\n        netbox_token: thisIsMyToken\n        data:\n          name: \"\"\n          device_type: C9410R\n          device_role: Core Switch\n          site: Main\n        state: present\n\n    - name: Delete device within netbox\n      netbox_device:\n        netbox_url: http://netbox.local\n        netbox_token: thisIsMyToken\n        data:\n          name: Test Device\n        state: absent\n\n    - name: Create device with tags\n      netbox_device:\n        netbox_url: http://netbox.local\n        netbox_token: thisIsMyToken\n        data:\n          name: Another Test Device\n          device_type: C9410R\n          device_role: Core Switch\n          site: Main\n          local_context_data:\n            bgp: \"65000\"\n          tags:\n            - Schnozzberry\n        state: present\n\n    - name: Update the rack and position of an existing device\n      netbox_device:\n        netbox_url: http://netbox.local\n        netbox_token: thisIsMyToken\n        data:\n          name: Test Device\n          rack: Test Rack\n          position: 10\n          face: Front\n        state: present\n\n\nRETURN VALUES:\n- device\n        Serialized object as created or already existent within Netbox\n\n        returned: success (when `state=present')\n        type: dict\n\n- msg\n        Message indicating failure or info about what has been\n        achieved\n\n        returned: always\n        type: str\n</code></pre>"},{"location":"collection_install/#summary","title":"Summary","text":"<p>Overall the process for getting going with this collection is two steps, of installing the Python dependency and installing the collection via Ansible Galaxy. With these done, you are on your way to using the NetBox Ansible Collection in your environment.</p>"},{"location":"collection_install/#up-next","title":"Up Next","text":"<ul> <li>Ansible Inventory</li> </ul>"},{"location":"netbox-ansible-inventory_plugin/","title":"NetBox Ansible Collection: Inventory - Starting Out","text":"<p>The documentation can be found on ReadTheDocs. This is going to be starting out with the basics of the plugin and getting some sample output and to show how to form groups to be used.</p> <p>Note</p> <p>This post was created when NetBox was an open source project used often in my automation framework. I have moved on to using Nautobot due to the project vision and providing a methodology that will drive network automation forward further. You may want to take a look at it yourself.</p> <p>This particular plugin DOES NOT require pynetbox to be used. </p> <p>Late addition: You can see a corresponding video on YouTube:</p>"},{"location":"netbox-ansible-inventory_plugin/#purpose","title":"Purpose","text":"<p>The purpose of the NetBox Inventory plugin is to provide an inventory to use within your Ansible automations. The plugin will gather information from NetBox and from the data create groups and an inventory for use by Ansible.</p>"},{"location":"netbox-ansible-inventory_plugin/#install","title":"Install","text":"<p>Please checkout the first post in the series for the getting started / installation process. </p>"},{"location":"netbox-ansible-inventory_plugin/#environment","title":"Environment","text":"<p>For this demo, as there are many pieces of information brought back by the inventory I have reduced the inventory to two hosts. </p> <p>Here are the rest of the installation versions:</p> Component Version NetBox v2.9.9 (NetBox Docker) NetBox Ansible Collection v1.1.0 pynetbox Not required - Not installed"},{"location":"netbox-ansible-inventory_plugin/#basic-setup-no-groups","title":"Basic Setup - No Groups","text":"<p>First a good practice for working on systems is to use environment variables to define the server and secret information within the environment. The environment has been configured with for this demo are:</p> Environment Variable Name What is in the variable NETBOX_API API URL for NetBox such as <code>http://netbox.example.com</code> NETBOX_TOKEN API Token set inside of NetBox Admin -&gt; Admin -&gt; Tokens <p>From the documentation these variables if not defined in the inventory YAML file that the plugin will search for the values to be in the environment with NETBOX_API and NETBOX_TOKEN. This allows you to share the inventory file to be shared across multiple NetBox environments, all based on the environment.</p> <p>You configure the inventory with a YAML file that references the plugin. Here is the starting point to verify that we can get data from NetBox into an inventory before we add some of the filtering and groupings. </p> <pre><code># netbox_inventory.yml\n---\nplugin: netbox.netbox.nb_inventory\nvalidate_certs: false\nconfig_context: false\n</code></pre> <p>What is being done here:</p> Line Number Key Value 2 plugin Which inventory plugin to be used, the FQCN of <code>netbox.netbox.nb_inventory</code> 3 validate_certs If TLS is configured, then the certificates will not be validated. 4 config_context This is important on large NetBox environments. Setting <code>config_context</code> to false tells the API to not return the config_context field, reducing the amount of data being returned. <p>Alternatively if you do not define the items in the environment, then you would add the keys into the file, such that it looks like below. The rest of the post will assume that the items are configured in the environment.</p> <pre><code># netbox_inventory.yml\n---\nplugin: netbox.netbox.nb_inventory\napi_endpoint: http://netbox.example.com\ntoken: &lt;API_TOKEN here&gt;\nvalidate_certs: false\nconfig_context: false\n</code></pre>"},{"location":"netbox-ansible-inventory_plugin/#basic-setup-result","title":"Basic Setup Result","text":"<p>When executing the inventory <code>ansible-inventory -i netbox_inventory.yml</code>. This example the file name is netbox_inventory.yml. This is completely arbitrary. You can name the inventory any file you wish. Most likely you should name it something that relates to the inventory you are creating. The result of the inventory gives the following:</p> <pre><code>{\n    \"_meta\": {\n        \"hostvars\": {\n            \"dcrtr001\": {\n                \"ansible_host\": \"192.0.2.10\",\n                \"custom_fields\": {\n                    \"os_version\": null\n                \"device_roles\": [\n                    \"network\"\n                ],\n                \"device_types\": [\n                    \"iosv\"\n                ],\n                \"is_virtual\": false,\n                \"local_context_data\": [\n                    null\n                ],\n                \"manufacturers\": [\n                    \"cisco\"\n                ],\n                \"platforms\": [\n                    \"cisco_ios\"\n                ],\n                \"primary_ip4\": \"192.0.2.10\",\n                \"regions\": [],\n                \"services\": [],\n                \"sites\": [\n                    \"site01\"\n                ],\n                \"tags\": []\n            },\n            \"wanrtr002\": {\n                \"ansible_host\": \"10.10.0.2\",\n                \"custom_fields\": {\n                    \"os_version\": null\n                },\n                \"device_roles\": [\n                    \"network\"\n                ],\n                \"device_types\": [\n                    \"csr1000v\"\n                ],\n                \"is_virtual\": false,\n                \"local_context_data\": [\n                    null\n                ],\n                \"manufacturers\": [\n                    \"cisco\"\n                ],\n                \"platforms\": [\n                    \"cisco_ios\"\n                ],\n                \"primary_ip4\": \"10.10.0.2\",\n                \"regions\": [],\n                \"services\": [],\n                \"sites\": [\n                    \"site01\"\n                ],\n                \"tags\": []\n            }\n        }\n    },\n    \"all\": {\n        \"children\": [\n            \"ungrouped\"\n        ]\n    },\n    \"ungrouped\": {\n        \"hosts\": [\n            \"dcrtr001\",\n            \"wanrtr002\"\n        ]\n    }\n}\n</code></pre> <p>In this output, you get several components made available. First take notice on lines 62-74 at the bottom. This gives the groupings that will be made available. Since this is the basic query with no groupings defined, there is a single group of ungrouped. This is a child of the all group. Which allows only hosts defined as all or the specific hostname.  </p> <p>Taking a look at the hostvars that get assigned (lines 1-62), you get several host variables defined from the API call. Including the primary_ip4 address, device_role, an Ansible Host from the primary_ip4 address, and also the custom fields that are part of the NetBox environment.</p>"},{"location":"netbox-ansible-inventory_plugin/#groupings","title":"Groupings","text":"<p>Next up is adding some more context and adding groupings. This is done within your inventory YAML file with the key of group_by. There are several choices of what you can group objects by, including sites, tenants, tags, platforms, and many more. Take a look at the documentation reference for all of the options. Taking a look at having a grouping of by device_role, tags, sites, and platform gives the following additional groups: </p> <pre><code># netbox_inventory_group_by.yml\n---\nplugin: netbox.netbox.nb_inventory\nvalidate_certs: false\nconfig_context: false\ngroup_by:\n  - device_roles\n  - platforms\n  - tags\n  - sites\n</code></pre> <p>This now yields the following:</p> <pre><code>{\n    \"all\": {\n        \"children\": [\n            \"device_roles_network\",\n            \"platforms_cisco_ios\",\n            \"sites_datacenter01\",\n            \"sites_site01\",\n            \"tags_virtual\",\n            \"ungrouped\"\n        ]\n    },\n    \"device_roles_network\": {\n        \"hosts\": [\n            \"dcrtr001\",\n            \"wanrtr002\"\n        ]\n    },\n    \"platforms_cisco_ios\": {\n        \"hosts\": [\n            \"dcrtr001\",\n            \"wanrtr002\"\n        ]\n    },\n    \"sites_datacenter01\": {\n        \"hosts\": [\n            \"dcrtr001\"\n        ]\n    },\n    \"sites_site01\": {\n        \"hosts\": [\n            \"wanrtr002\"\n        ]\n    },\n    \"tags_virtual\": {\n        \"hosts\": [\n            \"dcrtr001\"\n        ]\n    }\n}\n</code></pre> <p>This filtered out the hostvars as these are not changing.</p> <p>There are now 5 additional groups showing up in the all group. And each of these have different hosts available for you to use in your playbooks. Now these are making sense and you can now have groupings for each of the items.</p>"},{"location":"netbox-ansible-inventory_plugin/#filtering-devices-from-netbox","title":"Filtering Devices from NetBox","text":"<p>Next up is how do you filter hosts from the NetBox environment? That is done with query_filters key. From the documentation page: </p> <p>List of parameters passed to the query string for both devices and VMs (Multiple values may be separated by commas)</p> <p>In the testing I was not able to get multiple values on a status. This may need some clarification from the project.  </p> <p>First search that was changed is that I moved <code>wanrtr002</code> to an offline state. Now when running the following inventory with a query filter status of active, the host is no longer in the grouping:</p> <pre><code># netbox_inventory_filtered.yml\n---\nplugin: netbox.netbox.nb_inventory\nvalidate_certs: false\nconfig_context: false\ngroup_by:\n  - device_roles\n  - platforms\n  - tags\n  - sites\nquery_filters:\n  - status: \"active\"\n</code></pre> <p>Looking at just the platforms_cisco_ios grouping, there is now a single device.</p> <pre><code>    \"platforms_cisco_ios\": {\n        \"hosts\": [\n            \"dcrtr001\"\n        ]\n    },\n</code></pre> <p>You can add additional queries to the query_filters key, such that status is listed twice. Once we add the second status then both devices show up again:</p> <pre><code>query_filters:\n  - status: \"active\"\n  - status: \"offline\"\n</code></pre> <pre><code>    \"platforms_cisco_ios\": {\n        \"hosts\": [\n            \"dcrtr001\",\n            \"wanrtr002\"\n        ]\n    },\n</code></pre>"},{"location":"netbox-ansible-inventory_plugin/#adding-variables","title":"Adding Variables","text":"<p>The method to add custom hostvars to your inventory from NetBox, is done with the compose parameter. From the documentation site:</p> <p>List of custom ansible host vars to create from the device object fetched from NetBox</p> <p>THe first item that I use often with the compose parameter of the plugin is to set the <code>ansible_network_os</code> for a device. This is very helpful with a multi-vendor environment that allows you to run tasks against different network OS's. The final demonstration inventory YAML file is this:</p> <pre><code># netbox_inventory_all.yml\n---\nplugin: netbox.netbox.nb_inventory\nvalidate_certs: false\nconfig_context: false\ngroup_by:\n  - device_roles\n  - platforms\n  - tags\n  - sites\nquery_filters:\n  - status: \"active\"\n  - status: \"offline\"\ncompose:\n  ansible_network_os: platform.slug\n</code></pre> <p>With this modification the output shows the following hostvars with the groups unchanged. Notice specifically line 3 that there is now the ansible_network_os is set to ios, which will match the Ansible Network OS that Ansible will use. I just make sure that the slug for the Platform name matches that of the primary automation system.</p> <pre><code>\"dcrtr001\": {\n    \"ansible_host\": \"192.0.2.10\",\n    \"ansible_network_os\": \"ios\",\n    \"custom_fields\": {\n        \"os_version\": null\n    },\n    \"device_roles\": [\n        \"network\"\n    ],\n    \"device_types\": [\n        \"iosv\"\n    ],\n    \"is_virtual\": false,\n    \"local_context_data\": [\n        null\n    ],\n    \"manufacturers\": [\n        \"cisco\"\n    ],\n    \"platforms\": [\n        \"cisco_ios\"\n    ],\n    \"primary_ip4\": \"192.0.2.10\",\n    \"regions\": [],\n    \"services\": [],\n    \"sites\": [\n        \"datacenter01\"\n    ],\n    \"tags\": [\n        \"virtual\"\n    ]\n},\n</code></pre>"},{"location":"netbox-ansible-inventory_plugin/#summary","title":"Summary","text":"<p>The NetBox Inventory Plugin for Ansible is quite powerful. It provides for methods to group your devices, filter on data points maintained within NetBox, and to create additional hostvars. The parameters for the plugin are quite extensive and you should take a look at what makes sense for your environment or needs within the Ansible playbooks. There are a few defaults that are set to no, such as interfaces, that helps keep the data provided at a proper level. You can add more information to the hostvars that you may use in the playbooks.  </p> <p>I think the plugin is awesome and has a lot of work that has gone into it. There is likely even more to come as you go.</p>"},{"location":"netbox-ansible-lookup-plugin/","title":"NetBox Ansible Collection: Lookup Plugin","text":"<p>The NetBox lookup plugin is to get information out of NetBox for use within Ansible. This uses pynetbox to query the NetBox API for the information requested. On top of being helpful in gathering data from NetBox (when it is not your inventory source), but it is extremely helpful in larger NetBox deployments when compared to using the URI module as well. If you wish to use NetBox as your inventory source, you should definitely read my previous post on getting started with the NetBox Inventory Plugin.</p> <ul> <li>Read the Docs</li> <li>GitHub Source File</li> <li>Installing the Colleciton</li> </ul> <p>The recommended Jinja function to use with this lookup plugin is the <code>query</code> function. This tells Ansible that the result of the lookup should be a type list. The same behavior is also available by using the <code>lookup</code> function, in conjunction with the parameter <code>wantlist=true</code>. For this post we will use the <code>query</code> method.</p> <p>Note</p> <p>This post was created when NetBox was an open source project used often in my automation framework. I have moved on to using Nautobot due to the project vision and providing a methodology that will drive network automation forward further. You may want to take a look at it yourself.</p>"},{"location":"netbox-ansible-lookup-plugin/#methodology","title":"Methodology","text":"<p>My methodology for gathering this information is that the plugin is looking to get the Django application (Sites, Devices, IPAM)</p>"},{"location":"netbox-ansible-lookup-plugin/#environment","title":"Environment","text":"<p>For this demo, here are the versions shown:</p> Component Version NetBox v2.9.9 (NetBox Docker) NetBox Ansible Collection v1.1.0 pynetbox 5.1.0"},{"location":"netbox-ansible-lookup-plugin/#query-plugin-note","title":"Query Plugin Note","text":"<p>So I was originally trying to use the Jinja variable template when I was working with the plugin. The query function does not need the templating language (wrapped in <code>{{ }}</code>). The variable name should be used without the wrapping and just work.</p>"},{"location":"netbox-ansible-lookup-plugin/#parameter-terms","title":"Parameter: Terms","text":"<p>The parameter <code>_terms</code> as I can understand relates to which end point within the pynetbox endpoint is being referenced. Digging into the code itself, I have found that the following is the available endpoints:</p> Lookup Endpoint Corresponding pynetbox endpoint aggregates netbox.ipam.aggregates circuit-terminations netbox.circuits.circuit_terminations circuit-types netbox.circuits.circuit_types circuits netbox.circuits.circuits circuit-providers netbox.circuits.providers cables netbox.dcim.cables cluster-groups netbox.virtualization.cluster_groups cluster-types netbox.virtualization.cluster_types clusters netbox.virtualization.clusters config-contexts netbox.extras.config_contexts console-connections netbox.dcim.console_connections console-ports netbox.dcim.console_ports console-server-port-templates\" netbox.dcim.console_server_port_templates console-server-ports netbox.dcim.console_server_ports device-bay-templates netbox.dcim.device_bay_templates device-bays netbox.dcim.device_bays device-roles netbox.dcim.device_roles device-types netbox.dcim.device_types devices netbox.dcim.devices export-templates netbox.dcim.export_templates front-port-templates netbox.dcim.front_port_templates front-ports netbox.dcim.front_ports graphs netbox.extras.graphs image-attachments netbox.extras.image_attachments interface-connections netbox.dcim.interface_connections interface-templates netbox.dcim.interface_templates interfaces netbox.dcim.interfaces inventory-items netbox.dcim.inventory_items ip-addresses netbox.ipam.ip_addresses manufacturers netbox.dcim.manufacturers object-changes netbox.extras.object_changes platforms netbox.dcim.platforms power-connections netbox.dcim.power_connections power-outlet-templates netbox.dcim.power_outlet_templates power-outlets netbox.dcim.power_outlets power-port-templates netbox.dcim.power_port_templates power-ports netbox.dcim.power_ports prefixes netbox.ipam.prefixes rack-groups netbox.dcim.rack_groups rack-reservations netbox.dcim.rack_reservations rack-roles netbox.dcim.rack_roles racks netbox.dcim.racks rear-port-templates netbox.dcim.rear_port_templates rear-ports netbox.dcim.rear_ports regions netbox.dcim.regions reports netbox.extras.reports rirs netbox.ipam.rirs roles netbox.ipam.roles secret-roles netbox.secrets.secret_roles secrets netbox.secrets.secrets services netbox.ipam.services sites netbox.dcim.sites tags netbox.extras.tags tenant-groups netbox.tenancy.tenant_groups tenants netbox.tenancy.tenants topology-maps netbox.extras.topology_maps virtual-chassis netbox.dcim.virtual_chassis virtual-machines netbox.virtualization.virtual_machines virtualization-interfaces netbox.virtualization.interfaces vlan-groups netbox.ipam.vlan_groups vlans netbox.ipam.vlans vrfs netbox.ipam.vrfs"},{"location":"netbox-ansible-lookup-plugin/#examples","title":"Examples","text":"<p>The goal is to really dig into the examples to see how it is used. Let's get right to it. For these demos I am using the following definitions for the Play:</p> <pre><code>---\n- name: \"GATHER DATA FROM NETBOX\"\n  hosts: localhost\n  connection: local\n  gather_facts: no\n  vars:\n    netbox_url: \"{{ lookup('env', 'NETBOX_API') }}\"\n    netbox_token: \"{{ lookup('env', 'NETBOX_TOKEN') }}\"\n</code></pre> <p>Of note, the URL and token are embedded into the environment. To maintain consistency with the NetBox Inventory plugin I am mapping <code>NETBOX_API</code> to be the netbox_url. At this moment of writing the lookup plugin does not natively lookup the environment like the other modules do. I do intended to submit a PR to update this.</p>"},{"location":"netbox-ansible-lookup-plugin/#gathering-sites","title":"Gathering Sites","text":"<p>When taking a look at gathering sites the following task is used:</p> <pre><code>- name: \"TASK 1: GET SITES WITH NB_QUERY\"\n  set_fact:\n    sites: \"{{ query('netbox.netbox.nb_lookup', 'sites', api_endpoint=netbox_url, token=netbox_token) }}\"\n\n- name: \"TASK 2: PRINT JUST THE SITE NAMES\"\n  debug:\n    msg: \"{{ sites | json_query('[*].value.name') }}\"\n</code></pre>"},{"location":"netbox-ansible-lookup-plugin/#gathering-sites-task-1-output","title":"Gathering Sites: TASK 1 Output","text":"<p>Lines 1-3 are the collecting of data from NetBox itself. In my NetBox demo environment I currently have 4 sites. Instead of giving you the entire output that is quite long, below is that output. Note that there is a bunch of information available to you about each site here.</p> <pre><code>   - key: 6\n      value:\n        asn: null\n        circuit_count: null\n        comments: ''\n        contact_email: ''\n        contact_name: ''\n        contact_phone: ''\n        created: '2020-12-06'\n        custom_fields: {}\n        description: Portland\n        device_count: 3\n        facility: ''\n        id: 6\n        last_updated: '2020-12-06T15:29:45.112426Z'\n        latitude: null\n        longitude: null\n        name: PDX\n        physical_address: ''\n        prefix_count: null\n        rack_count: null\n        region: null\n        shipping_address: ''\n        slug: pdx\n        status:\n          label: Active\n          value: active\n        tags: []\n        tenant: null\n        time_zone: America/Los_Angeles\n        url: http://netbox-demo/api/dcim/sites/6/\n        virtualmachine_count: null\n        vlan_count: null\n</code></pre>"},{"location":"netbox-ansible-lookup-plugin/#gathering-sites-task-2-output-getting-just-the-site-names","title":"Gathering Sites: TASK 2 Output - Getting Just the Site Names","text":"<p>I used json_query (which uses JMESPATH) to get just the site names. I see a future post on this coming in the future. The result of this gives me the output of just the four sites and not the rest of the data:</p> <pre><code>ok: [localhost] =&gt; \n  msg:\n  - DEN\n  - MSP\n  - NYC\n  - PDX\n</code></pre>"},{"location":"netbox-ansible-lookup-plugin/#get-devices-filtered-to-a-single-site","title":"Get Devices - Filtered to a single site","text":"<p>You can then filter with the lookup plugin as well. In these two tasks I'm going to filter and get the devices that are located at the DEN site. Task 3 you add the <code>api_filter</code> to the plugin definition on the set_fact. The API filters are key/value and are separated with a space inside of the string to have multiple searches. If you wish to search multiple sites or multiple roles (or multiple anything) then you add a second instance of it. The value of the key/value pair is the corresponding slug associated with the search.  </p> <pre><code>- name: \"TASK 3: GET DEVICES WITH ROLE ROUTER AT DEN SITE\"\n  set_fact:\n    den_devices: \"{{ query('netbox.netbox.nb_lookup', 'devices', api_filter='site=den role=router', api_endpoint=netbox_url, token=netbox_token) }}\"\n\n- name: \"TASK 4: PRINT THE DEVICES\"\n  debug:\n    msg: \"{{ den_devices | json_query('[*].value.name') }}\"\n</code></pre>"},{"location":"netbox-ansible-lookup-plugin/#getting-denver-routers-output","title":"Getting Denver Routers Output","text":"<p>The output from these two tasks where there is a single router device at the site Denver is then the following output with the single device on line 77.</p> <pre><code>TASK [TASK 3: GET ROLE ROUTERS AT DEN SITE] **************************************************************************************************************************\n  ansible_facts:\n    den_devices:\n    - key: 4\n      value:\n        asset_tag: null\n        cluster: null\n        comments: ''\n        config_context: {}\n        created: '2020-12-06'\n        custom_fields: {}\n        device_role:\n          id: 3\n          name: Router\n          slug: router\n          url: http://netbox-demo/api/dcim/device-roles/3/\n        device_type:\n          display_name: Cisco IOSV\n          id: 3\n          manufacturer:\n            id: 1\n            name: Cisco\n            slug: cisco\n            url: http://netbox-demo/api/dcim/manufacturers/1/\n          model: IOSV\n          slug: iosv\n          url: http://netbox-demo/api/dcim/device-types/3/\n        display_name: den-wan01\n        face: null\n        id: 4\n        last_updated: '2020-12-12T19:55:16.432666Z'\n        local_context_data: null\n        name: den-wan01\n        parent_device: null\n        platform:\n          id: 2\n          name: cisco_ios\n          slug: ios\n          url: http://netbox-demo/api/dcim/platforms/2/\n        position: null\n        primary_ip:\n          address: 10.16.0.2/24\n          family: 4\n          id: 4\n          url: http://netbox-demo/api/ipam/ip-addresses/4/\n        primary_ip4:\n          address: 10.16.0.2/24\n          family: 4\n          id: 4\n          url: http://netbox-demo/api/ipam/ip-addresses/4/\n        primary_ip6: null\n        rack: null\n        serial: 90Q1VEN47MPBMU2718KJ1\n        site:\n          id: 4\n          name: DEN\n          slug: den\n          url: http://netbox-demo/api/dcim/sites/4/\n        status:\n          label: Active\n          value: active\n        tags:\n        - color: 3f51b5\n          id: 2\n          name: snmp_monitoring\n          slug: snmp_monitoring\n          url: http://netbox-demo/api/extras/tags/2/\n        tenant: null\n        url: http://netbox-demo/api/dcim/devices/4/\n        vc_position: null\n        vc_priority: null\n        virtual_chassis: null\n\nTASK [TASK 4: PRINT THE DEVICES OF TYPE ROUTER AT DENVER LOCATION] ***************************************************************************************************\nok: [localhost] =&gt; \n  msg:\n  - den-wan01\n</code></pre>"},{"location":"netbox-ansible-lookup-plugin/#searching-multiple-locations","title":"Searching Multiple Locations","text":"<p>With this you are able to filter many things. To filter multiple sites, say you wanted to get the devices at both of the sites DEN and MSP. To do this you change the filter to be <code>site=den site=-msp</code>. Seeing this you get the following:</p> <p>I am only showing a single device corresponding to MSP &amp; DEN site.</p> <pre><code>  ansible_facts:\n    msp_den_devices:\n    - key: 5\n      value:\n        asset_tag: null\n        cluster: null\n        comments: ''\n        config_context: {}\n        created: '2020-12-06'\n        custom_fields: {}\n        device_role:\n          id: 2\n          name: Network\n          slug: network\n          url: http://netbox-demo/api/dcim/device-roles/2/\n        device_type:\n          display_name: Cisco IOSV\n          id: 3\n          manufacturer:\n            id: 1\n            name: Cisco\n            slug: cisco\n            url: http://netbox-demo/api/dcim/manufacturers/1/\n          model: IOSV\n          slug: iosv\n          url: http://netbox-demo/api/dcim/device-types/3/\n        display_name: den-dist01\n        face: null\n        id: 5\n        last_updated: '2020-12-06T17:16:02.266041Z'\n        local_context_data: null\n        name: den-dist01\n        parent_device: null\n        platform:\n          id: 2\n          name: cisco_ios\n          slug: ios\n          url: http://netbox-demo/api/dcim/platforms/2/\n        position: null\n        primary_ip:\n          address: 10.17.1.2/30\n          family: 4\n          id: 5\n          url: http://netbox-demo/api/ipam/ip-addresses/5/\n        primary_ip4:\n          address: 10.17.1.2/30\n          family: 4\n          id: 5\n          url: http://netbox-demo/api/ipam/ip-addresses/5/\n        primary_ip6: null\n        rack: null\n        serial: 9ZYX8XZUMP0AF69YGO5Z5\n        site:\n          id: 4\n          name: DEN\n          slug: den\n          url: http://netbox-demo/api/dcim/sites/4/\n        status:\n          label: Active\n          value: active\n        tags:\n        - color: 3f51b5\n          id: 2\n          name: snmp_monitoring\n          slug: snmp_monitoring\n          url: http://netbox-demo/api/extras/tags/2/\n        tenant: null\n        url: http://netbox-demo/api/dcim/devices/5/\n        vc_position: null\n        vc_priority: null\n        virtual_chassis: null\n</code></pre> <p>Here is the result of Task 6, which is the list of the devices:</p> <pre><code>TASK [TASK 6: PRINT THE DEVICES AT DEN &amp; MSP LOCATIONS] **************************************************************************************************************\nok: [localhost] =&gt; \n  msg:\n  - den-dist01\n  - den-dist02\n  - den-wan01\n  - msp-dist01\n  - msp-dist02\n  - msp-wan01\n</code></pre>"},{"location":"netbox-ansible-lookup-plugin/#working-with-large-data-sets","title":"Working With Large Data Sets","text":"<p>One may say that I can just get the data from using the URI module from Ansible. I've been there as well on this. One of the larger draws of using the lookup plugin is the ability to handle pagination of the results natively. Consider the following task, where I have changed the <code>MAX_PAGE_SIZE</code> environment variable to 2 in order to demonstrate the paging setup.</p> <pre><code>- name: \"TASK 7: GET DATA FROM NETBOX VIA THE REST API\"\n  uri:\n    url: \"{{ lookup('env', 'NETBOX_URL') }}/api/dcim/devices/?site=den&amp;limit=2\"\n    method: \"GET\"\n    headers:\n      Content-Type: \"application/json\"\n      Authorization: \"token {{ lookup('env', 'NETBOX_TOKEN') }}\"\n    status_code: 200\n  register: search_result\n\n- name: \"TASK 8: PRINT LENGTH OF PAGED SETUP\"\n  debug:\n    msg:\n      - \"Length of result on paginated response: {{ search_result['json']['results'] | length }}\"\n      - \"Total results (if no paging): {{ search_result['json']['count'] }}\"\n</code></pre> <p>Task 7 gets the data, and looking at the response data coming back we can see that there is a second page by the next field:</p> <pre><code>  json:\n    count: 3\n    next: http://netbox-demo/api/dcim/devices/?limit=2&amp;offset=2&amp;site=den\n    previous: null\n    results:\n</code></pre> <p>Task 8 then confirms this for us:</p> <pre><code>  msg:\n  - 'Length of result on paginated response: 2'\n  - 'Total results (if no paging): 3'\n</code></pre> <p>This is where leveraging pynetbox under the hood and it handling the pagination will be helpful. Some day there may be an Ansible module that handles API calls and combines the results on multiple responses. But today one would need to add a fair amount of logic handling into a Playbook execution to handle pagination.  </p> <p>The result is handling the paging in the task and makes the life very easy to get data from NetBox with it!</p> <pre><code>TASK [TASK 10: PRINT THE DEVICES AT DEN LOCATION] ********************************************************************************************************************\nok: [localhost] =&gt; \n  msg:\n  - den-dist01\n  - den-dist02\n</code></pre>"},{"location":"netbox-ansible-lookup-plugin/#summary","title":"Summary","text":"<p>The lookup plugin from the NetBox Ansible Content Collection is a great tool to help get your NetBox search data into your Ansible Playbooks. You can filter as needed, and you get the data into a format that you can then use!</p>"},{"location":"netbox-ansible-lookup-plugin/#final-playbook","title":"Final Playbook","text":"<p>Since there were a lot of demos in here, below is the final playbook. In the environment are the NETBOX variables that you set the environment variables and you can use this same playbook to get started. Just make the updates as needed!</p> <pre><code>---\n- name: \"GATHER DATA FROM NETBOX\"\n  hosts: localhost\n  connection: local\n  gather_facts: no\n  vars:\n    netbox_url: \"{{ lookup('env', 'NETBOX_API') }}\"\n    netbox_token: \"{{ lookup('env', 'NETBOX_TOKEN') }}\"\n  tasks:\n    - name: \"GET SITES WITH NB_QUERY\"\n      set_fact:\n        sites: \"{{ query('netbox.netbox.nb_lookup', 'sites', api_endpoint=netbox_url, token=netbox_token) }}\"\n\n    - debug:\n        msg: \"{{ sites | json_query('[*].value.name') }}\"\n\n    - name: \"TASK 3: GET ROLE ROUTERS AT DEN SITE\"\n      set_fact:\n        den_devices: \"{{ query('netbox.netbox.nb_lookup', 'devices', api_filter='site=den role=router', api_endpoint=netbox_url, token=netbox_token) }}\"\n\n    - name: \"TASK 4: PRINT THE DEVICES OF TYPE ROUTER AT DENVER LOCATION\"\n      debug:\n        msg: \"{{ den_devices | json_query('[*].value.name') }}\"\n\n    - name: \"TASK 5: GET DEVICES AT DEN &amp; MSP SITES\"\n      set_fact:\n        msp_den_devices: \"{{ query('netbox.netbox.nb_lookup', 'devices', api_filter='site=den site=msp', api_endpoint=netbox_url, token=netbox_token) }}\"\n\n    - name: \"TASK 6: PRINT THE DEVICES AT DEN &amp; MSP LOCATIONS\"\n      debug:\n        msg: \"{{ msp_den_devices | json_query('[*].value.name') }}\"\n\n    - name: \"TASK 7: GET DATA FROM NETBOX VIA THE REST API\"\n      uri:\n        url: \"{{ lookup('env', 'NETBOX_URL') }}/api/dcim/devices/?site=den\"\n        method: \"GET\"\n        headers:\n          Content-Type: \"application/json\"\n          Authorization: \"token {{ lookup('env', 'NETBOX_TOKEN') }}\"\n        status_code: 200\n      register: search_result\n\n    - name: \"TASK 8: PRINT LENGTH OF PAGED SETUP\"\n      debug:\n        msg:\n          - \"Length of result on paginated response: {{ search_result['json']['results'] | length }}\"\n          - \"Total results (if no paging): {{ search_result['json']['count'] }}\"\n\n    - name: \"TASK 9: GET DEVICES AT DEN SITE\"\n      set_fact:\n        den_devices: \"{{ query('netbox.netbox.nb_lookup', 'devices', api_filter='site=den', api_endpoint=netbox_url, token=netbox_token) }}\"\n\n    - name: \"TASK 10: PRINT THE DEVICES AT DEN LOCATION\"\n      debug:\n        msg: \"{{ den_devices | json_query('[*].value.name') }}\"\n</code></pre> <p>Let me know your thoughts below! Like it if you have found it valuable.</p> <p>Josh</p>"},{"location":"netbox-ansible-sites/","title":"NetBox Ansible Collection: Site Module","text":"<p>This post dives into the NetBox Ansible Content Collection module to create/update a Site. As I start into this series on looking at the modules that create/update/delete data from NetBox, the question that I keep asking myself is should I be looking at the modules that are creating/updating/deleting items? The reason that I ask this to myself is because I am a firm believer that automation should be coming from NetBox as its Source of Truth (SoT). You can hear/read plenty more about these thoughts on posts and videos here:</p> <ul> <li>Minneapolis Ansible Meetup April 2020 Talk</li> <li>Ansible Guest Blog Post</li> </ul> <p>Note</p> <p>This post was created when NetBox was an open source project used often in my automation framework. I have moved on to using Nautobot due to the project vision and providing a methodology that will drive network automation forward further. You may want to take a look at it yourself.</p> <p>When it comes to creating and deleting sites in NetBox, this one is an easy one. In my opinion this is a yes it should be. Most likely an IT tool is not the tool that will be the Source of Truth as it comes to physical sites. So this module in particualr that should be looked at and put into production use with Ansible.</p>"},{"location":"netbox-ansible-sites/#environment","title":"Environment","text":"<p>For this demo, here are the versions shown:</p> Component Version NetBox v2.9.9 (NetBox Docker) NetBox Ansible Collection v1.1.0 pynetbox 5.1.0"},{"location":"netbox-ansible-sites/#site-module","title":"Site Module","text":"<p>Within NetBox, the site is the most basic unit, and is required for devices to be added. This is the first thing that you should do when creating a NetBox instance is to start to build out sites. There are a many set of parameters that you can add to your sites, but the minimum required are:</p> <ul> <li>name: The name of the site</li> </ul> <p>Take a look at the documentation for all of the additional parameters. The ones that stick out to me (and there are many more) include:</p> <ul> <li>asn: The BGP AS Number</li> <li>contact name &amp; email: Site contact information</li> <li>physical and shipping addresses</li> <li>tags</li> <li>time_zone</li> </ul>"},{"location":"netbox-ansible-sites/#examples","title":"Examples","text":"<p>The point of these posts are to show examples and get you started. So let's get started. At the beginning of this there are going to be four sites that we can check out with the query function:</p> <pre><code>TASK [05 - QUERY SITES] **********************************************************************************************************************************************\n  ansible_facts:\n    site_list_before:\n    - DEN\n    - MSP\n    - NYC\n    - PDX\n</code></pre> <p>The data here instead of coming from a system of record that has sites will come from a YAML file. So the first step would be to look at getting data from a data source that has sites. This could be a CRM tool if you were a MSP, or any other tooling that has your sites. Here is what the data would look like:</p> <pre><code>---\nsites:\n  - name: MSP\n    time_zone: America/Chicago\n    status: active\n    description: Minneapolis\n  - name: DEN\n    time_zone: America/Denver\n    status: active\n    description: Denver\n  - name: NYC\n    time_zone: America/New_York\n    status: active\n    description: New York\n  - name: PDX\n    time_zone: America/Los_Angeles\n    status: active\n    description: Portland\n</code></pre> <p>Running the following playbook multiple times will show that the module itself is idempotent in that it will not keep creating sites.</p> <pre><code>---\n- name: \"SETUP SITES\"\n  hosts: localhost\n  connection: local\n  gather_facts: no\n  tasks:\n    - name: \"05 - QUERY SITES\"\n      set_fact:\n        site_list_before: \"{{ query('netbox.netbox.nb_lookup', 'sites') | json_query('[*].value.name') }}\"\n\n    - name: \"10 - SETUP SITES\"\n      netbox.netbox.netbox_site:\n        netbox_url: \"{{ lookup('env', 'NETBOX_URL') }}\"\n        netbox_token: \"{{ lookup('env', 'NETBOX_TOKEN') }}\"\n        data: \"{{ site }}\"\n        state: present\n        validate_certs: False\n      loop: \"{{ sites }}\"\n      loop_control:\n        loop_var: site\n        label: \"{{ site['name'] }}\"\n</code></pre> <p>The output below is from a second run. The sites for the current NetBox demo was originally deployed with this.</p> <pre><code>TASK [05 - QUERY SITES] **********************************************************************************************\nok: [localhost] =&gt; changed=false \n  ansible_facts:\n    site_list_before:\n    - DEN\n    - MSP\n    - NYC\n    - PDX\n\nTASK [10 - SETUP SITES] **********************************************************************************************\nok: [localhost] =&gt; (item=MSP) =&gt; changed=false \n  ansible_loop_var: site\n  msg: site MSP already exists\n  site:\n    description: Minneapolis\n    name: MSP\n    status: active\n    time_zone: America/Chicago\nok: [localhost] =&gt; (item=DEN) =&gt; changed=false \n  ansible_loop_var: site\n  msg: site DEN already exists\n  site:\n    description: Denver\n    name: DEN\n    status: active\n    time_zone: America/Denver\nok: [localhost] =&gt; (item=NYC) =&gt; changed=false \n  ansible_loop_var: site\n  msg: site NYC already exists\n  site:\n    description: New York\n    name: NYC\n    status: active\n    time_zone: America/New_York\nok: [localhost] =&gt; (item=PDX) =&gt; changed=false \n  ansible_loop_var: site\n  msg: site PDX already exists\n  site:\n    description: Portland\n    name: PDX\n    status: active\n    time_zone: America/Los_Angeles\n\nPLAY RECAP ***********************************************************************************************************\nlocalhost                  : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n</code></pre> <p>In here we see that there were two tasks that showed ok and no tasks in the other sections of the play recap.</p>"},{"location":"netbox-ansible-sites/#adding-an-additional-site","title":"Adding an additional site","text":"<p>The source for the sites just had a new site added. This is adding the Orlando location. As such the data now looks like this:</p> <pre><code>---\nsites:\n  - name: MSP\n    time_zone: America/Chicago\n    status: active\n    description: Minneapolis\n  - name: DEN\n    time_zone: America/Denver\n    status: active\n    description: Denver\n  - name: NYC\n    time_zone: America/New_York\n    status: active\n    description: New York\n  - name: PDX\n    time_zone: America/Los_Angeles\n    status: active\n    description: Portland\n  - name: MCO\n    time_zone: America/New_York\n    status: active\n    description: Orlando\n</code></pre> <p>With the new location, the Ansible Playbook is executed and we see a new site is added:</p> <pre><code>TASK [05 - QUERY SITES] **********************************************************************************************\nok: [localhost] =&gt; changed=false \n  ansible_facts:\n    site_list_before:\n    - DEN\n    - MSP\n    - NYC\n    - PDX\n\nTASK [10 - SETUP SITES] **********************************************************************************************\nok: [localhost] =&gt; (item=MSP) =&gt; changed=false \n  ansible_loop_var: site\n  msg: site MSP already exists\n  site:\n    description: Minneapolis\n    name: MSP\n    status: active\n    time_zone: America/Chicago\nok: [localhost] =&gt; (item=DEN) =&gt; changed=false \n  ansible_loop_var: site\n  msg: site DEN already exists\n  site:\n    description: Denver\n    name: DEN\n    status: active\n    time_zone: America/Denver\nok: [localhost] =&gt; (item=NYC) =&gt; changed=false \n  ansible_loop_var: site\n  msg: site NYC already exists\n  site:\n    description: New York\n    name: NYC\n    status: active\n    time_zone: America/New_York\nok: [localhost] =&gt; (item=PDX) =&gt; changed=false \n  ansible_loop_var: site\n  msg: site PDX already exists\n  site:\n    description: Portland\n    name: PDX\n    status: active\n    time_zone: America/Los_Angeles\nchanged: [localhost] =&gt; (item=MCO) =&gt; changed=true \n  ansible_loop_var: site\n  msg: site MCO created\n  site:\n    description: Orlando\n    name: MCO\n    status: active\n    time_zone: America/New_York\n\nPLAY RECAP ***********************************************************************************************************\nlocalhost                  : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n</code></pre> <p>Taking a look at line 45 in this last execution you see the message site MCO created. This shows that it as created and there was a task that showed chagned in the play recap.</p>"},{"location":"netbox-ansible-sites/#removing-sites","title":"Removing Sites","text":"<p>In this example for the removing of sites I am going to keep it a little bit more manual. I'm going to create a new variable in the <code>group_vars/all/sites.yml</code> file called <code>closed_sites</code>. So in this scenario the Orlando site was opened, but very quickly it was decided to close it down. So now we need to remove the site from NetBox. The <code>group_vars/all/sites.yml</code> now looks like below:</p> <pre><code>---\nsites:\n  - name: MSP\n    time_zone: America/Chicago\n    status: active\n    description: Minneapolis\n  - name: DEN\n    time_zone: America/Denver\n    status: active\n    description: Denver\n  - name: NYC\n    time_zone: America/New_York\n    status: active\n    description: New York\n  - name: PDX\n    time_zone: America/Los_Angeles\n    status: active\n    description: Portland\nclosed_sites:\n  - name: MCO\n    time_zone: America/New_York\n    status: active\n    description: Orlando\n</code></pre> <p>The updated Ansible Playbook now needs to remove any sites that are showing up in the closed sites:</p> <pre><code>---\n- name: \"SETUP SITES\"\n  hosts: localhost\n  connection: local\n  gather_facts: no\n  tasks:\n    - name: \"05 - QUERY SITES\"\n      set_fact:\n        site_list_before: \"{{ query('netbox.netbox.nb_lookup', 'sites') | json_query('[*].value.name') }}\"\n\n    - name: \"10 - SETUP SITES\"\n      netbox.netbox.netbox_site:\n        netbox_url: \"{{ lookup('env', 'NETBOX_URL') }}\"\n        netbox_token: \"{{ lookup('env', 'NETBOX_TOKEN') }}\"\n        data: \"{{ site }}\"\n        state: present\n        validate_certs: False\n      loop: \"{{ sites }}\"\n      loop_control:\n        loop_var: site\n        label: \"{{ site['name'] }}\"\n\n    - name: \"20 - REMOVE CLOSED SITES\"\n      netbox.netbox.netbox_site:\n        netbox_url: \"{{ lookup('env', 'NETBOX_URL') }}\"\n        netbox_token: \"{{ lookup('env', 'NETBOX_TOKEN') }}\"\n        data: \"{{ site }}\"\n        state: absent\n        validate_certs: False\n      loop: \"{{ sites }}\"\n      loop_control:\n        loop_var: site\n        label: \"{{ site['name'] }}\"\n\n    - name: \"25 - QUERY SITES AT END\"\n      set_fact:\n        site_list_end: \"{{ query('netbox.netbox.nb_lookup', 'sites') | json_query('[*].value.name') }}\"\n\n    - name: \"30 - SHOW RESULTS\"\n      debug:\n        msg:\n          - \"{{ site_list_before }}\"\n          - \"{{ site_list_end }}\"\n</code></pre> <p>The result of the playbook shows that we had the site at the beginning, then we were able to successfully remove it in Task 20 to remove the closed sites.</p> <p>When removing a site, you do need to make sure that all of the corresponding devices and other relationships are gone from the site. NetBox will not allow you to remove a site without it being empty first.</p> <pre><code>TASK [05 - QUERY SITES] **********************************************************************************************\ntask path: /local/add_sites.yml:7\nok: [localhost] =&gt; changed=false \n  ansible_facts:\n    site_list_before:\n    - DEN\n    - MCO\n    - MSP\n    - NYC\n    - PDX\n\nTASK [10 - SETUP SITES] **********************************************************************************************\ntask path: /local/add_sites.yml:11\nok: [localhost] =&gt; (item=MSP) =&gt; changed=false \n  ansible_loop_var: site\n  msg: site MSP already exists\n  site:\n    description: Minneapolis\n    name: MSP\n    status: active\n    time_zone: America/Chicago\nok: [localhost] =&gt; (item=DEN) =&gt; changed=false \n  ansible_loop_var: site\n  msg: site DEN already exists\n  site:\n    description: Denver\n    name: DEN\n    status: active\n    time_zone: America/Denver\nok: [localhost] =&gt; (item=NYC) =&gt; changed=false \n  ansible_loop_var: site\n  msg: site NYC already exists\n  site:\n    description: New York\n    name: NYC\n    status: active\n    time_zone: America/New_York\nok: [localhost] =&gt; (item=PDX) =&gt; changed=false \n  ansible_loop_var: site\n  msg: site PDX already exists\n  site:\n    description: Portland\n    name: PDX\n    status: active\n    time_zone: America/Los_Angeles\n\nTASK [20 - REMOVE CLOSED SITES] **************************************************************************************\ntask path: /local/add_sites.yml:23\nchanged: [localhost] =&gt; (item=MCO) =&gt; changed=true \n  ansible_loop_var: site\n  msg: site MCO deleted\n  site:\n    description: Orlando\n    name: MCO\n    status: active\n    time_zone: America/New_York\n\nTASK [25 - QUERY SITES AT END] ***************************************************************************************\ntask path: /local/add_sites.yml:35\nok: [localhost] =&gt; changed=false \n  ansible_facts:\n    site_list_end:\n    - DEN\n    - MSP\n    - NYC\n    - PDX\n\nTASK [30 - SHOW RESULTS] *********************************************************************************************\ntask path: /local/add_sites.yml:39\nok: [localhost] =&gt; \n  msg:\n  - - DEN\n    - MCO\n    - MSP\n    - NYC\n    - PDX\n  - - DEN\n    - MSP\n    - NYC\n    - PDX\nMETA: ran handlers\nMETA: ran handlers\n\nPLAY RECAP ***********************************************************************************************************\nlocalhost                  : ok=5    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0  \n</code></pre>"},{"location":"netbox-ansible-sites/#summary","title":"Summary","text":"<p>This module is a very good module with a lot of options to get you started. This is absolutely a module that I would become familiar with as your organization is changing over time. This will allow you to keep your NetBox environment up to date with the site changes as you get new and closed sites alike. Hopefully this has been helpful to demonstrate it's capabities. Let me know your comments below, or give it a thumbs up if you have found this helpful.  </p> <p>Thanks,</p> <p>Josh</p>"},{"location":"netbox-ansible-manufacturers/","title":"NetBox Ansible Collection: Manufacturers","text":"<p>Adding your manufacturers via code is the easy way to get started with your NetBox devices. Immediately after adding Sites, the next thing to get going when using NetBox as your Source of Truth is to add in Manufacturers. These are just that, who makes the gear that you use. For this demonstration you will see adding just a few manufacturers. I'm not necessarily picking on any vendors and who should or shouldn't be here. It is just what my background brings.</p> <p>Note</p> <p>This post was created when NetBox was an open source project used often in my automation framework. I have moved on to using Nautobot due to the project vision and providing a methodology that will drive network automation forward further. You may want to take a look at it yourself.</p>"},{"location":"netbox-ansible-manufacturers/#module-documentation","title":"Module Documentation","text":"<ul> <li>Read the Docs</li> <li>GitHub</li> </ul> <p>This module does require pynetbox to execute properly</p>"},{"location":"netbox-ansible-manufacturers/#environment","title":"Environment","text":"<p>For this demo, here are the versions shown:</p> Component Version NetBox v2.9.9 (NetBox Docker) NetBox Ansible Collection v1.1.0 pynetbox 5.1.0"},{"location":"netbox-ansible-manufacturers/#data-file","title":"Data File","text":"<p>The documentation indicates that there are two parameters, name and slug. I'm not going to modify the slug in any way for these as the auto-generated slug is just fine. Because of this, the demo will not have a more complex variable, just a list of manufacturers.</p> <pre><code>---\nmanufacturers:\n  - Arista\n  - Cisco\n  - Juniper\n</code></pre>"},{"location":"netbox-ansible-manufacturers/#example","title":"Example","text":""},{"location":"netbox-ansible-manufacturers/#example-adding-devices","title":"Example - Adding Devices","text":"<p>Getting started I already have a Cisco manufacturer included from a different demo. This will not hurt what is being demonstrated here. The task to add a manufacturer looks like:</p> <pre><code>---\n- name: \"ADD MANUFACTURERS TO NETBOX\"\n  hosts: localhost\n  connection: local\n  gather_facts: false # No gathering facts about the container execution env\n  tasks:\n    - name: \"05 - ADD MANUFACTURERS\" # Already present, showing idempotency\n      netbox.netbox.netbox_manufacturer:\n        netbox_url: \"{{ lookup('env', 'NETBOX_URL') }}\"\n        netbox_token: \"{{ lookup('env', 'NETBOX_TOKEN') }}\"\n        data:\n          name: \"{{ item }}\"\n      loop: \"{{ manufacturers }}\"\n</code></pre> <p>Here is the before:</p> <p></p>"},{"location":"netbox-ansible-manufacturers/#example-execution","title":"Example - Execution","text":"<p>Pretty short and sweet on this playbook. With having <code>Cisco</code> already present, you can see that the module is idempotent:</p> <pre><code>josh-v@d27199d82bfc:~$ ansible-playbook add_manufacturers.yml \n[WARNING]: No inventory was parsed, only implicit localhost is available\n[WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all'\n\nPLAY [ADD MANUFACTURERS TO NETBOX] *******************************************************************************************************\n\nTASK [05 - ADD MANUFACTURERS] ************************************************************************************************************\nchanged: [localhost] =&gt; (item=Arista)\nok: [localhost] =&gt; (item=Cisco)\nchanged: [localhost] =&gt; (item=Juniper)\n\nPLAY RECAP *******************************************************************************************************************************\nlocalhost                  : ok=1    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n</code></pre> <p>Now you see all of them showing up in NetBox.</p> <p></p>"},{"location":"netbox-ansible-manufacturers/#summary","title":"Summary","text":"<p>The manufacturers are a base to adding devices. To add/sync manufacturers you can leverage the NetBox Ansible Collection for manufacturers to get this data synced. To have your entire environment completely automated with using Ansible, this is a great solution.</p>"},{"location":"netbox-ansible-platforms/","title":"NetBox Ansible Collection: Platforms","text":"<p>Platforms are an optional item when adding devices into NetBox. The platform is the OS that you are going to be using. Most often this is used to help identify which driver your automation platform is going to be using. Specifically the slug of the platform is what needs to match. So in the terms of Ansible (since we are using Ansible to populate NetBox), you will want to set Cisco IOS devices to ios. By having the slug match the automation platform name you have that information in your inventory. For these reasons I strongly recommend setting the Platform for devices.</p> <p>Note</p> <p>This post was created when NetBox was an open source project used often in my automation framework. I have moved on to using Nautobot due to the project vision and providing a methodology that will drive network automation forward further. You may want to take a look at it yourself.</p>"},{"location":"netbox-ansible-platforms/#module-documentation","title":"Module Documentation","text":"<ul> <li>Read the Docs</li> <li>GitHub</li> </ul> <p>This module does require pynetbox to execute properly</p>"},{"location":"netbox-ansible-platforms/#environment","title":"Environment","text":"<p>For this demo, here are the versions shown:</p> Component Version NetBox v2.9.9 (NetBox Docker) NetBox Ansible Collection v1.1.0 pynetbox 5.1.0"},{"location":"netbox-ansible-platforms/#data-file","title":"Data File","text":"<p>Now that you may want to have a different slug than what is displayed, the data structure is getting slightly more complex than the manufacturers file. There will be a list of dictionaries, where the dictionary has three keys: name, slug, and manufacturer.</p> <pre><code>---\nplatforms:\n  - name: Arista EOS\n    slug: eos\n    manufacturer: Arista\n  - name: Cisco IOS\n    slug: ios\n    manufacturer: Cisco\n  - name: JUNOS\n    slug: junos\n    manufacturer: Juniper\n</code></pre>"},{"location":"netbox-ansible-platforms/#example","title":"Example","text":""},{"location":"netbox-ansible-platforms/#example-adding-devices","title":"Example - Adding Devices","text":"<p>Getting started I already have a Cisco manufacturer included from a different demo. This will not hurt what is being demonstrated here. The task to add a manufacturer looks like:</p> <pre><code>---\n- name: \"ADD PLATFORMS TO NETBOX\"\n  hosts: localhost\n  connection: local\n  gather_facts: false # No gathering facts about the container execution env\n  tasks:\n    - name: \"05 - ADD PLATFORMS\"\n      netbox.netbox.netbox_platform:\n        netbox_url: \"{{ lookup('env', 'NETBOX_URL') }}\"\n        netbox_token: \"{{ lookup('env', 'NETBOX_TOKEN') }}\"\n        data:\n          name: \"{{ item['name'] }}\"\n      loop: \"{{ platforms }}\"\n</code></pre>"},{"location":"netbox-ansible-platforms/#example-execution","title":"Example - Execution","text":"<p>This execution shows that all of the platforms are added.</p> <pre><code>josh-v@d27199d82bfc:~$ ansible-playbook add_platforms.yml \n[WARNING]: No inventory was parsed, only implicit localhost is available\n[WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all'\n\nPLAY [ADD PLATFORMS TO NETBOX] ***********************************************************************************************************************\n\nTASK [05 - ADD PLATFORMS] ****************************************************************************************************************************\nchanged: [localhost] =&gt; (item={'name': 'Arista EOS', 'slug': 'eos', 'manufacturer': 'Arista'})\nchanged: [localhost] =&gt; (item={'name': 'Cisco IOS', 'slug': 'ios', 'manufacturer': 'Cisco'})\nchanged: [localhost] =&gt; (item={'name': 'JUNOS', 'slug': 'junos', 'manufacturer': 'Juniper'})\n\nPLAY RECAP *******************************************************************************************************************************************\nlocalhost                  : ok=1    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n</code></pre> <p>The second execution of playbook shows that with these three settings the module is idempotent:</p> <pre><code>josh-v@d27199d82bfc:~$ ansible-playbook add_platforms.yml \n[WARNING]: No inventory was parsed, only implicit localhost is available\n[WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all'\n\nPLAY [ADD PLATFORMS TO NETBOX] ***********************************************************************************************************************\n\nTASK [05 - ADD PLATFORMS] ****************************************************************************************************************************\nok: [localhost] =&gt; (item={'name': 'Arista EOS', 'slug': 'eos', 'manufacturer': 'Arista'})\nok: [localhost] =&gt; (item={'name': 'Cisco IOS', 'slug': 'ios', 'manufacturer': 'Cisco'})\nok: [localhost] =&gt; (item={'name': 'JUNOS', 'slug': 'junos', 'manufacturer': 'Juniper'})\n\nPLAY RECAP *******************************************************************************************************************************************\nlocalhost                  : ok=1    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n</code></pre> <p>Now you see all of them showing up in NetBox.</p> <p></p> <p>When editing the Cisco platform you see the result visually.</p> <p></p>"},{"location":"netbox-ansible-platforms/#summary","title":"Summary","text":"<p>Platforms are one of the items that you will strongly want to get updated into NetBox. By associating a device with a platform you can then use it in the inventory plugins to identify things such as the <code>ansible_network_os</code> dynamically. Need to have a new platform to test things with, just create a new platform, change a few settings, and the information is dynamically available within your playbooks.  </p> <p>Hope this has helped. If so, let me know with a comment below or give a thumbs up on the post.</p>"},{"location":"netbox-ansible-device-roles/","title":"NetBox Ansible Collection: Device Roles","text":"<p>A device role is aptly named, the role of the device. This is likely to be something that is meaningful to your organization and could change. For example you may have the 3 tier system of Core, Distribution, and Access layer environments. These are just fine. So you would want to have the roles there to reflect this reality. You may have leaf-spine environments, there are two more roles. And in my past I have also had roles that would indicate that there are dedicated DMZ, WAN edge, Internet edge devices. So this is the place to set this.</p> <p>Note</p> <p>This post was created when NetBox was an open source project used often in my automation framework. I have moved on to using Nautobot due to the project vision and providing a methodology that will drive network automation forward further. You may want to take a look at it yourself.</p>"},{"location":"netbox-ansible-device-roles/#module-documentation","title":"Module Documentation","text":"<ul> <li>Read the Docs</li> <li>GitHub</li> </ul> <p>This module does require pynetbox to execute properly</p> <p>Outside of the NetBox URL and Token, the data parameter has a single required parameter of name. There are only a few additional options, so those are worth mentioning here of color, slug (will be auto-generated if not), and a yes/no parameter of vm_role.</p>"},{"location":"netbox-ansible-device-roles/#environment","title":"Environment","text":"<p>For this demo, here are the versions shown:</p> Component Version NetBox v2.9.10 (NetBox Docker) NetBox Ansible Collection v2.0.0 pynetbox 5.3.1"},{"location":"netbox-ansible-device-roles/#data-file","title":"Data File","text":"<p>The roles are going to be a little more straight forward. We will only set the name, color, and if the role can be a VM or not, from the vm_role key.</p> <pre><code>---\ndevice_roles:\n  - name: Firewall\n    color: \"FF0000\"\n    vm_role: true\n  - name: Leaf\n    color: \"008000\"\n    vm_role: false\n  - name: Router\n    color: \"000080\"\n    vm_role: true\n  - name: Server\n    color: \"000000\"\n    vm_role: false\n  - name: Spine\n    color: \"0000FF\"\n    vm_role: false\n  - name: Switch\n    color: \"008000\"\n    vm_role: true\n  - name: VM\n    color: \"00FFFF\"\n    vm_role: true\n</code></pre>"},{"location":"netbox-ansible-device-roles/#example","title":"Example","text":""},{"location":"netbox-ansible-device-roles/#example-adding-device-roles","title":"Example - Adding Device Roles","text":"<p>Running the playbook on the roles are going to be straight to the point.</p> <pre><code>---\n- name: \"ADD DEVICE ROLES TO NETBOX\"\n  hosts: localhost\n  connection: local\n  gather_facts: false # No gathering facts about the container execution env\n  tasks:\n    - name: \"05 - ADD DEVICE ROLES\" # Already present, showing idempotency\n      netbox.netbox.netbox_device_role:\n        netbox_url: \"{{ lookup('env', 'NETBOX_URL') }}\"\n        netbox_token: \"{{ lookup('env', 'NETBOX_TOKEN') }}\"\n        data:\n          name: \"{{ item['name'] }}\"\n          color: \"{{ item['color'] }}\"\n          vm_role: \"{{ item['vm_role'] }}\"\n      loop: \"{{ device_roles }}\"\n</code></pre>"},{"location":"netbox-ansible-device-roles/#example-execution","title":"Example - Execution","text":"<p>This execution shows that all of the device types are added.</p> <pre><code>josh-v@588715249c44:~$ ansible-playbook add_device_role.yml \n[WARNING]: No inventory was parsed, only implicit localhost is available\n[WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all'\n\nPLAY [ADD DEVICE ROLES TO NETBOX] *********************************************************************************************************************\n\nTASK [05 - ADD DEVICE ROLES] **************************************************************************************************************************\nchanged: [localhost] =&gt; (item={'name': 'Firewall', 'color': 'FF0000', 'vm_role': True})\nchanged: [localhost] =&gt; (item={'name': 'Leaf', 'color': '008000', 'vm_role': False})\nchanged: [localhost] =&gt; (item={'name': 'Router', 'color': '000080', 'vm_role': True})\nchanged: [localhost] =&gt; (item={'name': 'Server', 'color': '000000', 'vm_role': False})\nchanged: [localhost] =&gt; (item={'name': 'Spine', 'color': '0000FF', 'vm_role': False})\nchanged: [localhost] =&gt; (item={'name': 'Switch', 'color': '008000', 'vm_role': True})\nchanged: [localhost] =&gt; (item={'name': 'VM', 'color': '00FFFF', 'vm_role': True})\n\nPLAY RECAP ********************************************************************************************************************************************\nlocalhost                  : ok=1    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n</code></pre> <p>The second execution of playbook shows that with these three settings the module is idempotent:</p> <pre><code>josh-v@588715249c44:~$ ansible-playbook add_device_role.yml \n[WARNING]: No inventory was parsed, only implicit localhost is available\n[WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all'\n\nPLAY [ADD DEVICE ROLES TO NETBOX] *********************************************************************************************************************\n\nTASK [05 - ADD DEVICE ROLES] **************************************************************************************************************************\nok: [localhost] =&gt; (item={'name': 'Firewall', 'color': 'FF0000', 'vm_role': True})\nok: [localhost] =&gt; (item={'name': 'Leaf', 'color': '008000', 'vm_role': False})\nok: [localhost] =&gt; (item={'name': 'Router', 'color': '000080', 'vm_role': True})\nok: [localhost] =&gt; (item={'name': 'Server', 'color': '000000', 'vm_role': False})\nok: [localhost] =&gt; (item={'name': 'Spine', 'color': '0000FF', 'vm_role': False})\nok: [localhost] =&gt; (item={'name': 'Switch', 'color': '008000', 'vm_role': True})\nok: [localhost] =&gt; (item={'name': 'VM', 'color': '00FFFF', 'vm_role': True})\n\nPLAY RECAP ********************************************************************************************************************************************\nlocalhost                  : ok=1    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n</code></pre> <p>After completion of this you will have the device roles are now available to be assigned out.</p>"},{"location":"netbox-ansible-device-roles/#summary","title":"Summary","text":"<p>Device roles are a required item to add devices to NetBox. This can be as generic as \"Device\" or \"Network Device\". However, I strongly encourage you to look at putting some thought into the roles that you will assign to devices. This will become very helpful in the future as you look at building out the automation platform. You can see in the inventory build, you can assign devices based on roles to an inventory group. This becomes particularly helpful when you want to run a playbook against a single group, such as all Leaf switches, or all Spine switches that must have a particular configuration set.  </p> <p>Hope this has helped. If so, let me know with a comment below or give a thumbs up on the post.</p>"},{"location":"netbox-ansible-device-types/","title":"NetBox Ansible Collection: Device Types","text":"<p>A device type is the next piece in the NetBox Device onboarding requirements. The device type corresponds to the model number of the hardware (or virtual machine). This is where you are able to template out devices during their creation. So if you have a console port on a device type, that console port will be created when you create the device. However, there is NOT a relationship built between the device type and the device. If the device type gets updated after the device is created, the device itself is not updated. </p> <p>Note</p> <p>This post was created when NetBox was an open source project used often in my automation framework. I have moved on to using Nautobot due to the project vision and providing a methodology that will drive network automation forward further. You may want to take a look at it yourself.</p>"},{"location":"netbox-ansible-device-types/#module-documentation","title":"Module Documentation","text":"<ul> <li>Read the Docs</li> <li>GitHub</li> </ul> <p>This module does require pynetbox to execute properly</p>"},{"location":"netbox-ansible-device-types/#environment","title":"Environment","text":"<p>For this demo, here are the versions shown:</p> Component Version NetBox v2.9.9 (NetBox Docker) NetBox Ansible Collection v1.1.0 pynetbox 5.1.0"},{"location":"netbox-ansible-device-types/#data-file","title":"Data File","text":"<p>This gets to be a little more of the complex data source types. There are many data parameters that are good to include. The minimum data parameter has just the model. But there are going to be many more options as you build out your NetBox environment that feeds into the data correlation that makes NetBox a pleasure to use. Such as the manufacturer that it is tied to, the part number, and the u_height as you build rack diagrams from NetBox.  </p> <p>In the demo the model, manufacturer, part number, and slug will get defined. The slug will be the lower case of the model name. The primary key is the model name in this case.</p> <pre><code>---\ndevice_types:\n  - model: \"ASAv\"\n    manufacturer: \"Cisco\"\n    slug: \"asav\"\n    part_number: \"asav\"\n  - model: \"CSR1000v\"\n    manufacturer: \"Cisco\"\n    slug: \"csr1000v\"\n    part_number: \"csr1000v\"\n  - model: \"IOSv\"\n    manufacturer: \"Cisco\"\n    slug: \"iosv\"\n    part_number: \"iosv\"\n  - model: \"nxosv\"\n    manufacturer: \"Cisco\"\n    slug: \"nxosv\"\n    part_number: \"nxosv\"\n  - model: \"vEOS\"\n    manufacturer: \"Arista\"\n    slug: \"veos\"\n    part_number: \"veos\"\n</code></pre>"},{"location":"netbox-ansible-device-types/#example","title":"Example","text":""},{"location":"netbox-ansible-device-types/#example-adding-device-types","title":"Example - Adding Device Types","text":"<p>Getting started I already have a Cisco manufacturer included from a different demo. This will not hurt what is being demonstrated here. The task to add a manufacturer looks like:</p> <pre><code>---\n- name: \"ADD DEVICE TYPES TO NETBOX\"\n  hosts: localhost\n  connection: local\n  gather_facts: false # No gathering facts about the container execution env\n  tasks:\n    - name: \"05 - ADD DEVICE TYPES\"\n      netbox.netbox.netbox_device_type:\n        netbox_url: \"{{ lookup('env', 'NETBOX_URL') }}\"\n        netbox_token: \"{{ lookup('env', 'NETBOX_TOKEN') }}\"\n        data:\n          model: \"{{ item['model'] }}\"\n          manufacturer: \"{{ item['manufacturer'] }}\"\n          slug: \"{{ item['slug'] }}\"\n          part_number: \"{{ item['part_number'] }}\"\n      loop: \"{{ device_types }}\"\n</code></pre>"},{"location":"netbox-ansible-device-types/#example-execution","title":"Example - Execution","text":"<p>This execution shows that all of the device types are added.</p> <pre><code>josh-v@d27199d82bfc:~$ ansible-playbook add_devices.yml \n[WARNING]: No inventory was parsed, only implicit localhost is available\n[WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all'\n\nPLAY [ADD DEVICE TYPES TO NETBOX] ********************************************************************************************************************\n\nTASK [05 - ADD DEVICE TYPES] *************************************************************************************************************************\nchanged: [localhost] =&gt; (item={'model': 'ASAv', 'manufacturer': 'Cisco', 'slug': 'asav', 'part_number': 'asav'})\nchanged: [localhost] =&gt; (item={'model': 'CSR1000v', 'manufacturer': 'Cisco', 'slug': 'csr1000v', 'part_number': 'csr1000v'})\nchanged: [localhost] =&gt; (item={'model': 'IOSv', 'manufacturer': 'Cisco', 'slug': 'iosv', 'part_number': 'iosv'})\nchanged: [localhost] =&gt; (item={'model': 'nxosv', 'manufacturer': 'Cisco', 'slug': 'nxosv', 'part_number': 'nxosv'})\nchanged: [localhost] =&gt; (item={'model': 'vEOS', 'manufacturer': 'Arista', 'slug': 'veos', 'part_number': 'veos'})\n\nPLAY RECAP *******************************************************************************************************************************************\nlocalhost                  : ok=1    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0  \n</code></pre> <p>The second execution of playbook shows that with these three settings the module is idempotent:</p> <pre><code>josh-v@d27199d82bfc:~$ ansible-playbook add_devices.yml \n[WARNING]: No inventory was parsed, only implicit localhost is available\n[WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all'\n\nPLAY [ADD DEVICE TYPES TO NETBOX] ********************************************************************************************************************\n\nTASK [05 - ADD DEVICE TYPES] *************************************************************************************************************************\nok: [localhost] =&gt; (item={'model': 'ASAv', 'manufacturer': 'Cisco', 'slug': 'asav', 'part_number': 'asav'})\nok: [localhost] =&gt; (item={'model': 'CSR1000v', 'manufacturer': 'Cisco', 'slug': 'csr1000v', 'part_number': 'csr1000v'})\nok: [localhost] =&gt; (item={'model': 'IOSv', 'manufacturer': 'Cisco', 'slug': 'iosv', 'part_number': 'iosv'})\nok: [localhost] =&gt; (item={'model': 'nxosv', 'manufacturer': 'Cisco', 'slug': 'nxosv', 'part_number': 'nxosv'})\nok: [localhost] =&gt; (item={'model': 'vEOS', 'manufacturer': 'Arista', 'slug': 'veos', 'part_number': 'veos'})\n\nPLAY RECAP *******************************************************************************************************************************************\nlocalhost                  : ok=1    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n</code></pre> <p>After completion of this you will have the device types (hardware models) available for you to assign to devices (coming up next).</p>"},{"location":"netbox-ansible-device-types/#summary","title":"Summary","text":"<p>Device types are important so you know what model of devices you have to work with. This will come in handy as well in your automations that if you have a particular device type that you need to do something against. Such as having a separate type for Cisco Catalyst 3750G vs Catalyst 3750X. They are all 3750 switches, however you may need to apply a unique configuration set against a particular device type. By having this predefined in your source of truth, you are all set to be able to run automations against each.  </p> <p>Hope this has helped. If so, let me know with a comment below or give a thumbs up on the post.</p>"},{"location":"netbox-ansible-devices/","title":"NetBox Ansible Collection: Devices","text":"<p>All of the work through the modules thus far in the series have brought us to what we all want to see. How to get or update device information inside of NetBox. Adding of sites, device types, device roles are required to get us to this point. Now you can see how to add a device to NetBox using the netbox.netbox.netbox_device module.  </p> <p>Note</p> <p>This post was created when NetBox was an open source project used often in my automation framework. I have moved on to using Nautobot due to the project vision and providing a methodology that will drive network automation forward further. You may want to take a look at it yourself.</p> <p>There are many optional parameters for the module specifically. I encourage you to take a look at the module documentaation (linked below) in order to get a good sense of all of the options available. The required parameters for a device that is present are:</p> <ul> <li>device_role</li> <li>device_type</li> <li>name</li> <li>site</li> </ul> <p>An important caveat for me is that this is something that should be done with rarity. Only when truly adding a device to NetBox, in a programmatic way this should be used. I do not advocate for running this module constantly based on your devices. The idea is to get NetBox to be your source of truth about devices, not to have devices be the source of truth and updating NetBox.  </p> <p>So where do I see this being run? I do absolutely see it being a part of a pipeline or a service portal. The idea being that the service portal has a request for a new site to be turned up. That in turn kicks off an Ansible Playbook that will make the necessary updates to NetBox, and is done in a consistent manor.</p>"},{"location":"netbox-ansible-devices/#module-documentation","title":"Module Documentation","text":"<ul> <li>Read the Docs</li> <li>GitHub</li> </ul> <p>This module does require pynetbox to execute properly</p>"},{"location":"netbox-ansible-devices/#environment","title":"Environment","text":"<p>For this demo, here are the versions shown:</p> Component Version NetBox v2.9.10 (NetBox Docker) NetBox Ansible Collection v1.1.0 pynetbox 5.1.0"},{"location":"netbox-ansible-devices/#data-file","title":"Data File","text":"<p>The NetBox devices file is going to be a little bit more involved. In this particular demo case there are no existing inventories to use. If you want to see a demo of how to add devices to NetBox using an existing Ansible inventory, I encourage you to take a look at my GitHub repository where I did a Meetup video on working with Ansible + NetBox.  </p> <p>To simulate the idea that we are going to be running a playbook execution as part of a service request, here is the data file that will be fed to the Ansible playbook:</p> <pre><code># group_vars/all/devices.yml\n---\ndevices:\n  - name: \"grb-rtr01\"\n    site: \"GRB\"\n    device_role: \"Router\"\n    device_type: \"IOSv\"\n  - name: \"msp-rtr01\"\n    site: \"MSP\"\n    device_role: \"Router\"\n    device_type: \"IOSv\"\n</code></pre>"},{"location":"netbox-ansible-devices/#example","title":"Example","text":""},{"location":"netbox-ansible-devices/#example-adding-device-types","title":"Example - Adding Device Types","text":"<p>First if you are following along with the examples thus far, I made a new site here. So in order to accommodate the new site, I added GRB and re-ran the playbook to create sites. That was done successfully and idempotently with only the GRB site being added.</p> <pre><code>---\n- name: \"ADD DEVICES TO NETBOX\"\n  hosts: localhost\n  connection: local\n  gather_facts: false # No gathering facts about the container execution env\n  tasks:\n    - name: \"05 - ADD DEVICES\"\n      netbox.netbox.netbox_device:\n        netbox_url: \"{{ lookup('env', 'NETBOX_URL') }}\"\n        netbox_token: \"{{ lookup('env', 'NETBOX_TOKEN') }}\"\n        data:\n          name: \"{{ item['name'] }}\"\n          site: \"{{ item['site'] }}\"\n          device_role: \"{{ item['device_role'] }}\"\n          device_type: \"{{ item['device_type'] }}\"\n      loop: \"{{ devices }}\"\n</code></pre>"},{"location":"netbox-ansible-devices/#example-execution","title":"Example - Execution","text":"<p>This execution shows that all of the device types are added.</p> <pre><code>josh-v@588715249c44:~$ ansible-playbook add_devices.yml\n[WARNING]: No inventory was parsed, only implicit localhost is available\n[WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all'\n\nPLAY [ADD DEVICES TO NETBOX] **************************************************************************************************************************\n\nTASK [05 - ADD DEVICES] *******************************************************************************************************************************\nchanged: [localhost] =&gt; (item={'name': 'grb-rtr01', 'site': 'GRB', 'device_role': 'Router', 'device_type': 'IOSv'})\nchanged: [localhost] =&gt; (item={'name': 'msp-rtr01', 'site': 'MSP', 'device_role': 'Router', 'device_type': 'IOSv'})\n\nPLAY RECAP ********************************************************************************************************************************************\nlocalhost                  : ok=1    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0 \n</code></pre> <p>The second execution of playbook shows that with these three settings the module is idempotent:</p> <pre><code>josh-v@588715249c44:~$ ansible-playbook add_devices.yml\n[WARNING]: No inventory was parsed, only implicit localhost is available\n[WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all'\n\nPLAY [ADD DEVICES TO NETBOX] **************************************************************************************************************************\n\nTASK [05 - ADD DEVICES] *******************************************************************************************************************************\nok: [localhost] =&gt; (item={'name': 'grb-rtr01', 'site': 'GRB', 'device_role': 'Router', 'device_type': 'IOSv'})\nok: [localhost] =&gt; (item={'name': 'msp-rtr01', 'site': 'MSP', 'device_role': 'Router', 'device_type': 'IOSv'})\n\nPLAY RECAP ********************************************************************************************************************************************\nlocalhost                  : ok=1    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n</code></pre> <p>Now that you have some devices, you can start to do a little bit more with your NetBox environment. This playbook purposely did not add more information about the device yet, such as the serial number, interfaces, or IP addressing. This is all information that you can add more about the device as well using Ansible Facts and Resource Modules to continue to develop your source of truth. More likely to come in the future, or you can check out the content on GitHub and YouTube referenced above for immediate reference.</p>"},{"location":"netbox-ansible-devices/#summary","title":"Summary","text":"<p>Getting devices into NetBox provides a powerful place to put your source of truth for automation. It does take a small bit to get to a good place, but with a little bit of effort up front you can get things done in a consistent and repeatable fashion. No more having to do things by hand with the data points.  </p> <p>Hope this has helped. If so, let me know with a comment below or give a thumbs up on the post.</p>"},{"location":"collection_install/","title":"Nautobot Ansible Collection: Installation","text":"<p>This is the first post as I shift into taking a closer look at the Nautobot Ansible Collection. The collection includes many of the needed modules to effectively manage your Nautobot environment. If  This will take a deeper dive into several of the components of the inventory plugin, but not all of the options. The documentation for all of the collection can be found at:</p> <ul> <li>ReadTheDocs: https://nautobot-ansible.readthedocs.io</li> <li>Galaxy Page: https://galaxy.ansible.com/networktocode/nautobot </li> </ul> <p>This post is going to give information on how to install the collection as it may be applicable to every post in the series (as they get posted).  </p> <p>If you were a user of the NetBox Ansible Collection previously, you will notice a few differences. The first big difference in the modules is that there is no preface of nautobot_ before each module. Since this Collection is developed after Ansible 2.10 they are using the FQCN (Fully Qualified Collection Name), there is no longer the need to prefix the name to the module name. So where there was a netbox_device before it will now be just device, underneath the FQCN of <code>networktocode.nautobot.device</code> as an example.  </p>"},{"location":"collection_install/#installation","title":"Installation","text":"<p>Installation is done via Ansible Galaxy. It is recommended to have the latest version of the collection when working on it as there are updates happening routinely. There is a Python requirement with many the modules of the pynautobot Python package.  </p> <p>It does not matter which order you install these in, you just need to install both before you start using the module.</p>"},{"location":"collection_install/#installation-pynautobot","title":"Installation - pynautobot","text":"<p>To install you execute the following to get the latest version of pynautobot:</p> <pre><code>pip install pynautobot --upgrade\n</code></pre>"},{"location":"collection_install/#installation-nautobot-collection","title":"Installation - Nautobot Collection","text":"<p>The collection is installed via Ansible Galaxy as a primary method to install. You can also install the collection manually from GitHub, but the galaxy method is the preferred method.</p> <pre><code>ansible-galaxy collection install networktocode.nautobot\n</code></pre> <p>If you add on <code>--force</code> at the end, Ansible Galaxy will install the latest version on top of what you may already have. If you already have a version of the collection installed, Galaxy will not overwrite what you already have.</p>"},{"location":"collection_install/#verification-of-installation","title":"Verification of Installation","text":"<p>Once you have run the steps there are many ways to verify that the installation is completed successfully for the Python package. The one that I like to use is to execute a <code>pip freeze | grep &lt;package_name&gt;</code>. The execution looks like this on the current date:</p> <pre><code>pip3 freeze | grep pynautobot    \npynautobot==1.0.1\n</code></pre> <p>To verify that you have installed the Nautobot Ansible Collection, you can execute the Ansible Doc command to get the current documentation. This is done as followed with the device module to verify that the docs load:</p> <pre><code>ansible-doc networktocode.nautobot.device\n</code></pre> <p>If the module is not installed properly you will see, with a key in on the first line</p> <pre><code>$ ansible-doc networktocode.nautobot.device\n[WARNING]: module networktocode.nautobot.device not found in:\n/root/.ansible/plugins/modules:/usr/share/ansible/plugins/modules:/usr/local/lib/python3.7/site-packages/ansible/modules\n</code></pre> <p>When the collection is installed properly you will see the following output with the command:</p> <pre><code>&gt; NETWORKTOCODE.NAUTOBOT.DEVICE    (/root/.ansible/collections/ansible_collections/networktocode/nautobot/plugins/modules/device.py)\n\n        Creates, updates or removes devices from Nautobot\n\nOPTIONS (= is mandatory):\n\n= data\n        Defines the device configuration\n\n        type: dict\n\n        SUBOPTIONS:\n\n        - asset_tag\n            Asset tag that is associated to the device\n            [Default: (null)]\n            type: str\n\n        - cluster\n            Cluster that the device will be assigned to\n            [Default: (null)]\n            type: raw\n\n        - comments\n            Comments that may include additional information in regards to the device\n            [Default: (null)]\n            type: str\n\n        - custom_fields\n            must exist in Nautobot\n            [Default: (null)]\n            type: dict\n\n        - device_role\n            Required if `state=present' and the device does not exist yet\n            [Default: (null)]\n            type: raw\n\n        - device_type\n            Required if `state=present' and the device does not exist yet\n            [Default: (null)]\n            type: raw\n\n        - face\n            Required if `rack' is defined\n            (Choices: Front, front, Rear, rear)[Default: (null)]\n            type: str\n\n        - local_context_data\n            Arbitrary JSON data to define the devices configuration variables.\n            [Default: (null)]\n            type: dict\n\n        = name\n            The name of the device\n\n            type: str\n\n        - platform\n            The platform of the device\n            [Default: (null)]\n            type: raw\n\n        - position\n            The position of the device in the rack defined above\n            [Default: (null)]\n            type: int\n\n        - primary_ip4\n            Primary IPv4 address assigned to the device\n            [Default: (null)]\n            type: raw\n\n        - primary_ip6\n            Primary IPv6 address assigned to the device\n            [Default: (null)]\n            type: raw\n\n        - rack\n            The name of the rack to assign the device to\n            [Default: (null)]\n            type: raw\n\n        - serial\n            Serial number of the device\n            [Default: (null)]\n            type: str\n\n        - site\n            Required if `state=present' and the device does not exist yet\n            [Default: (null)]\n            type: raw\n\n        - status\n            The status of the device\n            [Default: (null)]\n            type: raw\n\n        - tags\n            Any tags that the device may need to be associated with\n            [Default: (null)]\n            type: list\n\n        - tenant\n            The tenant that the device will be assigned to\n            [Default: (null)]\n            type: raw\n\n        - vc_position\n            Position in the assigned virtual chassis\n            [Default: (null)]\n            type: int\n\n        - vc_priority\n            Priority in the assigned virtual chassis\n            [Default: (null)]\n            type: int\n\n        - virtual_chassis\n            Virtual chassis the device will be assigned to\n            [Default: (null)]\n            type: raw\n\n- query_params\n        This can be used to override the specified values in ALLOWED_QUERY_PARAMS that is defined\n        in plugins/module_utils/utils.py and provides control to users on what may make\n        an object unique in their environment.\n        [Default: (null)]\n        elements: str\n        type: list\n\n- state\n        Use `present' or `absent' for adding or removing.\n        (Choices: absent, present)[Default: present]\n        type: str\n\n= token\n        The token created within Nautobot to authorize API access\n\n        type: str\n\n= url\n        URL of the Nautobot instance resolvable by Ansible control host\n\n        type: str\n\n- validate_certs\n        If `no', SSL certificates will not be validated. This should only be used on personally\n        controlled sites using self-signed certificates.\n        [Default: True]\n        type: raw\n\n\nNOTES:\n      * Tags should be defined as a YAML list\n      * This should be ran with connection `local' and hosts `localhost'\n\n\nREQUIREMENTS:  pynautobot\n\nAUTHOR: Network to Code (@networktocode), David Gomez (@amb1s1)\n\nMETADATA:\n  metadata_version: '1.1'\n  status:\n  - preview\n  supported_by: community\n\n\nVERSION_ADDED_COLLECTION: networktocode.nautobot\n\nEXAMPLES:\n\n- name: \"Test Nautobot modules\"\n  connection: local\n  hosts: localhost\n  gather_facts: False\n\n  tasks:\n    - name: Create device within Nautobot with only required information\n      networktocode.nautobot.device:\n        url: http://nautobot.local\n        token: thisIsMyToken\n        data:\n          name: Test Device\n          device_type: C9410R\n          device_role: Core Switch\n          site: Main\n          status: active\n        state: present\n\n    - name: Create device within Nautobot with empty string name to generate UUID\n      networktocode.nautobot.device:\n        url: http://nautobot.local\n        token: thisIsMyToken\n        data:\n          name: \"\"\n          device_type: C9410R\n          device_role: Core Switch\n          site: Main\n          status: active\n        state: present\n\n    - name: Delete device within nautobot\n      networktocode.nautobot.device:\n        url: http://nautobot.local\n        token: thisIsMyToken\n        data:\n          name: Test Device\n        state: absent\n\n    - name: Create device with tags\n      networktocode.nautobot.device:\n        url: http://nautobot.local\n        token: thisIsMyToken\n        data:\n          name: Another Test Device\n          device_type: C9410R\n          device_role: Core Switch\n          site: Main\n          status: active\n          local_context_data:\n            bgp: \"65000\"\n          tags:\n            - Schnozzberry\n        state: present\n\n    - name: Update the rack and position of an existing device\n      networktocode.nautobot.device:\n        url: http://nautobot.local\n        token: thisIsMyToken\n        data:\n          name: Test Device\n          rack: Test Rack\n          position: 10\n          face: Front\n        state: present\n\n\nRETURN VALUES:\n- device\n        Serialized object as created or already existent within Nautobot\n\n        returned: success (when `state=present')\n        type: dict\n\n- msg\n        Message indicating failure or info about what has been achieved\n\n        returned: always\n        type: str\n</code></pre>"},{"location":"collection_install/#summary","title":"Summary","text":"<p>Overall the process for getting going with this collection is two steps, of installing the Python dependency and installing the collection via Ansible Galaxy. With these done, you are on your way to using the Nautobot Ansible Collection in your environment.</p>"},{"location":"collection_install/#up-next","title":"Up Next","text":"<ul> <li>Ansible Inventory with Nautobot collection</li> </ul>"},{"location":"nautobot-ansible-sites/","title":"Nautobot Ansible Collection: Site Module","text":"<p>This post dives into the Nautobot Ansible Content Collection sites module to create/update a Site. This series for the beginning will be a clone of what I had done previously with NetBox. So some of the language will be very similar. </p> <p>When it comes to creating and deleting sites in Nautobot, the question of should I be using Ansible to do this? In my opinion this is a yes it should be. Most likely an IT tool is not the tool that will be the Source of Truth as it comes to physical sites involved in an organization. So this module in particular that should be looked at and put into production use with Ansible.</p>"},{"location":"nautobot-ansible-sites/#environment","title":"Environment","text":"<p>For this demo, here are the versions shown:</p> Component Version Nautobot v1.0.0b2 Nautobot Ansible Collection v1.0.2 pynautobot 1.0.0"},{"location":"nautobot-ansible-sites/#site-module","title":"Site Module","text":"<p>Within Nautobot, the site is the most basic unit, and is required for devices to be added. This is the first thing that you should do when creating a Nautobot instance is to start to build out sites. There are a many set of parameters that you can add to your sites, but the minimum required are:</p> <ul> <li>name: The name of the site</li> </ul> <p>Take a look at the documentation for all of the additional parameters. The ones that stick out to me (and there are many more) include:</p> <ul> <li>asn: The BGP AS Number</li> <li>contact name &amp; email: Site contact information</li> <li>physical and shipping addresses</li> <li>tags</li> <li>time_zone</li> </ul>"},{"location":"nautobot-ansible-sites/#examples","title":"Examples","text":"<p>In this getting started demos I will be showing how things look with demo data maintained within YAML. In reality this is data that would be sourced from some other location, hopefully able to be gathered via an API. The following data will be added to the environment:</p> <pre><code>---\nsites:\n  - name: MSP\n    time_zone: America/Chicago\n    status: active\n    description: Minneapolis\n  - name: DEN\n    time_zone: America/Denver\n    status: active\n    description: Denver\n  - name: NYC\n    time_zone: America/New_York\n    status: active\n    description: New York\n  - name: PDX\n    time_zone: America/Los_Angeles\n    status: active\n    description: Portland\n  - name: GRB\n    time_zone: America/Chicago\n    status: active\n    description: Green Bay\n  - name: MCO\n    time_zone: America/New_York\n    status: active\n    description: Orlando\n</code></pre> <p>Running the following playbook multiple times will show that the module itself is idempotent in that it will not keep creating sites.</p> <p>You will notice one difference with the module compared to the NetBox collections. Instead of specifying <code>nautobot</code> everywhere, one just needs to put what it is that is being referenced. In this example instead of <code>nautobot_site</code>, it is <code>site</code>. And instead of <code>nautobot_token</code> or <code>nautobot_url</code>, it is just <code>token</code> and <code>url</code>.</p> <pre><code>---\n- name: \"SETUP SITES\"\n  hosts: localhost\n  connection: local\n  gather_facts: no\n  tasks:\n    - name: \"10 - SETUP SITES\"\n      networktocode.nautobot.site:\n        url: \"{{ lookup('env', 'NAUTOBOT_URL') }}\"\n        token: \"{{ lookup('env', 'NAUTOBOT_TOKEN') }}\"\n        data: \"{{ site }}\"\n        state: present\n        validate_certs: False\n      loop: \"{{ sites }}\"\n      loop_control:\n        loop_var: site\n        label: \"{{ site['name'] }}\"\n\n    - name: \"20 - REMOVE CLOSED SITES\"\n      when: \"closed_sites is defined\"\n      networktocode.nautobot.site:\n        url: \"{{ lookup('env', 'NAUTOBOT_URL') }}\"\n        token: \"{{ lookup('env', 'NAUTOBOT_TOKEN') }}\"\n        data: \"{{ site }}\"\n        state: absent\n        validate_certs: False\n      loop: \"{{ closed_sites }}\"\n      loop_control:\n        loop_var: site\n        label: \"{{ site['name'] }}\"\n</code></pre> <p>On the first run from an empty Nautobot host, there are changes for each of the sites:</p> <pre><code>josh-v@a6339c74e30d:~$ ansible-playbook add_sites.yml -vv\nansible-playbook 2.10.6\n  config file = /local/ansible.cfg\n  configured module search path = ['/local/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules']\n  ansible python module location = /usr/local/lib/python3.7/site-packages/ansible\n  executable location = /usr/local/bin/ansible-playbook\n  python version = 3.7.10 (default, Feb 16 2021, 19:28:34) [GCC 8.3.0]\nUsing /local/ansible.cfg as config file\n[WARNING]: No inventory was parsed, only implicit localhost is available\n[WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all'\nredirecting (type: callback) ansible.builtin.yaml to community.general.yaml\nredirecting (type: callback) ansible.builtin.yaml to community.general.yaml\nSkipping callback 'default', as we already have a stdout callback.\nSkipping callback 'minimal', as we already have a stdout callback.\nSkipping callback 'oneline', as we already have a stdout callback.\n\nPLAYBOOK: add_sites.yml ****************************************************************************************************\n1 plays in add_sites.yml\n\nPLAY [SETUP SITES] *********************************************************************************************************\nMETA: ran handlers\n\nTASK [10 - SETUP SITES] ****************************************************************************************************\ntask path: /local/add_sites.yml:7\nchanged: [localhost] =&gt; (item=MSP) =&gt; changed=true \n  ansible_loop_var: site\n  msg: site MSP created\n  site:\n    description: Minneapolis\n    name: MSP\n    status: active\n    time_zone: America/Chicago\nchanged: [localhost] =&gt; (item=DEN) =&gt; changed=true \n  ansible_loop_var: site\n  msg: site DEN created\n  site:\n    description: Denver\n    name: DEN\n    status: active\n    time_zone: America/Denver\nchanged: [localhost] =&gt; (item=NYC) =&gt; changed=true \n  ansible_loop_var: site\n  msg: site NYC created\n  site:\n    description: New York\n    name: NYC\n    status: active\n    time_zone: America/New_York\nchanged: [localhost] =&gt; (item=PDX) =&gt; changed=true \n  ansible_loop_var: site\n  msg: site PDX created\n  site:\n    description: Portland\n    name: PDX\n    status: active\n    time_zone: America/Los_Angeles\nchanged: [localhost] =&gt; (item=GRB) =&gt; changed=true \n  ansible_loop_var: site\n  msg: site GRB created\n  site:\n    description: Green Bay\n    name: GRB\n    status: active\n    time_zone: America/Chicago\nchanged: [localhost] =&gt; (item=MCO) =&gt; changed=true \n  ansible_loop_var: site\n  msg: site MCO created\n  site:\n    description: Orlando\n    name: MCO\n    status: active\n    time_zone: America/New_York\n\nTASK [20 - REMOVE CLOSED SITES] ********************************************************************************************\ntask path: /local/add_sites.yml:19\nskipping: [localhost] =&gt; changed=false \n  skip_reason: Conditional result was False\nMETA: ran handlers\nMETA: ran handlers\n\nPLAY RECAP *****************************************************************************************************************\nlocalhost                  : ok=1    changed=1    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0   \n</code></pre> <p>The output below is from a second run and shows that all of the sites already exist and that the idempotency is working well.</p> <pre><code>josh-v@a6339c74e30d:~$ ansible-playbook add_sites.yml -vv\nansible-playbook 2.10.6\n  config file = /local/ansible.cfg\n  configured module search path = ['/local/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules']\n  ansible python module location = /usr/local/lib/python3.7/site-packages/ansible\n  executable location = /usr/local/bin/ansible-playbook\n  python version = 3.7.10 (default, Feb 16 2021, 19:28:34) [GCC 8.3.0]\nUsing /local/ansible.cfg as config file\n[WARNING]: No inventory was parsed, only implicit localhost is available\n[WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all'\nredirecting (type: callback) ansible.builtin.yaml to community.general.yaml\nredirecting (type: callback) ansible.builtin.yaml to community.general.yaml\nSkipping callback 'default', as we already have a stdout callback.\nSkipping callback 'minimal', as we already have a stdout callback.\nSkipping callback 'oneline', as we already have a stdout callback.\n\nPLAYBOOK: add_sites.yml ****************************************************************************************************\n1 plays in add_sites.yml\n\nPLAY [SETUP SITES] *********************************************************************************************************\nMETA: ran handlers\n\nTASK [10 - SETUP SITES] ****************************************************************************************************\ntask path: /local/add_sites.yml:7\nok: [localhost] =&gt; (item=MSP) =&gt; changed=false \n  ansible_loop_var: site\n  msg: site MSP already exists\n  site:\n    description: Minneapolis\n    name: MSP\n    status: active\n    time_zone: America/Chicago\nok: [localhost] =&gt; (item=DEN) =&gt; changed=false \n  ansible_loop_var: site\n  msg: site DEN already exists\n  site:\n    description: Denver\n    name: DEN\n    status: active\n    time_zone: America/Denver\nok: [localhost] =&gt; (item=NYC) =&gt; changed=false \n  ansible_loop_var: site\n  msg: site NYC already exists\n  site:\n    description: New York\n    name: NYC\n    status: active\n    time_zone: America/New_York\nok: [localhost] =&gt; (item=PDX) =&gt; changed=false \n  ansible_loop_var: site\n  msg: site PDX already exists\n  site:\n    description: Portland\n    name: PDX\n    status: active\n    time_zone: America/Los_Angeles\nok: [localhost] =&gt; (item=GRB) =&gt; changed=false \n  ansible_loop_var: site\n  msg: site GRB already exists\n  site:\n    description: Green Bay\n    name: GRB\n    status: active\n    time_zone: America/Chicago\nok: [localhost] =&gt; (item=MCO) =&gt; changed=false \n  ansible_loop_var: site\n  msg: site MCO already exists\n  site:\n    description: Orlando\n    name: MCO\n    status: active\n    time_zone: America/New_York\n\nTASK [20 - REMOVE CLOSED SITES] ********************************************************************************************\ntask path: /local/add_sites.yml:19\nskipping: [localhost] =&gt; changed=false \n  skip_reason: Conditional result was False\nMETA: ran handlers\nMETA: ran handlers\n\nPLAY RECAP *****************************************************************************************************************\nlocalhost                  : ok=1    changed=0    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0   \n</code></pre> <p>In here we see that there was the single task showing ok and no tasks in the other sections of the play recap. The Nautobot sites page looks like:</p> <p></p>"},{"location":"nautobot-ansible-sites/#removing-a-site","title":"Removing a site","text":"<p>Now let's take a look of using the absent. I'm going to get the data by putting MCO into the closed sites key. A full production instance of this would need to first lookup the data within Nautobot, then determine which sites are open and closed. That is beyond the scope of the purpose of this blog, which is to show the executions. That is a little more logic that is very easily done in Ansible either with Ansible native tasks or from an Ansible filter or action plugin.  </p> <p>The data looks like the following:</p> <pre><code>---\nsites:\n  - name: MSP\n    time_zone: America/Chicago\n    status: active\n    description: Minneapolis\n  - name: DEN\n    time_zone: America/Denver\n    status: active\n    description: Denver\n  - name: NYC\n    time_zone: America/New_York\n    status: active\n    description: New York\n  - name: PDX\n    time_zone: America/Los_Angeles\n    status: active\n    description: Portland\n  - name: GRB\n    time_zone: America/Chicago\n    status: active\n    description: Green Bay\nclosed_sites:\n  - name: MCO\n    time_zone: America/New_York\n    status: active\n    description: Orlando\n</code></pre> <p>With a closed site, the next playbook run will then remove the site MCO and have the appropriate setup in place.</p> <pre><code>josh-v@a6339c74e30d:~$ ansible-playbook add_sites.yml -vv\nansible-playbook 2.10.6\n  config file = /local/ansible.cfg\n  configured module search path = ['/local/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules']\n  ansible python module location = /usr/local/lib/python3.7/site-packages/ansible\n  executable location = /usr/local/bin/ansible-playbook\n  python version = 3.7.10 (default, Feb 16 2021, 19:28:34) [GCC 8.3.0]\nUsing /local/ansible.cfg as config file\n[WARNING]: No inventory was parsed, only implicit localhost is available\n[WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all'\nredirecting (type: callback) ansible.builtin.yaml to community.general.yaml\nredirecting (type: callback) ansible.builtin.yaml to community.general.yaml\nSkipping callback 'default', as we already have a stdout callback.\nSkipping callback 'minimal', as we already have a stdout callback.\nSkipping callback 'oneline', as we already have a stdout callback.\n\nPLAYBOOK: add_sites.yml ****************************************************************************************************\n1 plays in add_sites.yml\n\nPLAY [SETUP SITES] *********************************************************************************************************\nMETA: ran handlers\n\nTASK [10 - SETUP SITES] ****************************************************************************************************\ntask path: /local/add_sites.yml:7\nok: [localhost] =&gt; (item=MSP) =&gt; changed=false \n  ansible_loop_var: site\n  msg: site MSP already exists\n  site:\n    description: Minneapolis\n    name: MSP\n    status: active\n    time_zone: America/Chicago\nok: [localhost] =&gt; (item=DEN) =&gt; changed=false \n  ansible_loop_var: site\n  msg: site DEN already exists\n  site:\n    description: Denver\n    name: DEN\n    status: active\n    time_zone: America/Denver\nok: [localhost] =&gt; (item=NYC) =&gt; changed=false \n  ansible_loop_var: site\n  msg: site NYC already exists\n  site:\n    description: New York\n    name: NYC\n    status: active\n    time_zone: America/New_York\nok: [localhost] =&gt; (item=PDX) =&gt; changed=false \n  ansible_loop_var: site\n  msg: site PDX already exists\n  site:\n    description: Portland\n    name: PDX\n    status: active\n    time_zone: America/Los_Angeles\nok: [localhost] =&gt; (item=GRB) =&gt; changed=false \n  ansible_loop_var: site\n  msg: site GRB already exists\n  site:\n    description: Green Bay\n    name: GRB\n    status: active\n    time_zone: America/Chicago\n\nTASK [20 - REMOVE CLOSED SITES] ********************************************************************************************\ntask path: /local/add_sites.yml:19\nchanged: [localhost] =&gt; (item=MCO) =&gt; changed=true \n  ansible_loop_var: site\n  msg: site MCO deleted\n  site:\n    description: Orlando\n    name: MCO\n    status: active\n    time_zone: America/New_York\nMETA: ran handlers\nMETA: ran handlers\n\nPLAY RECAP *****************************************************************************************************************\nlocalhost                  : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n</code></pre> <p>Note that TASK 20 now is not skipped. This runs and removes the site with a message that the site was deleted. This is once again idempotent, with the capability to run time and again without causing any changes unless changes are required.</p>"},{"location":"nautobot-ansible-sites/#summary","title":"Summary","text":"<p>This module is a very good module with a lot of options to get you started. This is absolutely a module that I would become familiar with as your organization is changing over time. This will allow you to keep your Nautobot environment up to date with the site changes as you get new and closed sites alike. Hopefully this has been helpful to demonstrate it's capabilities. Let me know your comments below, or give it a thumbs up if you have found this helpful.  </p> <p>Let me know what you think! Comment below or you can find me on Twitter https://twitter.com/vanderaaj/.</p> <p>Thanks,</p> <p>Josh</p>"},{"location":"nautobot-ansible-device-roles/","title":"Nautobot Ansible Collection: Device Roles","text":"<p>A device role is aptly named, the role of the device. This is likely to be something that is meaningful to your organization and could change. For example you may have the 3 tier system of Core, Distribution, and Access layer environments. These are just fine. So you would want to have the roles there to reflect this reality. You may have leaf-spine environments, there are two more roles. And in my past I have also had roles that would indicate that there are dedicated DMZ, WAN edge, Internet edge devices. So this is the place to set this.</p>"},{"location":"nautobot-ansible-device-roles/#module-documentation","title":"Module Documentation","text":"<ul> <li>Read the Docs</li> <li>GitHub</li> </ul> <p>This module does require pynautobot to execute properly</p>"},{"location":"nautobot-ansible-device-roles/#environment","title":"Environment","text":"<p>For this demo, here are the versions shown:</p> Component Version Nautobot v1.0.0b2 Nautobot Ansible Collection v1.0.3 pynautobot 1.0.0"},{"location":"nautobot-ansible-device-roles/#data-file","title":"Data File","text":"<p>The roles are going to be a little more straight forward. We will only set the name, color, and if the role can be a VM or not, from the vm_role key.</p> <pre><code>---\ndevice_roles:\n  - name: Firewall\n    color: \"FF0000\"\n    vm_role: true\n  - name: Leaf\n    color: \"008000\"\n    vm_role: false\n  - name: Router\n    color: \"000080\"\n    vm_role: true\n  - name: Server\n    color: \"000000\"\n    vm_role: false\n  - name: Spine\n    color: \"0000FF\"\n    vm_role: false\n  - name: Switch\n    color: \"008000\"\n    vm_role: true\n  - name: VM\n    color: \"00FFFF\"\n    vm_role: true\n</code></pre>"},{"location":"nautobot-ansible-device-roles/#example","title":"Example","text":""},{"location":"nautobot-ansible-device-roles/#example-adding-device-roles","title":"Example - Adding Device Roles","text":"<p>Running the playbook on the roles are going to be straight to the point. Before the execution Nautobot's UI shows no device roles:</p> <p></p> <pre><code>---\n- name: \"ADD DEVICE ROLES\"\n  hosts: localhost\n  connection: local\n  gather_facts: false # No gathering facts about the container execution env\n  tasks:\n    - name: \"05 - ADD DEVICE ROLES\" # Already present, showing idempotency\n      networktocode.nautobot.device_role:\n        url: \"{{ lookup('env', 'NAUTOBOT_URL') }}\"\n        token: \"{{ lookup('env', 'NAUTOBOT_TOKEN') }}\"\n        data:\n          name: \"{{ item['name'] }}\"\n          color: \"{{ item['color'] }}\"\n          vm_role: \"{{ item['vm_role'] }}\"\n      loop: \"{{ device_roles }}\"\n</code></pre>"},{"location":"nautobot-ansible-device-roles/#example-execution","title":"Example - Execution","text":"<p>This execution shows that all of the device types are added.</p> <pre><code>josh-v@a6339c74e30d:~$ ansible-playbook add_device_role.yml -vv\nansible-playbook 2.10.6\n  config file = /local/ansible.cfg\n  configured module search path = ['/local/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules']\n  ansible python module location = /usr/local/lib/python3.7/site-packages/ansible\n  executable location = /usr/local/bin/ansible-playbook\n  python version = 3.7.10 (default, Feb 16 2021, 19:28:34) [GCC 8.3.0]\nUsing /local/ansible.cfg as config file\n[WARNING]: No inventory was parsed, only implicit localhost is available\n[WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all'\nredirecting (type: callback) ansible.builtin.yaml to community.general.yaml\nredirecting (type: callback) ansible.builtin.yaml to community.general.yaml\nSkipping callback 'default', as we already have a stdout callback.\nSkipping callback 'minimal', as we already have a stdout callback.\nSkipping callback 'oneline', as we already have a stdout callback.\n\nPLAYBOOK: add_device_role.yml **********************************************************************************************\n1 plays in add_device_role.yml\n\nPLAY [ADD DEVICE ROLES] ****************************************************************************************************\nMETA: ran handlers\n\nTASK [05 - ADD DEVICE ROLES] ***********************************************************************************************\ntask path: /local/add_device_role.yml:7\nchanged: [localhost] =&gt; (item={'name': 'Firewall', 'color': 'FF0000', 'vm_role': True}) =&gt; changed=true \n  ansible_loop_var: item\n  device_role:\n    color: ff0000\n    created: '2021-03-14'\n    custom_fields: {}\n    description: ''\n    id: 252bd7ba-3b15-4651-a7e3-e43cdd85227c\n    last_updated: '2021-03-14T18:50:53.994175Z'\n    name: Firewall\n    slug: firewall\n    url: http://nautobot-demo.josh-v.com/api/dcim/device-roles/252bd7ba-3b15-4651-a7e3-e43cdd85227c/\n    vm_role: true\n  item:\n    color: FF0000\n    name: Firewall\n    vm_role: true\n  msg: device_role Firewall created\nchanged: [localhost] =&gt; (item={'name': 'Leaf', 'color': '008000', 'vm_role': False}) =&gt; changed=true \n  ansible_loop_var: item\n  device_role:\n    color: 008000\n    created: '2021-03-14'\n    custom_fields: {}\n    description: ''\n    id: 35f176c4-9b20-4e3c-b961-47c624afaa56\n    last_updated: '2021-03-14T18:50:54.942372Z'\n    name: Leaf\n    slug: leaf\n    url: http://nautobot-demo.josh-v.com/api/dcim/device-roles/35f176c4-9b20-4e3c-b961-47c624afaa56/\n    vm_role: false\n  item:\n    color: 008000\n    name: Leaf\n    vm_role: false\n  msg: device_role Leaf created\nchanged: [localhost] =&gt; (item={'name': 'Router', 'color': '000080', 'vm_role': True}) =&gt; changed=true \n  ansible_loop_var: item\n  device_role:\n    color: 000080\n    created: '2021-03-14'\n    custom_fields: {}\n    description: ''\n    id: c6909cfd-0fd9-4ab1-b0e7-58493fff84b7\n    last_updated: '2021-03-14T18:50:55.901588Z'\n    name: Router\n    slug: router\n    url: http://nautobot-demo.josh-v.com/api/dcim/device-roles/c6909cfd-0fd9-4ab1-b0e7-58493fff84b7/\n    vm_role: true\n  item:\n    color: 000080\n    name: Router\n    vm_role: true\n  msg: device_role Router created\nchanged: [localhost] =&gt; (item={'name': 'Server', 'color': '000000', 'vm_role': False}) =&gt; changed=true \n  ansible_loop_var: item\n  device_role:\n    color: '000000'\n    created: '2021-03-14'\n    custom_fields: {}\n    description: ''\n    id: f9acf678-7b71-4cf9-88f8-4e3ad3f499cb\n    last_updated: '2021-03-14T18:50:57.004599Z'\n    name: Server\n    slug: server\n    url: http://nautobot-demo.josh-v.com/api/dcim/device-roles/f9acf678-7b71-4cf9-88f8-4e3ad3f499cb/\n    vm_role: false\n  item:\n    color: '000000'\n    name: Server\n    vm_role: false\n  msg: device_role Server created\nchanged: [localhost] =&gt; (item={'name': 'Spine', 'color': '0000FF', 'vm_role': False}) =&gt; changed=true \n  ansible_loop_var: item\n  device_role:\n    color: 0000ff\n    created: '2021-03-14'\n    custom_fields: {}\n    description: ''\n    id: 4121b0bf-8085-424e-bf0d-b11855cd9c04\n    last_updated: '2021-03-14T18:50:57.912158Z'\n    name: Spine\n    slug: spine\n    url: http://nautobot-demo.josh-v.com/api/dcim/device-roles/4121b0bf-8085-424e-bf0d-b11855cd9c04/\n    vm_role: false\n  item:\n    color: 0000FF\n    name: Spine\n    vm_role: false\n  msg: device_role Spine created\nchanged: [localhost] =&gt; (item={'name': 'Switch', 'color': '008000', 'vm_role': True}) =&gt; changed=true \n  ansible_loop_var: item\n  device_role:\n    color: 008000\n    created: '2021-03-14'\n    custom_fields: {}\n    description: ''\n    id: c99fe14b-5172-446f-8bd5-78c1e84aaa1c\n    last_updated: '2021-03-14T18:50:58.743836Z'\n    name: Switch\n    slug: switch\n    url: http://nautobot-demo.josh-v.com/api/dcim/device-roles/c99fe14b-5172-446f-8bd5-78c1e84aaa1c/\n    vm_role: true\n  item:\n    color: 008000\n    name: Switch\n    vm_role: true\n  msg: device_role Switch created\nchanged: [localhost] =&gt; (item={'name': 'VM', 'color': '00FFFF', 'vm_role': True}) =&gt; changed=true \n  ansible_loop_var: item\n  device_role:\n    color: 00ffff\n    created: '2021-03-14'\n    custom_fields: {}\n    description: ''\n    id: 00a2da7f-fe44-4332-bc58-391b429c97eb\n    last_updated: '2021-03-14T18:50:59.548621Z'\n    name: VM\n    slug: vm\n    url: http://nautobot-demo.josh-v.com/api/dcim/device-roles/00a2da7f-fe44-4332-bc58-391b429c97eb/\n    vm_role: true\n  item:\n    color: 00FFFF\n    name: VM\n    vm_role: true\n  msg: device_role VM created\nMETA: ran handlers\nMETA: ran handlers\n\nPLAY RECAP *****************************************************************************************************************\nlocalhost                  : ok=1    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n</code></pre> <p>After completion of this you will have the device roles are now available to be assigned to devices. Taking a look the UI now has the data:</p> <p></p>"},{"location":"nautobot-ansible-device-roles/#summary","title":"Summary","text":"<p>Device roles are a required item to add devices to Nautobot. This can be as generic as \"Device\" or \"Network Device\". However, I strongly encourage you to look at putting some thought into the roles that you will assign to devices. This will become very helpful in the future as you look at building out the automation platform. You can see in the inventory build, you can assign devices based on roles to an inventory group. This becomes particularly helpful when you want to run a playbook against a single group, such as all Leaf switches, or all Spine switches that must have a particular configuration set.  </p> <p>Hope this has helped. If so, let me know with a comment below or give a thumbs up on the post. Connect with me on Twitter @vanderaaj.</p>"},{"location":"nautobot-ansible-manufacturers/","title":"Nautobot Ansible Collection: Manufacturers","text":"<p>Adding your manufacturers via code is the easy way to get started with your Nautobot devices. Immediately after adding Sites, the next thing to get going when using Nautobot as your Source of Truth is to add in Manufacturers. These are just that, who makes the gear that you use. For this demonstration you will see adding just a few manufacturers. I'm not necessarily picking on any vendors and who should or shouldn't be here. It is just what my background brings.</p>"},{"location":"nautobot-ansible-manufacturers/#module-documentation","title":"Module Documentation","text":"<ul> <li>Read the Docs</li> <li>GitHub</li> </ul> <p>This module does require pynautobot to execute properly</p>"},{"location":"nautobot-ansible-manufacturers/#environment","title":"Environment","text":"<p>For this demo, here are the versions shown:</p> Component Version Nautobot v1.0.0b2 Nautobot Ansible Collection v1.0.3 pynautobot 1.0.0"},{"location":"nautobot-ansible-manufacturers/#data-file","title":"Data File","text":"<p>The documentation indicates that there are two parameters, name and slug. I'm not going to modify the slug in any way for these as the auto-generated slug is just fine. Because of this, the demo will not have a more complex variable, just a list of manufacturers.</p> <pre><code>---\nmanufacturers:\n  - Arista\n  - Cisco\n  - Juniper\n</code></pre>"},{"location":"nautobot-ansible-manufacturers/#example","title":"Example","text":""},{"location":"nautobot-ansible-manufacturers/#example-adding-devices","title":"Example - Adding Devices","text":"<p>Getting started I already have a Cisco manufacturer included from a different demo. This will not hurt what is being demonstrated here. The task to add a manufacturer looks like:</p> <pre><code>---\n- name: \"SETUP MANUFACTURERS\"\n  hosts: localhost\n  connection: local\n  gather_facts: false # No gathering facts about the container execution env\n  tasks:\n    - name: \"05 - ADD MANUFACTURERS\"\n      networktocode.nautobot.manufacturer:\n        url: \"{{ lookup('env', 'NAUTOBOT_URL') }}\"\n        token: \"{{ lookup('env', 'NAUTOBOT_TOKEN') }}\"\n        data:\n          name: \"{{ item }}\"\n      loop: \"{{ manufacturers }}\"\n</code></pre> <p>Here is the before:</p> <p></p>"},{"location":"nautobot-ansible-manufacturers/#example-execution","title":"Example - Execution","text":"<p>Pretty short and sweet on this playbook. With having <code>Cisco</code> already present, you can see that the module is idempotent:</p> <pre><code>josh-v@a6339c74e30d:~$ ansible-playbook add_manufacturers.yml -vv\nansible-playbook 2.10.6\n  config file = /local/ansible.cfg\n  configured module search path = ['/local/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules']\n  ansible python module location = /usr/local/lib/python3.7/site-packages/ansible\n  executable location = /usr/local/bin/ansible-playbook\n  python version = 3.7.10 (default, Feb 16 2021, 19:28:34) [GCC 8.3.0]\nUsing /local/ansible.cfg as config file\n[WARNING]: No inventory was parsed, only implicit localhost is available\n[WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all'\nredirecting (type: callback) ansible.builtin.yaml to community.general.yaml\nredirecting (type: callback) ansible.builtin.yaml to community.general.yaml\nSkipping callback 'default', as we already have a stdout callback.\nSkipping callback 'minimal', as we already have a stdout callback.\nSkipping callback 'oneline', as we already have a stdout callback.\n\nPLAYBOOK: add_manufacturers.yml ********************************************************************************************\n1 plays in add_manufacturers.yml\n\nPLAY [SETUP MANUFACTURERS] *************************************************************************************************\nMETA: ran handlers\n\nTASK [05 - ADD MANUFACTURERS] **********************************************************************************************\ntask path: /local/add_manufacturers.yml:7\nchanged: [localhost] =&gt; (item=Arista) =&gt; changed=true \n  ansible_loop_var: item\n  item: Arista\n  manufacturer:\n    created: '2021-03-14'\n    custom_fields: {}\n    description: ''\n    id: fdead7a3-58a6-4a62-bb52-e21bbe2c6bf7\n    last_updated: '2021-03-14T17:43:35.202049Z'\n    name: Arista\n    slug: arista\n    url: http://nautobot-demo.josh-v.com/api/dcim/manufacturers/fdead7a3-58a6-4a62-bb52-e21bbe2c6bf7/\n  msg: manufacturer Arista created\nchanged: [localhost] =&gt; (item=Cisco) =&gt; changed=true \n  ansible_loop_var: item\n  item: Cisco\n  manufacturer:\n    created: '2021-03-14'\n    custom_fields: {}\n    description: ''\n    id: 58c56ff8-f507-4356-9b6f-915be289831b\n    last_updated: '2021-03-14T17:43:36.293444Z'\n    name: Cisco\n    slug: cisco\n    url: http://nautobot-demo.josh-v.com/api/dcim/manufacturers/58c56ff8-f507-4356-9b6f-915be289831b/\n  msg: manufacturer Cisco created\nchanged: [localhost] =&gt; (item=Juniper) =&gt; changed=true \n  ansible_loop_var: item\n  item: Juniper\n  manufacturer:\n    created: '2021-03-14'\n    custom_fields: {}\n    description: ''\n    id: 3aa03612-10d9-41e6-81ad-90a0d52fe03a\n    last_updated: '2021-03-14T17:43:37.481073Z'\n    name: Juniper\n    slug: juniper\n    url: http://nautobot-demo.josh-v.com/api/dcim/manufacturers/3aa03612-10d9-41e6-81ad-90a0d52fe03a/\n  msg: manufacturer Juniper created\nMETA: ran handlers\nMETA: ran handlers\n\nPLAY RECAP *****************************************************************************************************************\nlocalhost                  : ok=1    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n</code></pre>"},{"location":"nautobot-ansible-manufacturers/#example-idempotency","title":"Example - Idempotency","text":"<p>Give this a second run and the playbook shows everything coming back as OK, without any changes.</p> <pre><code>josh-v@a6339c74e30d:~$ ansible-playbook add_manufacturers.yml -vv\nansible-playbook 2.10.6\n  config file = /local/ansible.cfg\n  configured module search path = ['/local/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules']\n  ansible python module location = /usr/local/lib/python3.7/site-packages/ansible\n  executable location = /usr/local/bin/ansible-playbook\n  python version = 3.7.10 (default, Feb 16 2021, 19:28:34) [GCC 8.3.0]\nUsing /local/ansible.cfg as config file\n[WARNING]: No inventory was parsed, only implicit localhost is available\n[WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all'\nredirecting (type: callback) ansible.builtin.yaml to community.general.yaml\nredirecting (type: callback) ansible.builtin.yaml to community.general.yaml\nSkipping callback 'default', as we already have a stdout callback.\nSkipping callback 'minimal', as we already have a stdout callback.\nSkipping callback 'oneline', as we already have a stdout callback.\n\nPLAYBOOK: add_manufacturers.yml ********************************************************************************************\n1 plays in add_manufacturers.yml\n\nPLAY [SETUP MANUFACTURERS] *************************************************************************************************\nMETA: ran handlers\n\nTASK [05 - ADD MANUFACTURERS] **********************************************************************************************\ntask path: /local/add_manufacturers.yml:7\nok: [localhost] =&gt; (item=Arista) =&gt; changed=false \n  ansible_loop_var: item\n  item: Arista\n  manufacturer:\n    created: '2021-03-14'\n    custom_fields: {}\n    description: ''\n    devicetype_count: 0\n    id: fdead7a3-58a6-4a62-bb52-e21bbe2c6bf7\n    inventoryitem_count: 0\n    last_updated: '2021-03-14T17:43:35.202049Z'\n    name: Arista\n    platform_count: 0\n    slug: arista\n    url: http://nautobot-demo.josh-v.com/api/dcim/manufacturers/fdead7a3-58a6-4a62-bb52-e21bbe2c6bf7/\n  msg: manufacturer Arista already exists\nok: [localhost] =&gt; (item=Cisco) =&gt; changed=false \n  ansible_loop_var: item\n  item: Cisco\n  manufacturer:\n    created: '2021-03-14'\n    custom_fields: {}\n    description: ''\n    devicetype_count: 0\n    id: 58c56ff8-f507-4356-9b6f-915be289831b\n    inventoryitem_count: 0\n    last_updated: '2021-03-14T17:43:36.293444Z'\n    name: Cisco\n    platform_count: 0\n    slug: cisco\n    url: http://nautobot-demo.josh-v.com/api/dcim/manufacturers/58c56ff8-f507-4356-9b6f-915be289831b/\n  msg: manufacturer Cisco already exists\nok: [localhost] =&gt; (item=Juniper) =&gt; changed=false \n  ansible_loop_var: item\n  item: Juniper\n  manufacturer:\n    created: '2021-03-14'\n    custom_fields: {}\n    description: ''\n    devicetype_count: 0\n    id: 3aa03612-10d9-41e6-81ad-90a0d52fe03a\n    inventoryitem_count: 0\n    last_updated: '2021-03-14T17:43:37.481073Z'\n    name: Juniper\n    platform_count: 0\n    slug: juniper\n    url: http://nautobot-demo.josh-v.com/api/dcim/manufacturers/3aa03612-10d9-41e6-81ad-90a0d52fe03a/\n  msg: manufacturer Juniper already exists\nMETA: ran handlers\nMETA: ran handlers\n\nPLAY RECAP *****************************************************************************************************************\nlocalhost                  : ok=1    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n</code></pre> <p>Now you see all of them showing up in Nautobot.</p> <p></p>"},{"location":"nautobot-ansible-manufacturers/#summary","title":"Summary","text":"<p>The manufacturers are a base to adding devices. To add/sync manufacturers you can leverage the Nautobot Ansible Collection for manufacturers to get this data synced. To have your entire environment completely automated with using Ansible, this is a great solution.</p>"},{"location":"nautobot-ansible-platforms/","title":"Nautobot Ansible Collection: Platforms","text":"<p>Platforms are an optional item when adding devices into Nautobot. The platform is the OS that you are going to be using. Most often this is used to help identify which driver your automation platform is going to be using. Specifically the slug of the platform is what needs to match. So in the terms of Ansible (since we are using Ansible to populate Nautobot), you will want to set Cisco IOS devices to ios. By having the slug match the automation platform name you have that information in your inventory. For these reasons I strongly recommend setting the Platform for devices.</p>"},{"location":"nautobot-ansible-platforms/#module-documentation","title":"Module Documentation","text":"<ul> <li>Read the Docs</li> <li>GitHub</li> </ul> <p>This module does require pynautobot to execute properly</p>"},{"location":"nautobot-ansible-platforms/#environment","title":"Environment","text":"<p>For this demo, here are the versions shown:</p> Component Version Nautobot v1.0.0b2 Nautobot Ansible Collection v1.0.3 pynautobot 1.0.0"},{"location":"nautobot-ansible-platforms/#data-file","title":"Data File","text":"<p>Now that you may want to have a different slug than what is displayed, the data structure is getting slightly more complex than the manufacturers file. There will be a list of dictionaries, where the dictionary has three keys: name, slug, and manufacturer. Because in this series the platform is going to be tied to the manufacturer, the manufacturer is the next item in the list to get added after the site.</p> <pre><code>---\nplatforms:\n  - name: Arista EOS\n    slug: eos\n    manufacturer: Arista\n  - name: Cisco IOS\n    slug: ios\n    manufacturer: Cisco\n  - name: JUNOS\n    slug: junos\n    manufacturer: Juniper\n</code></pre>"},{"location":"nautobot-ansible-platforms/#example","title":"Example","text":""},{"location":"nautobot-ansible-platforms/#example-adding-platform","title":"Example - Adding Platform","text":"<p>Getting started I already have a Cisco manufacturer included from a different demo. This will not hurt what is being demonstrated here. The task to add a manufacturer looks like:</p> <pre><code>---\n- name: \"ADD PLATFORMS\"\n  hosts: localhost\n  connection: local\n  gather_facts: false # No gathering facts about the container execution env\n  tasks:\n    - name: \"05 - ADD PLATFORMS\"\n      networktocode.nautobot.platform:\n        url: \"{{ lookup('env', 'NAUTOBOT_URL') }}\"\n        token: \"{{ lookup('env', 'NAUTOBOT_TOKEN') }}\"\n        data:\n          name: \"{{ item['name'] }}\"\n          slug: \"{{ item['slug'] }}\"\n          manufacturer: \"{{ item['manufacturer'] }}\"\n      loop: \"{{ platforms }}\"\n</code></pre> <p>Before the execution there are no platforms showing in Nautobot.</p> <p></p>"},{"location":"nautobot-ansible-platforms/#example-execution","title":"Example - Execution","text":"<p>This execution shows that all of the platforms are added.</p> <pre><code>josh-v@a6339c74e30d:~$ ansible-playbook add_platforms.yml -vv\nansible-playbook 2.10.6\n  config file = /local/ansible.cfg\n  configured module search path = ['/local/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules']\n  ansible python module location = /usr/local/lib/python3.7/site-packages/ansible\n  executable location = /usr/local/bin/ansible-playbook\n  python version = 3.7.10 (default, Feb 16 2021, 19:28:34) [GCC 8.3.0]\nUsing /local/ansible.cfg as config file\n[WARNING]: No inventory was parsed, only implicit localhost is available\n[WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all'\nredirecting (type: callback) ansible.builtin.yaml to community.general.yaml\nredirecting (type: callback) ansible.builtin.yaml to community.general.yaml\nSkipping callback 'default', as we already have a stdout callback.\nSkipping callback 'minimal', as we already have a stdout callback.\nSkipping callback 'oneline', as we already have a stdout callback.\n\nPLAYBOOK: add_platforms.yml ************************************************************************************************\n1 plays in add_platforms.yml\n\nPLAY [ADD PLATFORMS] *******************************************************************************************************\nMETA: ran handlers\n\nTASK [05 - ADD PLATFORMS] **************************************************************************************************\ntask path: /local/add_platforms.yml:7\nchanged: [localhost] =&gt; (item={'name': 'Arista EOS', 'slug': 'eos', 'manufacturer': 'Arista'}) =&gt; changed=true \n  ansible_loop_var: item\n  item:\n    manufacturer: Arista\n    name: Arista EOS\n    slug: eos\n  msg: platform Arista EOS created\n  platform:\n    created: '2021-03-14'\n    custom_fields: {}\n    description: ''\n    id: 1d6b8698-c709-49f7-921e-d4a4c85f3317\n    last_updated: '2021-03-14T18:33:18.255025Z'\n    manufacturer: fdead7a3-58a6-4a62-bb52-e21bbe2c6bf7\n    name: Arista EOS\n    napalm_args: null\n    napalm_driver: ''\n    slug: eos\n    url: http://nautobot-demo.josh-v.com/api/dcim/platforms/1d6b8698-c709-49f7-921e-d4a4c85f3317/\nchanged: [localhost] =&gt; (item={'name': 'Cisco IOS', 'slug': 'ios', 'manufacturer': 'Cisco'}) =&gt; changed=true \n  ansible_loop_var: item\n  item:\n    manufacturer: Cisco\n    name: Cisco IOS\n    slug: ios\n  msg: platform Cisco IOS created\n  platform:\n    created: '2021-03-14'\n    custom_fields: {}\n    description: ''\n    id: 96d58cc1-ad7e-43c7-a79f-db748e6eb894\n    last_updated: '2021-03-14T18:33:19.081567Z'\n    manufacturer: 58c56ff8-f507-4356-9b6f-915be289831b\n    name: Cisco IOS\n    napalm_args: null\n    napalm_driver: ''\n    slug: ios\n    url: http://nautobot-demo.josh-v.com/api/dcim/platforms/96d58cc1-ad7e-43c7-a79f-db748e6eb894/\nchanged: [localhost] =&gt; (item={'name': 'JUNOS', 'slug': 'junos', 'manufacturer': 'Juniper'}) =&gt; changed=true \n  ansible_loop_var: item\n  item:\n    manufacturer: Juniper\n    name: JUNOS\n    slug: junos\n  msg: platform JUNOS created\n  platform:\n    created: '2021-03-14'\n    custom_fields: {}\n    description: ''\n    id: a6abfcb8-3ec9-4067-9aca-e88f9fa8eb87\n    last_updated: '2021-03-14T18:33:19.884201Z'\n    manufacturer: 3aa03612-10d9-41e6-81ad-90a0d52fe03a\n    name: JUNOS\n    napalm_args: null\n    napalm_driver: ''\n    slug: junos\n    url: http://nautobot-demo.josh-v.com/api/dcim/platforms/a6abfcb8-3ec9-4067-9aca-e88f9fa8eb87/\nMETA: ran handlers\nMETA: ran handlers\n\nPLAY RECAP *****************************************************************************************************************\nlocalhost                  : ok=1    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n</code></pre> <p>Now you see all of the defined platforms showing up in Nautobot.</p> <p></p>"},{"location":"nautobot-ansible-platforms/#example-idempotency","title":"Example - Idempotency","text":"<p>Showing the idempotency of the module, on the second run there are no changes made.</p> <pre><code>josh-v@a6339c74e30d:~$ ansible-playbook add_platforms.yml -vv\nansible-playbook 2.10.6\n  config file = /local/ansible.cfg\n  configured module search path = ['/local/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules']\n  ansible python module location = /usr/local/lib/python3.7/site-packages/ansible\n  executable location = /usr/local/bin/ansible-playbook\n  python version = 3.7.10 (default, Feb 16 2021, 19:28:34) [GCC 8.3.0]\nUsing /local/ansible.cfg as config file\n[WARNING]: No inventory was parsed, only implicit localhost is available\n[WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all'\nredirecting (type: callback) ansible.builtin.yaml to community.general.yaml\nredirecting (type: callback) ansible.builtin.yaml to community.general.yaml\nSkipping callback 'default', as we already have a stdout callback.\nSkipping callback 'minimal', as we already have a stdout callback.\nSkipping callback 'oneline', as we already have a stdout callback.\n\nPLAYBOOK: add_platforms.yml ************************************************************************************************\n1 plays in add_platforms.yml\n\nPLAY [ADD PLATFORMS] *******************************************************************************************************\nMETA: ran handlers\n\nTASK [05 - ADD PLATFORMS] **************************************************************************************************\ntask path: /local/add_platforms.yml:7\nok: [localhost] =&gt; (item={'name': 'Arista EOS', 'slug': 'eos', 'manufacturer': 'Arista'}) =&gt; changed=false \n  ansible_loop_var: item\n  item:\n    manufacturer: Arista\n    name: Arista EOS\n    slug: eos\n  msg: platform Arista EOS already exists\n  platform:\n    created: '2021-03-14'\n    custom_fields: {}\n    description: ''\n    device_count: 0\n    id: 1d6b8698-c709-49f7-921e-d4a4c85f3317\n    last_updated: '2021-03-14T18:33:18.255025Z'\n    manufacturer: fdead7a3-58a6-4a62-bb52-e21bbe2c6bf7\n    name: Arista EOS\n    napalm_args: null\n    napalm_driver: ''\n    slug: eos\n    url: http://nautobot-demo.josh-v.com/api/dcim/platforms/1d6b8698-c709-49f7-921e-d4a4c85f3317/\n    virtualmachine_count: 0\nok: [localhost] =&gt; (item={'name': 'Cisco IOS', 'slug': 'ios', 'manufacturer': 'Cisco'}) =&gt; changed=false \n  ansible_loop_var: item\n  item:\n    manufacturer: Cisco\n    name: Cisco IOS\n    slug: ios\n  msg: platform Cisco IOS already exists\n  platform:\n    created: '2021-03-14'\n    custom_fields: {}\n    description: ''\n    device_count: 0\n    id: 96d58cc1-ad7e-43c7-a79f-db748e6eb894\n    last_updated: '2021-03-14T18:33:19.081567Z'\n    manufacturer: 58c56ff8-f507-4356-9b6f-915be289831b\n    name: Cisco IOS\n    napalm_args: null\n    napalm_driver: ''\n    slug: ios\n    url: http://nautobot-demo.josh-v.com/api/dcim/platforms/96d58cc1-ad7e-43c7-a79f-db748e6eb894/\n    virtualmachine_count: 0\nok: [localhost] =&gt; (item={'name': 'JUNOS', 'slug': 'junos', 'manufacturer': 'Juniper'}) =&gt; changed=false \n  ansible_loop_var: item\n  item:\n    manufacturer: Juniper\n    name: JUNOS\n    slug: junos\n  msg: platform JUNOS already exists\n  platform:\n    created: '2021-03-14'\n    custom_fields: {}\n    description: ''\n    device_count: 0\n    id: a6abfcb8-3ec9-4067-9aca-e88f9fa8eb87\n    last_updated: '2021-03-14T18:33:19.884201Z'\n    manufacturer: 3aa03612-10d9-41e6-81ad-90a0d52fe03a\n    name: JUNOS\n    napalm_args: null\n    napalm_driver: ''\n    slug: junos\n    url: http://nautobot-demo.josh-v.com/api/dcim/platforms/a6abfcb8-3ec9-4067-9aca-e88f9fa8eb87/\n    virtualmachine_count: 0\nMETA: ran handlers\nMETA: ran handlers\n\nPLAY RECAP *****************************************************************************************************************\nlocalhost                  : ok=1    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n</code></pre>"},{"location":"nautobot-ansible-platforms/#summary","title":"Summary","text":"<p>Platforms are one of the items that you will strongly want to get updated into Nautobot. By associating a device with a platform you can then use it in the inventory plugins to identify things such as the <code>ansible_network_os</code> dynamically. Need to have a new platform to test things with, just create a new platform, change a few settings, and the information is dynamically available within your playbooks.  </p> <p>Hope this has helped. If so, let me know with a comment below or give a thumbs up on the post. You can leave a comment or connect with me on Twitter as well, at @vanderaaj.  </p> <p>Thanks,</p> <p>Josh</p>"},{"location":"nautobot-ansible-device-types/","title":"Nautobot Ansible Collection: Device Types","text":"<p>A device type is the next piece in the Nautobot Device onboarding requirements. The device type corresponds to the model number of the hardware (or virtual machine). This is where you are able to template out devices during their creation. So if you have a console port on a device type, that console port will be created when you create the device. However, there is NOT a relationship built between the device type and the device. If the device type gets updated after the device is created, the device itself is not updated. </p>"},{"location":"nautobot-ansible-device-types/#module-documentation","title":"Module Documentation","text":"<ul> <li>Read the Docs</li> <li>GitHub</li> </ul> <p>This module does require pynautobot to execute properly</p>"},{"location":"nautobot-ansible-device-types/#environment","title":"Environment","text":"<p>For this demo, here are the versions shown:</p> Component Version Nautobot v1.0.0b2 Nautobot Ansible Collection v1.0.3 pynautobot 1.0.1"},{"location":"nautobot-ansible-device-types/#data-file","title":"Data File","text":"<p>This gets to be a little more of the complex data source types. There are many data parameters that are good to include. The minimum data parameter has just the model. But there are going to be many more options as you build out your Nautobot environment that feeds into the data correlation that makes Nautobot a pleasure to use. Such as the manufacturer that it is tied to, the part number, and the u_height as you build rack diagrams from Nautobot.  </p> <p>In the demo the model, manufacturer, part number, and slug will get defined. The slug will be the lower case of the model name. The primary key is the model name in this case.</p> <pre><code>---\ndevice_types:\n  - model: \"ASAv\"\n    manufacturer: \"Cisco\"\n    slug: \"asav\"\n    part_number: \"asav\"\n  - model: \"CSR1000v\"\n    manufacturer: \"Cisco\"\n    slug: \"csr1000v\"\n    part_number: \"csr1000v\"\n  - model: \"IOSv\"\n    manufacturer: \"Cisco\"\n    slug: \"iosv\"\n    part_number: \"iosv\"\n  - model: \"nxosv\"\n    manufacturer: \"Cisco\"\n    slug: \"nxosv\"\n    part_number: \"nxosv\"\n  - model: \"vEOS\"\n    manufacturer: \"Arista\"\n    slug: \"veos\"\n    part_number: \"veos\"\n</code></pre>"},{"location":"nautobot-ansible-device-types/#example","title":"Example","text":""},{"location":"nautobot-ansible-device-types/#example-adding-device-types","title":"Example - Adding Device Types","text":"<p>Getting started I already have a Cisco manufacturer included from a different demo. This will not hurt what is being demonstrated here. The task to add a manufacturer looks like:</p> <pre><code>---\n- name: \"ADD DEVICE TYPES\"\n  hosts: localhost\n  connection: local\n  gather_facts: false # No gathering facts about the container execution env\n  tasks:\n    - name: \"05 - ADD DEVICE TYPES\"\n      networktocode.nautobot.device_type:\n        url: \"{{ lookup('env', 'NAUTOBOT_URL') }}\"\n        token: \"{{ lookup('env', 'NAUTOBOT_TOKEN') }}\"\n        data:\n          model: \"{{ item['model'] }}\"\n          manufacturer: \"{{ item['manufacturer'] }}\"\n          slug: \"{{ item['slug'] }}\"\n          part_number: \"{{ item['part_number'] }}\"\n      loop: \"{{ device_types }}\"\n</code></pre>"},{"location":"nautobot-ansible-device-types/#example-execution","title":"Example - Execution","text":"<p>This execution shows that all of the device types are added. Before the execution Nautobot does not have any device types.</p> <p></p> <pre><code>josh-v@60a6498959f8:~$ ansible-playbook add_device_types.yml -vv\nansible-playbook 2.10.6\n  config file = /local/ansible.cfg\n  configured module search path = ['/local/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules']\n  ansible python module location = /usr/local/lib/python3.7/site-packages/ansible\n  executable location = /usr/local/bin/ansible-playbook\n  python version = 3.7.10 (default, Feb 16 2021, 19:28:34) [GCC 8.3.0]\nUsing /local/ansible.cfg as config file\n[WARNING]: No inventory was parsed, only implicit localhost is available\n[WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all'\nredirecting (type: callback) ansible.builtin.yaml to community.general.yaml\nredirecting (type: callback) ansible.builtin.yaml to community.general.yaml\nSkipping callback 'default', as we already have a stdout callback.\nSkipping callback 'minimal', as we already have a stdout callback.\nSkipping callback 'oneline', as we already have a stdout callback.\n\nPLAYBOOK: add_device_types.yml **************************************************************************************************************************\n1 plays in add_device_types.yml\n\nPLAY [ADD DEVICE TYPES] *********************************************************************************************************************************\nMETA: ran handlers\n\nTASK [05 - ADD DEVICE TYPES] ****************************************************************************************************************************\ntask path: /local/add_device_types.yml:7\nchanged: [localhost] =&gt; (item={'model': 'ASAv', 'manufacturer': 'Cisco', 'slug': 'asav', 'part_number': 'asav'}) =&gt; changed=true \n  ansible_loop_var: item\n  device_type:\n    comments: ''\n    created: '2021-03-16'\n    custom_fields: {}\n    display_name: Cisco ASAv\n    front_image: null\n    id: 0bdb5944-a2c2-4093-a83c-c8c69485d5ac\n    is_full_depth: true\n    last_updated: '2021-03-16T00:15:49.347705Z'\n    manufacturer: 58c56ff8-f507-4356-9b6f-915be289831b\n    model: ASAv\n    part_number: asav\n    rear_image: null\n    slug: asav\n    subdevice_role: null\n    tags: []\n    u_height: 1\n    url: http://nautobot-demo.josh-v.com/api/dcim/device-types/0bdb5944-a2c2-4093-a83c-c8c69485d5ac/\n  item:\n    manufacturer: Cisco\n    model: ASAv\n    part_number: asav\n    slug: asav\n  msg: device_type asav created\nchanged: [localhost] =&gt; (item={'model': 'CSR1000v', 'manufacturer': 'Cisco', 'slug': 'csr1000v', 'part_number': 'csr1000v'}) =&gt; changed=true \n  ansible_loop_var: item\n  device_type:\n    comments: ''\n    created: '2021-03-16'\n    custom_fields: {}\n    display_name: Cisco CSR1000v\n    front_image: null\n    id: a804d796-194a-46e2-af72-bdfbc179af92\n    is_full_depth: true\n    last_updated: '2021-03-16T00:15:50.335484Z'\n    manufacturer: 58c56ff8-f507-4356-9b6f-915be289831b\n    model: CSR1000v\n    part_number: csr1000v\n    rear_image: null\n    slug: csr1000v\n    subdevice_role: null\n    tags: []\n    u_height: 1\n    url: http://nautobot-demo.josh-v.com/api/dcim/device-types/a804d796-194a-46e2-af72-bdfbc179af92/\n  item:\n    manufacturer: Cisco\n    model: CSR1000v\n    part_number: csr1000v\n    slug: csr1000v\n  msg: device_type csr1000v created\nchanged: [localhost] =&gt; (item={'model': 'IOSv', 'manufacturer': 'Cisco', 'slug': 'iosv', 'part_number': 'iosv'}) =&gt; changed=true \n  ansible_loop_var: item\n  device_type:\n    comments: ''\n    created: '2021-03-16'\n    custom_fields: {}\n    display_name: Cisco IOSv\n    front_image: null\n    id: 70504d2c-1641-4e6d-be40-192eb1d6e0c0\n    is_full_depth: true\n    last_updated: '2021-03-16T00:15:51.095938Z'\n    manufacturer: 58c56ff8-f507-4356-9b6f-915be289831b\n    model: IOSv\n    part_number: iosv\n    rear_image: null\n    slug: iosv\n    subdevice_role: null\n    tags: []\n    u_height: 1\n    url: http://nautobot-demo.josh-v.com/api/dcim/device-types/70504d2c-1641-4e6d-be40-192eb1d6e0c0/\n  item:\n    manufacturer: Cisco\n    model: IOSv\n    part_number: iosv\n    slug: iosv\n  msg: device_type iosv created\nchanged: [localhost] =&gt; (item={'model': 'nxosv', 'manufacturer': 'Cisco', 'slug': 'nxosv', 'part_number': 'nxosv'}) =&gt; changed=true \n  ansible_loop_var: item\n  device_type:\n    comments: ''\n    created: '2021-03-16'\n    custom_fields: {}\n    display_name: Cisco nxosv\n    front_image: null\n    id: 177ec0e0-6455-4d0c-9074-ee3f519cdcd8\n    is_full_depth: true\n    last_updated: '2021-03-16T00:15:51.913297Z'\n    manufacturer: 58c56ff8-f507-4356-9b6f-915be289831b\n    model: nxosv\n    part_number: nxosv\n    rear_image: null\n    slug: nxosv\n    subdevice_role: null\n    tags: []\n    u_height: 1\n    url: http://nautobot-demo.josh-v.com/api/dcim/device-types/177ec0e0-6455-4d0c-9074-ee3f519cdcd8/\n  item:\n    manufacturer: Cisco\n    model: nxosv\n    part_number: nxosv\n    slug: nxosv\n  msg: device_type nxosv created\nchanged: [localhost] =&gt; (item={'model': 'vEOS', 'manufacturer': 'Arista', 'slug': 'veos', 'part_number': 'veos'}) =&gt; changed=true \n  ansible_loop_var: item\n  device_type:\n    comments: ''\n    created: '2021-03-16'\n    custom_fields: {}\n    display_name: Arista vEOS\n    front_image: null\n    id: f93a4194-a71d-4532-b6f7-0af6e67f8155\n    is_full_depth: true\n    last_updated: '2021-03-16T00:15:52.673428Z'\n    manufacturer: fdead7a3-58a6-4a62-bb52-e21bbe2c6bf7\n    model: vEOS\n    part_number: veos\n    rear_image: null\n    slug: veos\n    subdevice_role: null\n    tags: []\n    u_height: 1\n    url: http://nautobot-demo.josh-v.com/api/dcim/device-types/f93a4194-a71d-4532-b6f7-0af6e67f8155/\n  item:\n    manufacturer: Arista\n    model: vEOS\n    part_number: veos\n    slug: veos\n  msg: device_type veos created\nMETA: ran handlers\nMETA: ran handlers\n\nPLAY RECAP **********************************************************************************************************************************************\nlocalhost                  : ok=1    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n</code></pre> <p>At this point the device types are now available inside of the UI.</p> <p> </p> <p>The second execution of playbook shows that with these three settings the module is idempotent:</p> <pre><code>josh-v@60a6498959f8:~$ ansible-playbook add_device_types.yml -vv\nansible-playbook 2.10.6\n  config file = /local/ansible.cfg\n  configured module search path = ['/local/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules']\n  ansible python module location = /usr/local/lib/python3.7/site-packages/ansible\n  executable location = /usr/local/bin/ansible-playbook\n  python version = 3.7.10 (default, Feb 16 2021, 19:28:34) [GCC 8.3.0]\nUsing /local/ansible.cfg as config file\n[WARNING]: No inventory was parsed, only implicit localhost is available\n[WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all'\nredirecting (type: callback) ansible.builtin.yaml to community.general.yaml\nredirecting (type: callback) ansible.builtin.yaml to community.general.yaml\nSkipping callback 'default', as we already have a stdout callback.\nSkipping callback 'minimal', as we already have a stdout callback.\nSkipping callback 'oneline', as we already have a stdout callback.\n\nPLAYBOOK: add_device_types.yml **************************************************************************************************************************\n1 plays in add_device_types.yml\n\nPLAY [ADD DEVICE TYPES] *********************************************************************************************************************************\nMETA: ran handlers\n\nTASK [05 - ADD DEVICE TYPES] ****************************************************************************************************************************\ntask path: /local/add_device_types.yml:7\nok: [localhost] =&gt; (item={'model': 'ASAv', 'manufacturer': 'Cisco', 'slug': 'asav', 'part_number': 'asav'}) =&gt; changed=false \n  ansible_loop_var: item\n  device_type:\n    comments: ''\n    created: '2021-03-16'\n    custom_fields: {}\n    device_count: 0\n    display_name: Cisco ASAv\n    front_image: null\n    id: 0bdb5944-a2c2-4093-a83c-c8c69485d5ac\n    is_full_depth: true\n    last_updated: '2021-03-16T00:15:49.347705Z'\n    manufacturer: 58c56ff8-f507-4356-9b6f-915be289831b\n    model: ASAv\n    part_number: asav\n    rear_image: null\n    slug: asav\n    subdevice_role: null\n    tags: []\n    u_height: 1\n    url: http://nautobot-demo.josh-v.com/api/dcim/device-types/0bdb5944-a2c2-4093-a83c-c8c69485d5ac/\n  item:\n    manufacturer: Cisco\n    model: ASAv\n    part_number: asav\n    slug: asav\n  msg: device_type asav already exists\nok: [localhost] =&gt; (item={'model': 'CSR1000v', 'manufacturer': 'Cisco', 'slug': 'csr1000v', 'part_number': 'csr1000v'}) =&gt; changed=false \n  ansible_loop_var: item\n  device_type:\n    comments: ''\n    created: '2021-03-16'\n    custom_fields: {}\n    device_count: 0\n    display_name: Cisco CSR1000v\n    front_image: null\n    id: a804d796-194a-46e2-af72-bdfbc179af92\n    is_full_depth: true\n    last_updated: '2021-03-16T00:15:50.335484Z'\n    manufacturer: 58c56ff8-f507-4356-9b6f-915be289831b\n    model: CSR1000v\n    part_number: csr1000v\n    rear_image: null\n    slug: csr1000v\n    subdevice_role: null\n    tags: []\n    u_height: 1\n    url: http://nautobot-demo.josh-v.com/api/dcim/device-types/a804d796-194a-46e2-af72-bdfbc179af92/\n  item:\n    manufacturer: Cisco\n    model: CSR1000v\n    part_number: csr1000v\n    slug: csr1000v\n  msg: device_type csr1000v already exists\nok: [localhost] =&gt; (item={'model': 'IOSv', 'manufacturer': 'Cisco', 'slug': 'iosv', 'part_number': 'iosv'}) =&gt; changed=false \n  ansible_loop_var: item\n  device_type:\n    comments: ''\n    created: '2021-03-16'\n    custom_fields: {}\n    device_count: 0\n    display_name: Cisco IOSv\n    front_image: null\n    id: 70504d2c-1641-4e6d-be40-192eb1d6e0c0\n    is_full_depth: true\n    last_updated: '2021-03-16T00:15:51.095938Z'\n    manufacturer: 58c56ff8-f507-4356-9b6f-915be289831b\n    model: IOSv\n    part_number: iosv\n    rear_image: null\n    slug: iosv\n    subdevice_role: null\n    tags: []\n    u_height: 1\n    url: http://nautobot-demo.josh-v.com/api/dcim/device-types/70504d2c-1641-4e6d-be40-192eb1d6e0c0/\n  item:\n    manufacturer: Cisco\n    model: IOSv\n    part_number: iosv\n    slug: iosv\n  msg: device_type iosv already exists\nok: [localhost] =&gt; (item={'model': 'nxosv', 'manufacturer': 'Cisco', 'slug': 'nxosv', 'part_number': 'nxosv'}) =&gt; changed=false \n  ansible_loop_var: item\n  device_type:\n    comments: ''\n    created: '2021-03-16'\n    custom_fields: {}\n    device_count: 0\n    display_name: Cisco nxosv\n    front_image: null\n    id: 177ec0e0-6455-4d0c-9074-ee3f519cdcd8\n    is_full_depth: true\n    last_updated: '2021-03-16T00:15:51.913297Z'\n    manufacturer: 58c56ff8-f507-4356-9b6f-915be289831b\n    model: nxosv\n    part_number: nxosv\n    rear_image: null\n    slug: nxosv\n    subdevice_role: null\n    tags: []\n    u_height: 1\n    url: http://nautobot-demo.josh-v.com/api/dcim/device-types/177ec0e0-6455-4d0c-9074-ee3f519cdcd8/\n  item:\n    manufacturer: Cisco\n    model: nxosv\n    part_number: nxosv\n    slug: nxosv\n  msg: device_type nxosv already exists\nok: [localhost] =&gt; (item={'model': 'vEOS', 'manufacturer': 'Arista', 'slug': 'veos', 'part_number': 'veos'}) =&gt; changed=false \n  ansible_loop_var: item\n  device_type:\n    comments: ''\n    created: '2021-03-16'\n    custom_fields: {}\n    device_count: 0\n    display_name: Arista vEOS\n    front_image: null\n    id: f93a4194-a71d-4532-b6f7-0af6e67f8155\n    is_full_depth: true\n    last_updated: '2021-03-16T00:15:52.673428Z'\n    manufacturer: fdead7a3-58a6-4a62-bb52-e21bbe2c6bf7\n    model: vEOS\n    part_number: veos\n    rear_image: null\n    slug: veos\n    subdevice_role: null\n    tags: []\n    u_height: 1\n    url: http://nautobot-demo.josh-v.com/api/dcim/device-types/f93a4194-a71d-4532-b6f7-0af6e67f8155/\n  item:\n    manufacturer: Arista\n    model: vEOS\n    part_number: veos\n    slug: veos\n  msg: device_type veos already exists\nMETA: ran handlers\nMETA: ran handlers\n\nPLAY RECAP **********************************************************************************************************************************************\nlocalhost                  : ok=1    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n</code></pre> <p>After completion of this you will have the device types (hardware models) available for you to assign to devices (coming up next).</p>"},{"location":"nautobot-ansible-device-types/#summary","title":"Summary","text":"<p>Device types are important so you know what model of devices you have to work with. This will come in handy as well in your automations that if you have a particular device type that you need to do something against. Such as having a separate type for Cisco Catalyst 3750G vs Catalyst 3750X. They are all 3750 switches, however you may need to apply a unique configuration set against a particular device type. By having this predefined in your source of truth, you are all set to be able to run automations against each.  </p> <p>Hope this has helped. If so, let me know with a comment below or give a thumbs up on the post. Feel free to connect with me on Twitter @vanderaaj</p>"},{"location":"nautobot-ansible-devices/","title":"Nautobot Ansible Collection: Devices","text":"<p>All of the work through the modules thus far in the series have brought us to what we all want to see. How to get or update device information inside of Nautobot. Adding of sites, device types, device roles are required to get us to this point. Now you can see how to add a device to Nautobot using the networktocode.nautobot.device module.  </p> <p>There are many optional parameters for the module specifically. I encourage you to take a look at the module documentation (linked below) in order to get a good sense of all of the options available. The required parameters for a device that is present are:</p> <ul> <li>device_role</li> <li>device_type</li> <li>name</li> <li>site</li> <li>status</li> </ul> <p>An important caveat for me is that this is something that should be done with rarity. Only when truly adding a device to Nautobot, in a programmatic way this should be used. I do not advocate for running this module constantly based on your devices. The idea is to get Nautobot to be your source of truth about devices, not to have devices be the source of truth and updating Nautobot.  </p> <p>So where do I see this being run? I do absolutely see it being a part of a pipeline or a service portal. The idea being that the service portal has a request for a new site to be turned up. That in turn kicks off an Ansible Playbook that will make the necessary updates to Nautobot, and is done in a consistent manor.</p>"},{"location":"nautobot-ansible-devices/#module-documentation","title":"Module Documentation","text":"<ul> <li>Read the Docs</li> <li>GitHub</li> </ul> <p>This module does require pynautobot to execute properly</p>"},{"location":"nautobot-ansible-devices/#environment","title":"Environment","text":"<p>For this demo, here are the versions shown:</p> Component Version Nautobot v1.0.0b2 Nautobot Ansible Collection v1.0.3 pynautobot 1.0.1"},{"location":"nautobot-ansible-devices/#data-file","title":"Data File","text":"<p>The Nautobot devices file is going to be a little bit more involved. In this particular demo case there are no existing inventories to use. If you want to see a demo of a similar how to add devices to NetBox using an existing Ansible inventory, I encourage you to take a look at my GitHub repository where I did a Meetup video on working with Ansible + NetBox. The same/similar concept can be used with Nautobot.  </p> <p>To simulate the idea that we are going to be running a playbook execution as part of a service request, here is the data file that will be fed to the Ansible playbook:</p> <pre><code># group_vars/all/devices.yml\n---\ndevices:\n  - name: \"grb-rtr01\"\n    site: \"GRB\"\n    device_role: \"Router\"\n    device_type: \"IOSv\"\n  - name: \"msp-rtr01\"\n    site: \"MSP\"\n    device_role: \"Router\"\n    device_type: \"IOSv\"\n</code></pre>"},{"location":"nautobot-ansible-devices/#example","title":"Example","text":""},{"location":"nautobot-ansible-devices/#example-adding-device-types","title":"Example - Adding Device Types","text":"<p>First if you are following along with the examples thus far, I made a new site here. So in order to accommodate the new site, I added GRB and re-ran the playbook to create sites. That was done successfully and idempotently with only the GRB site being added.</p> <pre><code>---\n- name: \"ADD DEVICES\"\n  hosts: localhost\n  connection: local\n  gather_facts: false # No gathering facts about the container execution env\n  tasks:\n    - name: \"05 - ADD DEVICES\"\n      networktocode.nautobot.device:\n        url: \"{{ lookup('env', 'NAUTOBOT_URL') }}\"\n        token: \"{{ lookup('env', 'NAUTOBOT_TOKEN') }}\"\n        data:\n          name: \"{{ item['name'] }}\"\n          site: \"{{ item['site'] }}\"\n          device_role: \"{{ item['device_role'] }}\"\n          device_type: \"{{ item['device_type'] }}\"\n          platform: \"IOS\"\n          status: \"Active\" # Newly required for Nautobot, a status of some kind\n      loop: \"{{ devices }}\"\n</code></pre>"},{"location":"nautobot-ansible-devices/#example-execution","title":"Example - Execution","text":"<p>Before the execution there are no devices within Nautobot:</p> <p></p> <p>This execution shows that all of the device types are added.</p> <pre><code>josh-v@1297da6292df:~$ ansible-playbook add_devices.yml -vv\nansible-playbook 2.10.6\n  config file = /local/ansible.cfg\n  configured module search path = ['/local/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules']\n  ansible python module location = /usr/local/lib/python3.7/site-packages/ansible\n  executable location = /usr/local/bin/ansible-playbook\n  python version = 3.7.10 (default, Feb 16 2021, 19:28:34) [GCC 8.3.0]\nUsing /local/ansible.cfg as config file\n[WARNING]: No inventory was parsed, only implicit localhost is available\n[WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all'\nredirecting (type: callback) ansible.builtin.yaml to community.general.yaml\nredirecting (type: callback) ansible.builtin.yaml to community.general.yaml\nSkipping callback 'default', as we already have a stdout callback.\nSkipping callback 'minimal', as we already have a stdout callback.\nSkipping callback 'oneline', as we already have a stdout callback.\n\nPLAYBOOK: add_devices.yml *******************************************************************************************************\n1 plays in add_devices.yml\n\nPLAY [ADD DEVICES] **************************************************************************************************************\nMETA: ran handlers\n\nTASK [05 - ADD DEVICES] *********************************************************************************************************\ntask path: /local/add_devices.yml:7\nchanged: [localhost] =&gt; (item={'name': 'grb-rtr01', 'site': 'GRB', 'device_role': 'Router', 'device_type': 'IOSv'}) =&gt; changed=true \n  ansible_loop_var: item\n  device:\n    asset_tag: null\n    cluster: null\n    comments: ''\n    config_context: {}\n    created: '2021-03-28'\n    custom_fields: {}\n    device_role: c6909cfd-0fd9-4ab1-b0e7-58493fff84b7\n    device_type: 70504d2c-1641-4e6d-be40-192eb1d6e0c0\n    display_name: grb-rtr01\n    face: null\n    id: 7757ffbd-cca8-49ee-978f-66fbdb3f6b14\n    last_updated: '2021-03-28T15:48:29.324146Z'\n    local_context_data: null\n    name: grb-rtr01\n    parent_device: null\n    platform: 96d58cc1-ad7e-43c7-a79f-db748e6eb894\n    position: null\n    primary_ip: null\n    primary_ip4: null\n    primary_ip6: null\n    rack: null\n    serial: ''\n    site: c92f368a-93a0-472b-a391-b9e9665b42a4\n    status: active\n    tags: []\n    tenant: null\n    url: http://nautobot-demo.josh-v.com/api/dcim/devices/7757ffbd-cca8-49ee-978f-66fbdb3f6b14/\n    vc_position: null\n    vc_priority: null\n    virtual_chassis: null\n  item:\n    device_role: Router\n    device_type: IOSv\n    name: grb-rtr01\n    site: GRB\n  msg: device grb-rtr01 created\nchanged: [localhost] =&gt; (item={'name': 'msp-rtr01', 'site': 'MSP', 'device_role': 'Router', 'device_type': 'IOSv'}) =&gt; changed=true \n  ansible_loop_var: item\n  device:\n    asset_tag: null\n    cluster: null\n    comments: ''\n    config_context: {}\n    created: '2021-03-28'\n    custom_fields: {}\n    device_role: c6909cfd-0fd9-4ab1-b0e7-58493fff84b7\n    device_type: 70504d2c-1641-4e6d-be40-192eb1d6e0c0\n    display_name: msp-rtr01\n    face: null\n    id: 72f14290-865d-43a6-af7b-4e29c6400460\n    last_updated: '2021-03-28T15:48:30.966452Z'\n    local_context_data: null\n    name: msp-rtr01\n    parent_device: null\n    platform: 96d58cc1-ad7e-43c7-a79f-db748e6eb894\n    position: null\n    primary_ip: null\n    primary_ip4: null\n    primary_ip6: null\n    rack: null\n    serial: ''\n    site: c72cce62-1dd6-483e-90a3-3331ea3155a8\n    status: active\n    tags: []\n    tenant: null\n    url: http://nautobot-demo.josh-v.com/api/dcim/devices/72f14290-865d-43a6-af7b-4e29c6400460/\n    vc_position: null\n    vc_priority: null\n    virtual_chassis: null\n  item:\n    device_role: Router\n    device_type: IOSv\n    name: msp-rtr01\n    site: MSP\n  msg: device msp-rtr01 created\nMETA: ran handlers\nMETA: ran handlers\n\nPLAY RECAP **********************************************************************************************************************\nlocalhost                  : ok=1    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n</code></pre> <p>The second execution of playbook shows that with these three settings the module is idempotent:</p> <pre><code>josh-v@1297da6292df:~$ ansible-playbook add_devices.yml -vv\nansible-playbook 2.10.6\n  config file = /local/ansible.cfg\n  configured module search path = ['/local/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules']\n  ansible python module location = /usr/local/lib/python3.7/site-packages/ansible\n  executable location = /usr/local/bin/ansible-playbook\n  python version = 3.7.10 (default, Feb 16 2021, 19:28:34) [GCC 8.3.0]\nUsing /local/ansible.cfg as config file\n[WARNING]: No inventory was parsed, only implicit localhost is available\n[WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all'\nredirecting (type: callback) ansible.builtin.yaml to community.general.yaml\nredirecting (type: callback) ansible.builtin.yaml to community.general.yaml\nSkipping callback 'default', as we already have a stdout callback.\nSkipping callback 'minimal', as we already have a stdout callback.\nSkipping callback 'oneline', as we already have a stdout callback.\n\nPLAYBOOK: add_devices.yml *******************************************************************************************************\n1 plays in add_devices.yml\n\nPLAY [ADD DEVICES] **************************************************************************************************************\nMETA: ran handlers\n\nTASK [05 - ADD DEVICES] *********************************************************************************************************\ntask path: /local/add_devices.yml:7\nok: [localhost] =&gt; (item={'name': 'grb-rtr01', 'site': 'GRB', 'device_role': 'Router', 'device_type': 'IOSv'}) =&gt; changed=false \n  ansible_loop_var: item\n  device:\n    asset_tag: null\n    cluster: null\n    comments: ''\n    config_context: {}\n    created: '2021-03-28'\n    custom_fields: {}\n    device_role: c6909cfd-0fd9-4ab1-b0e7-58493fff84b7\n    device_type: 70504d2c-1641-4e6d-be40-192eb1d6e0c0\n    display_name: grb-rtr01\n    face: null\n    id: 7757ffbd-cca8-49ee-978f-66fbdb3f6b14\n    last_updated: '2021-03-28T15:48:29.324146Z'\n    local_context_data: null\n    name: grb-rtr01\n    parent_device: null\n    platform: 96d58cc1-ad7e-43c7-a79f-db748e6eb894\n    position: null\n    primary_ip: null\n    primary_ip4: null\n    primary_ip6: null\n    rack: null\n    serial: ''\n    site: c92f368a-93a0-472b-a391-b9e9665b42a4\n    status: active\n    tags: []\n    tenant: null\n    url: http://nautobot-demo.josh-v.com/api/dcim/devices/7757ffbd-cca8-49ee-978f-66fbdb3f6b14/\n    vc_position: null\n    vc_priority: null\n    virtual_chassis: null\n  item:\n    device_role: Router\n    device_type: IOSv\n    name: grb-rtr01\n    site: GRB\n  msg: device grb-rtr01 already exists\nok: [localhost] =&gt; (item={'name': 'msp-rtr01', 'site': 'MSP', 'device_role': 'Router', 'device_type': 'IOSv'}) =&gt; changed=false \n  ansible_loop_var: item\n  device:\n    asset_tag: null\n    cluster: null\n    comments: ''\n    config_context: {}\n    created: '2021-03-28'\n    custom_fields: {}\n    device_role: c6909cfd-0fd9-4ab1-b0e7-58493fff84b7\n    device_type: 70504d2c-1641-4e6d-be40-192eb1d6e0c0\n    display_name: msp-rtr01\n    face: null\n    id: 72f14290-865d-43a6-af7b-4e29c6400460\n    last_updated: '2021-03-28T15:48:30.966452Z'\n    local_context_data: null\n    name: msp-rtr01\n    parent_device: null\n    platform: 96d58cc1-ad7e-43c7-a79f-db748e6eb894\n    position: null\n    primary_ip: null\n    primary_ip4: null\n    primary_ip6: null\n    rack: null\n    serial: ''\n    site: c72cce62-1dd6-483e-90a3-3331ea3155a8\n    status: active\n    tags: []\n    tenant: null\n    url: http://nautobot-demo.josh-v.com/api/dcim/devices/72f14290-865d-43a6-af7b-4e29c6400460/\n    vc_position: null\n    vc_priority: null\n    virtual_chassis: null\n  item:\n    device_role: Router\n    device_type: IOSv\n    name: msp-rtr01\n    site: MSP\n  msg: device msp-rtr01 already exists\nMETA: ran handlers\nMETA: ran handlers\n\nPLAY RECAP **********************************************************************************************************************\nlocalhost                  : ok=1    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n</code></pre>"},{"location":"nautobot-ansible-devices/#post-execution","title":"Post Execution","text":"<p>After the execution and notice that the module is idempotent, the two devices shown are all set to be added.</p> <p></p>"},{"location":"nautobot-ansible-devices/#summary","title":"Summary","text":"<p>Now that you have some devices, you can start to do a little bit more with your Nautobot environment. This playbook purposely did not add more information about the device yet, such as the serial number, interfaces, or IP addressing. This is all information that you can add more about the device as well using Ansible Facts and Resource Modules to continue to develop your source of truth. More likely to come in the future, or you can check out the content on GitHub and YouTube referenced above for immediate reference.</p> <p>Getting devices into Nautobot provides a powerful place to put your source of truth for automation. It does take a small bit to get to a good place, but with a little bit of effort up front you can get things done in a consistent and repeatable fashion. No more having to do things by hand with the data points.  </p> <p>Hope this has helped. If so, let me know with a comment below or give a thumbs up on the post.</p>"},{"location":"book-open-source-network-management/","title":"New Book - Open Source Network Management","text":"<p>Earlier this month I was able to hit the publish button on a new book - Open Source Network Management. The book dives into getting started with several open source network management tools. It is meant as a guide to help further your experience with using and installing open source tools, all on a single VM/host. The size of the host is meant to have minimal capital investment, in the way of a single NUC or a minimal VM deployed on a hypervisor in your environment.  </p> <p>The book is published on LeanPub, which is a publish early, publish often marketplace. The book is digital only, with PDF, ePub, and mobi formats available. Currently the book is indicating 80% completeness, with most of the technical content in place already! There are mainly soft edits in this early version.</p> <p>It has been a while since I have put some content out on my blog site. Why? Well, this book is the reason why. The time that I would have been making some content here, I have been putting into making the book. This will change. I will be putting out a few more posts upcoming.</p>","tags":["book","opensource","networkmanagment"]},{"location":"book-open-source-network-management/#tools-at-launch","title":"Tools At Launch","text":"<p>There are several open source tools that are covered. Starting out with installing Docker Community Edition (CE), then adding Docker Compose files to handle installation of the tools. After the Docker Compose is up, there is also a basic configuration to get up and running, actually using the project. Such as how to use the Nautobot Device Onboarding and Network Importer projects to get data into Nautobot. Or how to create a secrets vault to store your sensitive data, and then reference that data in other places. The current tool list includes:</p> <ul> <li>Nautobot (Source of Truth)</li> <li>Hashicorp Vault (Secrets Management)</li> <li>Telegraf (Metrics Gathering)</li> <li>Prometheus (Metrics Storage and Alerting)</li> <li>Grafana (Metrics Visualization)</li> <li>NGINX (Web Server/Reverse Proxy)</li> </ul> <p>With these components in place a modern network management stack can be put into place, with minimal investment.</p>","tags":["book","opensource","networkmanagment"]},{"location":"book-open-source-network-management/#tool-selection","title":"Tool Selection","text":"<p>These tools are all light weight tools that have the capability to be running on a single host to get up and running. Yet, after being light weight all will be able to scale out to meet the needs of some of the largest networks.</p>","tags":["book","opensource","networkmanagment"]},{"location":"book-open-source-network-management/#thank-you","title":"Thank You","text":"<p>Hopefully the content in the book is helpful! It was an enjoyable time to put it together.</p> <p>-Josh</p>","tags":["book","opensource","networkmanagment"]},{"location":"nautobot-jobs-execution/","title":"Nautobot Jobs - Your Custom API Endpoint","text":"<p>One of the best features of Nautobot as a Network Automation Platform is the ability to create your own custom code. This is executed via a job. What makes Nautobot unique is its ability to integrate with a Git repository to get those jobs and code for use into Nautobot. This provides perhaps the simplest, authenticated, and logged methodology for building your own API endpoints.</p> <p>Nautobot supplies an API endpoint to start execution of jobs. The big deal about why you would want to do this inside of Nautobot (even if you do not have any other data inside of Nautobot, but you should add data, it is a perk) is that you get an authentication mechanism with the Nautobot token setup and a logging mechanism. With Nautobot user accounts you can create tokens that will handle the API authentication. This is helpful that you do not need to add that into your own Flask, FastAPI, or Django application yourself. This is the same for the logging mechanism. Every job execution provides a log of the execution and the result.</p> <p>In this post I'm going to walk you through adding an API endpoint using the Git synchronization capabilities of Nautobot. This will provide a mechanism to add users to a Meraki organization. This is meant as an example. You can definitely look to leverage this in your own Nautobot install as well.</p>","tags":["sourceoftruth","networkmanagment","nautobot","jobs","meraki","cisco"]},{"location":"nautobot-jobs-execution/#creating-your-job","title":"Creating Your Job","text":"<p>There are three options for creating Jobs within Nautobot.</p> <ol> <li>Adding a Python file to the Jobs Root directory</li> <li>Using a Git repository to sync the job to Nautobot</li> <li>Creating a Nautobot App (plugin) that leverages a Job</li> </ol>","tags":["sourceoftruth","networkmanagment","nautobot","jobs","meraki","cisco"]},{"location":"nautobot-jobs-execution/#creating-your-git-repository","title":"Creating Your Git Repository","text":"<p>You can use any Git service that Nautobot has access to, and currently requires the repo to use a HTTPS endpoint. For this particular job setup, this will be hosted on GitHub. See the Nautobot docs for git as a data source for more details on setting up the Git sync. The following settings are used:</p> <p>Nautobot Git Repo Setup</p> <p>For the Git directory, the structure is going to look like the following:</p> <pre><code>\u276f tree\n.\n\u251c\u2500\u2500 jobs\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 meraki_users.py\n\u2514\u2500\u2500 README.md\n</code></pre> <p>The <code>jobs</code> directory is required. This allows for the Git data source to have multiple data sources, for creating your job, this is required. The <code>__init__.py</code> file is also required for the jobs to be synced properly. The <code>meraki_users.py</code> file is a file that I have chosen to make. This is where the Job class will reside. This can be any file name that you wish. The <code>README.md</code> is not required for the jobs to work. This is good documentation.</p>","tags":["sourceoftruth","networkmanagment","nautobot","jobs","meraki","cisco"]},{"location":"nautobot-jobs-execution/#creating-the-nautobot-git-source","title":"Creating the Nautobot Git Source","text":"<p>Navigate to Extensibility -&gt; Git Repositories. Select + Add button on the upper right of the section. Fill in the information, with the required fields in bold. If user authentication is required for the Git repository (such as a private repo), then you need to use a Personal Access Token (PAT). Once you click Create a synchronization will occur that will sync the jobs from the Repo. Navigate to the Extensibility -&gt; Jobs section. You will now see the jobs included on the page.</p> <p> </p> <p>For the Meraki Users jobs that are being setup, I am using two environment variables to control the environment. <code>MERAKI_DASHBOARD_API_KEY</code> for the API key to use to talk to the Meraki Dashboard. <code>NAUTOBOT_JOB_MERAKI_EMAIL_VALIDATION_REGEX</code> which is the regex string to do validation on email within the form. This defaults to allowing all if not set.</p>","tags":["sourceoftruth","networkmanagment","nautobot","jobs","meraki","cisco"]},{"location":"nautobot-jobs-execution/#writing-the-code","title":"Writing the Code","text":"","tags":["sourceoftruth","networkmanagment","nautobot","jobs","meraki","cisco"]},{"location":"nautobot-jobs-execution/#job-form-and-vars","title":"Job Form and Vars","text":"<p>Once the git repository is setup, and you have seen what the interface to get the Nautobot environment to sync to the repository it is on to writing the code. You define your class that will inherit from the Job class. With that in place, the first thing you define at the class level is the variables to display on the form. There are many types of variable inputs that the Job form will display. This is just a few from the example provided:</p> <pre><code>class CreateUsers(Job):\n    \"\"\"Class to create a Meraki user\n\n    Args:\n        Job (Nautobot Job): Meraki create user job\n    \"\"\"\n\n    user_email = StringVar(\n        description=\"User Email to add\",\n        label=f\"User Email, regex: {EMAIL_REGEX}\",\n        required=True,\n        regex=EMAIL_REGEX,\n    )\n\n    meraki_access_level = ChoiceVar(\n        description=\"Level of access\",\n        label=\"Access Level\",\n        choices=(\n            (\"full\", \"Full\"),\n            (\"read-only\", \"Read Only\"),\n            (\"enterprise\", \"Enterprise\"),\n            (\"none\", \"None\"),\n        ),\n    )\n</code></pre> <p>The <code>user_email</code> is a StringVar, that is a single line that will be presented. One of the options on the StringVar is the capability to complete a Regex validation. This is done with the <code>regex</code> key and provide it a regex to validate the response. This will provide the appropriate validation before launching the job. The <code>meraki_access_level</code> is a ChoiceVar that allows for multiple choices. This has a tuple type that is fed and provides for the value that is being provided as the first item in the tuple, and the display value on the second part of the tuple. </p> <p>The various types of vars that can be displayed on a job form can be found within Nautobot's code. The pre-defined Variable options are:</p> <ul> <li>BooleanVar</li> <li>ChoiceVar</li> <li>FileVar</li> <li>IntegerVar</li> <li>IPAddressVar</li> <li>IPAddressWithMaskVar</li> <li>IPNetworkVar</li> <li>MultiChoiceVar</li> <li>MultiObjectVar</li> <li>ObjectVar</li> <li>StringVar</li> <li>TextVar</li> </ul>","tags":["sourceoftruth","networkmanagment","nautobot","jobs","meraki","cisco"]},{"location":"nautobot-jobs-execution/#job-meta-data","title":"Job Meta data","text":"<p>The job section provides for a Meta class that will help to define data about the job class. This is where you can set a name, description, commit default setting, field order, and if this is read only or not.</p>","tags":["sourceoftruth","networkmanagment","nautobot","jobs","meraki","cisco"]},{"location":"nautobot-jobs-execution/#job-code","title":"Job Code","text":"<p>Once the form has been defined and the meta data has been provided, you can add the run function. This is what gets executed. The first part of the Job execution, while not necessary, can be helpful is to assign the data coming in.</p> <pre><code>def __init__(self):\n        super().__init__()\n        self.data = None\n        self.commit = None\n\n    def run(self, data, commit):\n        \"\"\"Run execution\n\n        Args:\n            data (dict): Data from the form\n            commit (bool): Commit changes to the database\n        \"\"\"\n        self.data = data\n        self.commit = commit\n\n        if self.commit is False:\n            self.log_info(obj=None, message=\"Commit set to False\")\n            self.log_info(obj=None, message=f\"Data pushed in: {self.data}\")\n            return\n</code></pre> <p>Inside the class initialization the data and commit objects are created and assigned None. Inside the run, <code>data</code> and <code>commit</code> kwargs are passed in. The <code>data</code> key maintains the data from the form that was filled out. This is a dictionary that contains the data passed in. If using an ObjectVar type from Nautobot, the objects will be what come over. The <code>commit</code> key is a boolean true/false should the job be making updates to the Nautobot database.</p> <p>This is the key part then in the run section. You can interact with other systems, such as in this case the Meraki Dashboard. As long as you can write Python to interact with the system, then you can have the Job execute the work. Of note here, those remote systems will have execution completed, even if commit is set to False. So you will need to put some logic into your class to handle commit to remote systems.</p>","tags":["sourceoftruth","networkmanagment","nautobot","jobs","meraki","cisco"]},{"location":"nautobot-jobs-execution/#api-details","title":"API Details","text":"","tags":["sourceoftruth","networkmanagment","nautobot","jobs","meraki","cisco"]},{"location":"nautobot-jobs-execution/#api-endpoint","title":"API EndPoint","text":"<p>There is an API endpoint that is created when using Jobs to launch the jobs. This varies depending on the method that was used to load the Job into Nautobot. The base is <code>https://nautobot.example.com/api/extras/jobs/</code>. The next component of the API URL is the method, with using git, then there is also the data source slug, such as <code>git.meraki-users</code>. The slug is what is used on the right portion of the <code>.</code> in the section. Then you add the file name to the URL endpoint, and lastly the class name. The last section of the endpoint is <code>/run/</code> knowing that this is the run endpoint.</p>","tags":["sourceoftruth","networkmanagment","nautobot","jobs","meraki","cisco"]},{"location":"nautobot-jobs-execution/#api-execution","title":"API Execution","text":"<p>The following execution, which was created with the Postman code snippet, shows what an execution would look like to launch a job. The two keys in the dictionary to pass to the endpoint are <code>commit</code> and <code>data</code>, just like what is passed into the class method. <code>data</code>'s value is a dictionary that has key/value pairs for each of the job fields, with the variables matching the Job UI variables.</p> <pre><code>import requests\nimport json\n\nurl = \"https://nautobot.example.com/api/extras/jobs/git.meraki-jobs/meraki_users/CreateUsers/run/\"\n\npayload = json.dumps({\n  \"data\": {\n    \"user_email\": \"josh@example.com\",\n    \"user_name\": \"Josh Testing API\",\n    \"meraki_org_id\": \"123456\",\n    \"meraki_network\": \"MN01\",\n    \"meraki_access_level\": \"full\"\n  },\n  \"commit\": True\n})\n\nheaders = {\n  'Authorization': f'Token {os.getenv(\"NAUTOBOT_TOKEN\")}',\n  'Content-Type': 'application/json',\n}\n\nresponse = requests.request(\"POST\", url, headers=headers, data=payload)\n\nprint(response.text)\n</code></pre>","tags":["sourceoftruth","networkmanagment","nautobot","jobs","meraki","cisco"]},{"location":"nautobot-jobs-execution/#api-response","title":"API Response","text":"<p>The example response looks like:</p> <pre><code>{\n    \"url\": \"https://nautobot.example.com/api/extras/jobs/git.meraki-jobs/meraki_users/CreateUsers/\",\n    \"id\": \"git.meraki-jobs/meraki_users/CreateUsers\",\n    \"name\": \"Create Meraki User\",\n    \"description\": \"\",\n    \"test_methods\": [],\n    \"vars\": {\n        \"user_email\": \"StringVar\",\n        \"user_name\": \"StringVar\",\n        \"meraki_org_id\": \"ChoiceVar\",\n        \"meraki_network\": \"StringVar\",\n        \"meraki_access_level\": \"ChoiceVar\"\n    },\n    \"result\": {\n        \"id\": \"d95051bb-4df6-4b45-a8a8-b967d4b3c66c\",\n        \"url\": \"https://nautobot.example.com/api/extras/job-results/d95051bb-4df6-4b45-a8a8-b967d4b3c66c/\",\n        \"created\": \"2021-10-09T12:34:42.024410-05:00\",\n        \"completed\": null,\n        \"name\": \"git.meraki-jobs/meraki_users/CreateUsers\",\n        \"obj_type\": \"extras.job\",\n        \"status\": {\n            \"value\": \"pending\",\n            \"label\": \"Pending\"\n        },\n        \"user\": {\n            \"id\": \"131e7ebb-3f50-48ad-8a64-4a0a077488de\",\n            \"url\": \"https://nautobot.example.com/api/users/users/131e7ebb-3f50-48ad-8a64-a10a077488de/\",\n            \"username\": \"test_user\",\n            \"display\": \"test_user\"\n        },\n        \"data\": null,\n        \"job_id\": \"57a86756-175b-43c8-9b76-1df1340a46e7\"\n    }\n}\n</code></pre> <p>The response gives an immediate response that the job status is pending within the <code>result</code> key. The biggest piece of data that is provided is the <code>result[\"url\"]</code> which you can use to get the status of the job. This job result will include the status, completed and start times, the total number of log statuses (success, warning, failure, info). </p> <pre><code>{\n    \"id\": \"bf0405ef-4c1a-491e-a5bf-9d11cbdaa7ad\",\n    \"url\": \"https://nautobot.example.com/api/extras/job-results/bf0405ef-4c1a-491e-a5bf-9d11cbdbb7ad/\",\n    \"created\": \"2021-10-11T09:26:27.802264-05:00\",\n    \"completed\": \"2021-10-11T09:26:30.602580-05:00\",\n    \"name\": \"git.meraki-users/meraki_users/CreateUsers\",\n    \"obj_type\": \"extras.job\",\n    \"status\": {\n        \"value\": \"completed\",\n        \"label\": \"Completed\"\n    },\n    \"user\": {\n        \"id\": \"131e7ebb-3f50-48ad-8a64-4a0a077488de\",\n        \"url\": \"https://nautobot.example.com/api/users/users/131e7ebb-3f50-48ad-8a64-4a0a077488de/\",\n        \"username\": \"test_user\",\n        \"display\": \"test_user\"\n    },\n    \"data\": {\n        \"run\": {\n            \"log\": [\n                [\n                    \"2021-10-11T14:26:30.591027+00:00\",\n                    \"info\",\n                    null,\n                    null,\n                    \"Commit set to False\"\n                ],\n                [\n                    \"2021-10-11T14:26:30.591682+00:00\",\n                    \"info\",\n                    null,\n                    null,\n                    \"Data pushed in: {'user_email': 'josh@example.com', 'user_name': 'Josh Vanderaa', 'meraki_org_id': '111111', 'meraki_network': '', 'meraki_access_level': 'read-only'}\"\n                ],\n                [\n                    \"2021-10-11T14:26:30.592706+00:00\",\n                    \"info\",\n                    null,\n                    null,\n                    \"Database changes have been reverted automatically.\"\n                ]\n            ],\n            \"info\": 3,\n            \"failure\": 0,\n            \"success\": 0,\n            \"warning\": 0\n        },\n        \"total\": {\n            \"info\": 3,\n            \"failure\": 0,\n            \"success\": 0,\n            \"warning\": 0\n        },\n        \"output\": \"\"\n    },\n    \"job_id\": \"0f9e369d-8cdd-4b52-a737-2af1e6c8bb92\"\n}\n</code></pre>","tags":["sourceoftruth","networkmanagment","nautobot","jobs","meraki","cisco"]},{"location":"nautobot-jobs-execution/#summary","title":"Summary","text":"<p>Nautobot is a tool that is well worth the time to look at. The capabilities that are being added to help Network Engineers manage, document, and automate the networks are well worth the investment. The Jobs end point and API endpoint are extremely helpful. And with having the capability to sync from a Git repository makes it easy to get started with an authenticated and logged API.</p> <p>For more on getting started with several open source network management tools, including Nautobot in this post, take a look at my book on LeanPub - Open Source Network Management.</p>","tags":["sourceoftruth","networkmanagment","nautobot","jobs","meraki","cisco"]},{"location":"devnet-expert-starting-point/","title":"DevNet Expert - Starting Point","text":"<p>This week Cisco announced the DevNet Expert certification exam. This exam and certification is something that I have been looking forward to for a long while. Dating back to the announcement of the certifications that were being provided. This was announced at Cisco Live 2019 in San Diego. I had started to lose some hope that this would become a reality with how long of a delay from the initial announcement to the announcement of availability. But it is now here. So here we go.</p>","tags":["devnet","ccdevnetexpert","cisco"]},{"location":"devnet-expert-starting-point/#devnet-cert-requirements","title":"DevNet Cert Requirements","text":"<p>For me this is going to be just the lab that is required. I already have the DevNet Professional certification, which has the DevCor exam as a requirement. This exam is the first requirement for the Expert certification. The second part is the lab, which is what the next several months of prep will be for me.</p>","tags":["devnet","ccdevnetexpert","cisco"]},{"location":"devnet-expert-starting-point/#helpful-links","title":"Helpful Links","text":"<p>So far I have accumulated a few helpful links from the Internet and the various announcements. Here are the links:</p> <ul> <li>Lab Topics</li> <li>Equipment and Software List</li> <li>DevNet Sandbox</li> </ul> <p>Let's cover how I plan to use these and how I'm getting started with the studies, right away.</p>","tags":["devnet","ccdevnetexpert","cisco"]},{"location":"devnet-expert-starting-point/#lab-topics","title":"Lab Topics","text":"<p>This is my blueprint. I'm going to be working through each of these items to make sure that I have an adequate proficiency in this arena. The plan that I have is to make sure that I have the knowledge, skills, and abilities that are going to make for an efficient execution of the lab.</p>","tags":["devnet","ccdevnetexpert","cisco"]},{"location":"devnet-expert-starting-point/#equipment-and-software-list","title":"Equipment and Software List","text":"<p>This is an excellent resource that Cisco is providing. I plan on having a VM built that is going to be emulating each of the pieces of the software on the device. I am anticipating that I will have a machine of similar capabilities, and maybe a UI to it in order to accomplish the practical lab.</p>","tags":["devnet","ccdevnetexpert","cisco"]},{"location":"devnet-expert-starting-point/#third-party-software-requirements","title":"Third Party Software Requirements","text":"<p>The first thing that is standing out to me is that I am already pretty familiar with several of these tools that are listed. I have written about them in my Open Source Network Management book. So I initially feel very good about using these tools:</p> <ul> <li>Telegraf</li> <li>Grafana</li> <li>Nginx</li> <li>Docker</li> <li>Hashicorp Vault</li> </ul> <p>I also use Ansible on a regular occurrence, and same with GitLab. I have some exposure to InfluxDB, and this may be one of the greater learning curves compared to my general experience with Prometheus.</p>","tags":["devnet","ccdevnetexpert","cisco"]},{"location":"devnet-expert-starting-point/#devnet-sandbox","title":"DevNet Sandbox","text":"<p>The greatest thing that we have as individuals preparing for the exam is that we have a sandbox that provides access to some of the Cisco specific items that will need to be automated/worked on. With that in mind, I'll be using that whenever possible. Hopefully the resources will be available for me as I look to leverage them.</p>","tags":["devnet","ccdevnetexpert","cisco"]},{"location":"devnet-expert-starting-point/#summary","title":"Summary","text":"<p>Overall I am definitely looking forward to taking a stab at the exam. I may have some other learnings that will come from this. The other aspect as I look at the exam topics is that it is in line with what I work with every day in the day job at Network to Code. This proficiency that is being tested matches up well with my day to day. </p>","tags":["devnet","ccdevnetexpert","cisco"]},{"location":"automation-inventory/","title":"Automation Inventory","text":"<p>This is a topic that I'm fairly opinionated on as of late is looking at what should be maintained within an inventory and the strategy of how to set up the inventory. </p> <p>For the case of this blog post, I am going to use the term playbook to represent the automation being run. This is yes an Ansible term, but also apply this as your automation run that is using Nornir or any other automation framework.</p>"},{"location":"automation-inventory/#what-should-be-in-an-inventory","title":"What Should Be In an Inventory","text":"<p>When taking a look at inventories there are usually a lot of options of what to include in your inventory for network devices. This can include the interfaces, VLANs on the device, BGP ASN, and connection information. For me, the only thing that should be in the inventory is connection information. All of the other items such as BGP ASN, Interface Names, and anything else should be on a playbook by playbook basis. </p> <p>An inventory is meant to represent what could be automated. Because of this, extra information such as interface information should not be included. Your environment may have a lot of playbooks that the interface information is relevant, but it is not always the case. And because of this, that information should be gathered at runtime as the playbook executes, not as part of the inventory.</p> <p>The only thing that should be in an inventory and is the basic needs of the inventory is connection information. This includes at a minimum the IP address/hostname of the device. It may also include SSH key/API key information or username and password to connect. This is also something that may be gathered as part of the process if you maintain this information inside of a password management system (such as Hashicorp Vault).</p>"},{"location":"automation-inventory/#inventory-strategy","title":"Inventory Strategy","text":"<p>When looking to set up your inventory, I would expect a minimal number of inventories. One that maintains production devices. One for development/test hosts. This may be broken up more into the teams that are responsible for particular devices, but if you can, I also argue that the devices across teams should be available within the same production inventory. This will allow for more automation collaboration to deliver results for the organization, which is the goal. There may be individual inventories for each of the scope of business, which may make sense as well, especially if there are some boundaries that may not want to be crossed.</p> <p>What about different sites? Well, these should all be within groups within the organization. If you have a large number of sites or buildings with a lot of gear, this is exactly where groups fit in. Both in Ansible and Nornir there is the concept of groups, which allows for the setup of the environment. There should not be an inventory per building/location, as this leads to difficulty as the automation scales.</p>"},{"location":"automation-inventory/#a-look-at-inventories","title":"A Look At Inventories","text":"<p>Looking at the graphic above where there are 4 playbooks that you currently have. With an automation framework, your inventory should be the same inventory for each of the playbook activities. Whether it is for an OS version check, checking or configuring BGP neighbors, or configuring access interfaces. Of those examples, only the changing of the access interface playbook would need to know what the interfaces are. So to have the interfaces live in the inventory, especially if the inventory is gathered at run time, is causing unnecessary data gathering that may get in the way of automation.</p>"},{"location":"automation-inventory/#possible-exception","title":"Possible Exception","text":"<p>One possible exception to this may be when using a system like AWX that maintains a database of inventory. In this setup, the inventory plugin execution may be run independently and at an off hours of automation usage. This would then allow for effective caching of these inventory data items. Since the inventory will then be used often without checking the source of truth for network devices, you would not be making unnecessary calls. This would still lead to ineffective loading of data though during development of playbooks. In development the inventory should be much lighter, which will allow for quick development regardless.</p> <p>Hopefully this information will help to keep your automation environment up and running smoothly! Or if you are just getting started, a good place to start from!</p> <p>Josh</p>"},{"location":"graphql-aliasing/","title":"GraphQL - Aliasing","text":"<p>One of the features that I find myself using periodically that I think is underrated as far as using GraphQL is its ability to alias return keys in the response. This can be extremely helpful for developers writing applications, as it allows them to have the API response with the keys they are looking for. I have found this feature particularly useful when working on applications like Meraki and Nautobot together. In Nautobot a place is typically defined as the key <code>site</code>. In the Meraki world this is commonly set up as a <code>network</code>. Without GraphQL's alias feature, the developer would need to translate this data over.</p> <p>Let's explore two scenarios where a developer might choose to alias the response from GraphQL:</p> <ul> <li>Quick translation between systems</li> <li>Response from multiple queries</li> </ul> <p>I will demonstrate the capabilities of these scenarios using the Nautobot demo instance at https://demo.nautobot.com. For each of these, make sure that you have logged in already before going to the GraphiQL page.</p>"},{"location":"graphql-aliasing/#base-graphql-query","title":"Base GraphQL Query","text":"<p>The base GraphQL query will be:</p> <pre><code>query {\n  devices(name__ic: \"den\") {\n    name\n    site {\n      name\n    }\n  }\n}\n</code></pre> <p>Note that I am filtering down to a smaller subset of the devices within Nautobot for brevity.</p> <p>With the filter in place looking for devices that include <code>bre</code> in the name you get the response of:</p> <pre><code>{\n  \"data\": {\n    \"devices\": [\n      {\n        \"name\": \"den01-dist-01\",\n        \"site\": {\n          \"name\": \"DEN01\"\n        }\n      },\n      {\n        \"name\": \"den01-edge-01\",\n        \"site\": {\n          \"name\": \"DEN01\"\n        }\n      },\n      {\n        \"name\": \"den01-edge-02\",\n        \"site\": {\n          \"name\": \"DEN01\"\n        }\n      },\n      {\n        \"name\": \"den01-leaf-01\",\n        \"site\": {\n          \"name\": \"DEN01\"\n        }\n      },\n      {\n        \"name\": \"den01-leaf-02\",\n        \"site\": {\n          \"name\": \"DEN01\"\n        }\n      },\n      {\n        \"name\": \"den01-leaf-03\",\n        \"site\": {\n          \"name\": \"DEN01\"\n        }\n      },\n      {\n        \"name\": \"den01-leaf-04\",\n        \"site\": {\n          \"name\": \"DEN01\"\n        }\n      },\n      {\n        \"name\": \"den01-leaf-05\",\n        \"site\": {\n          \"name\": \"DEN01\"\n        }\n      },\n      {\n        \"name\": \"den01-leaf-06\",\n        \"site\": {\n          \"name\": \"DEN01\"\n        }\n      },\n      {\n        \"name\": \"den01-leaf-07\",\n        \"site\": {\n          \"name\": \"DEN01\"\n        }\n      },\n      {\n        \"name\": \"den01-leaf-08\",\n        \"site\": {\n          \"name\": \"DEN01\"\n        }\n      }\n    ]\n  }\n}\n</code></pre> <p>First of all, it's pretty awesome to get just the data that you're looking for. This is by far one of the best features of GraphQL. Use it whenever you are getting data from a system that offers GraphQL.</p>"},{"location":"graphql-aliasing/#graphql-alias","title":"GraphQL - Alias","text":"<p>Now with the request in place, let's go down the path of changing the response where ever the key of <code>site</code> is found that GraphQL will instead send the key <code>network</code>. This will align more with the data format that Meraki is looking for within their API.</p> <p>The alias is done by having instead of just <code>site</code> on line 4 of the query, but to add the new key name in front of a colon. Such as line 4 of the query will now be <code>network: site {</code>.</p> <pre><code>query {\n  devices(name__ic: \"bre\") {\n    name\n    network: site {\n      name\n    }\n  }\n}\n</code></pre> <p>And the response you notice you no longer see <code>site:</code> any where in the response.</p> <pre><code>{\n  \"data\": {\n    \"devices\": [\n      {\n        \"name\": \"bre01-dist-01\",\n        \"network\": {\n          \"name\": \"BRE01\"\n        }\n      },\n      {\n        \"name\": \"bre01-edge-01\",\n        \"network\": {\n          \"name\": \"BRE01\"\n        }\n      },\n      {\n        \"name\": \"bre01-edge-02\",\n        \"network\": {\n          \"name\": \"BRE01\"\n        }\n      },\n      {\n        \"name\": \"bre01-leaf-01\",\n        \"network\": {\n          \"name\": \"BRE01\"\n        }\n      },\n      {\n        \"name\": \"bre01-leaf-02\",\n        \"network\": {\n          \"name\": \"BRE01\"\n        }\n      },\n      {\n        \"name\": \"bre01-leaf-03\",\n        \"network\": {\n          \"name\": \"BRE01\"\n        }\n      },\n      {\n        \"name\": \"bre01-leaf-04\",\n        \"network\": {\n          \"name\": \"BRE01\"\n        }\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"graphql-aliasing/#graphql-multiple-queries","title":"GraphQL - Multiple Queries","text":"<p>A second case, which is from the GraphQL learning page is where you have multiple queries to the same part of the API. This is where you would then need to alias the response in order to make the GraphQL query to be valid.</p>"},{"location":"graphql-aliasing/#multiple-search-query-error","title":"Multiple Search Query - Error","text":"<p>First let's look at what a bad query looks like from GraphQL that will generate an error. Let's say you want to get data about two sites from Nautobot. A query that would get you the data looks like:</p> <pre><code>query {\n  sites(name: \"ORD01\") {\n    facility\n  }\n  sites(name: \"DEN01\") {\n    facility\n  }\n}\n</code></pre> <p>Running that query generates the following error message:</p> <pre><code>\"message\": \"Fields \\\"sites\\\" conflict because they have differing arguments. Use different aliases on the fields to fetch both if this was intentional.\",\n</code></pre>"},{"location":"graphql-aliasing/#multiple-search-query-successful","title":"Multiple Search Query - Successful","text":"<p>The workaround is to alias the response. Such that instead of sites being sent back, we can use the site name with an incrementing counter number on the end. This may be something where you build a query offline in Python where you keep appending to a string, and in the end send over a large query with aliased keys.</p> <p>The new query looks like this:</p> <pre><code>query {\n  site1: sites(name: \"ORD01\") {\n    facility\n  }\n  site2: sites(name: \"DEN01\") {\n    facility\n  }\n}\n</code></pre> <p>There are now new site keys at the beginning. The response now gives you the data that you would expect:</p> <pre><code>{\n  \"data\": {\n    \"site1\": [\n      {\n        \"facility\": \"O'Hare International Airport\"\n      }\n    ],\n    \"site2\": [\n      {\n        \"facility\": \"Denver International Airport\"\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"graphql-aliasing/#using-in-python","title":"Using in Python","text":"<p>Taking the first example to Python, let's take a look at how you can then access the aliased response. The pynautobot package takes the response and has a convenience attribute of <code>.json</code> that will get the data into a Python dictionary for use. This will loop over each of the devices in the response and print the corresponding network (which will all be the same):</p> <pre><code>import json\n\nimport click\nimport pynautobot\n\n\n@click.command\n@click.option(\"--nautobot_url\", envvar=\"NAUTOBOT_URL\")\n@click.option(\"--nautobot_token\", envvar=\"NAUTOBOT_TOKEN\")\ndef main(nautobot_url, nautobot_token):\n    nautobot = pynautobot.api(url=nautobot_url, token=nautobot_token)\n\n    query = \"\"\"\nquery {\n  devices(name__ic: \"den\") {\n    name\n    network: site {\n      name\n    }\n  }\n}\n    \"\"\"\n\n    graphql_response = nautobot.graphql.query(query=query)\n    response = graphql_response.json\n    for device in response[\"data\"][\"devices\"]:\n        print(device[\"network\"])\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Running that code with setting the appropriate environment variables of <code>NAUTOBOT_URL</code> and <code>NAUTOBOT_TOKEN</code> gets you this response:</p> <pre><code>\u276f python graphql_aliasing.py\n{'name': 'DEN01'}\n{'name': 'DEN01'}\n{'name': 'DEN01'}\n{'name': 'DEN01'}\n{'name': 'DEN01'}\n{'name': 'DEN01'}\n{'name': 'DEN01'}\n{'name': 'DEN01'}\n{'name': 'DEN01'}\n{'name': 'DEN01'}\n{'name': 'DEN01'}\n</code></pre>"},{"location":"graphql-aliasing/#summary","title":"Summary","text":"<p>In summary, when you are able to use GraphQL to get data and the response may not be exactly the format you are looking for, take a look at using a GraphQL alias to get your response. This will come in super helpful over time. I'd love to hear what your thoughts are on this and where you are using it!</p> <p>Happy Automating!</p>"},{"location":"nautobot-ip-provisioning/","title":"Nautobot IP Provisioning","text":"<p>One of the great things about building an enterprise system, is being able to get systems to work cohesively amongst themselves to bring a complete solution. One of the workflows that is often required in a static IP address environment is the need to provide static IP addresses to hosts on a network segment. When using an IPAM (IP Address Management) solution such as Nautobot, the APIs and SDKs/modules made available for use in automation workflows is paramount to having the cohesion to make a seamless IT system. </p> <p>In this post I will be diving into the use of Nautobot as the IPAM. Using Ansible and the Nautobot modules, I will then show how you can get the next available IP address and assign it for use to the next VM. There will likely need to be some minor tweaks for use in your system.</p>"},{"location":"nautobot-ip-provisioning/#nautobot-setup","title":"Nautobot Setup","text":"<p>The first action is to get Nautobot set up with the tags and prefixes that are going to be used. I am using a tag of <code>VM Addresses</code> to assign to Prefixes that are to be allowed to assign VM addresses to. This way, you can grab the specific prefix based on the Nautobot data, not being static. You may want to look at having some other tags as well and passing these items into the Ansible execution either from a UI Survey, ExtraVars, or having multiple instances of the playbook.</p> <p>This playbook is also using Environment Variables of <code>NAUTOBOT_URL</code> and <code>NAUTOBOT_TOKEN</code> in order to be able to be ported to other Nautobot systems.</p> <pre><code>---\n# create_parent_prefix.yml\n- name: \"SET UP PARENT PREFIX\"\n  hosts: localhost\n  connection: local\n  gather_facts: no\n  vars:\n    nautobot_url: \"{{ lookup('ansible.builtin.env', 'NAUTOBOT_URL')}}\"\n    nautobot_token: \"{{ lookup('ansible.builtin.env', 'NAUTOBOT_TOKEN')}}\"\n  tasks:\n    - name: \"100: CREATE A TAG FOR PARENT PREFIX OF VM ADDRESSING\"\n      networktocode.nautobot.tag:\n        url: \"{{ nautobot_url }}\"\n        token: \"{{ nautobot_token }}\"\n        name: \"VM Addresses\"\n        description: \"Addresses for VMs to live in\"\n\n    - name: \"200: SET UP PARENT PREFIXES FOR ALL REMOTE SITES\"\n      networktocode.nautobot.prefix:\n        url: \"{{ nautobot_url }}\"\n        token: \"{{ nautobot_token }}\"\n        prefix: \"{{ item }}\"\n        status: Active\n        description: VM Addresses\n        family: 4\n        state: present\n        tags:\n          - VM Addresses\n      loop:\n        - \"198.51.100.0/30\"\n        - \"203.0.113.0/30\"\n        - \"198.51.100.192/26\"\n</code></pre> <p>The <code>loop</code> list could also become variables as well, which will allow even further dynamic capability. But this is just an example to do the work rather than having to set this in the UI and demo it. This set up method is much easier.</p>"},{"location":"nautobot-ip-provisioning/#provisioning-ip-addresses","title":"Provisioning IP Addresses","text":"<p>Now that the Nautobot environment is set up with tags for the subnet, it is easy to write a playbook to get the prefixes that have the tag. Then from the prefixes that are available, check to see which prefix has available IP addresses. If there is one available, then go ahead and get that IP address and assign it for use.</p> <p>This playbook is using GraphQL to query the data from Nautobot, this is the fastest way to gather data from Nautobot, using a filter on the prefixes tag <code>vm-addresses</code>. The <code>vm-addresses</code> is the slug of the Tag that is created in the first playbook execution.</p> <pre><code>---\n# get_and_set_next_available.yml\n- name: \"GET AND USE NEXT AVAILABLE ADDRESS\"\n  hosts: localhost\n  connection: local\n  gather_facts: no\n  vars:\n    nautobot_url: \"{{ lookup('ansible.builtin.env', 'NAUTOBOT_URL')}}\"\n    nautobot_token: \"{{ lookup('ansible.builtin.env', 'NAUTOBOT_TOKEN')}}\"\n    found_available_address: False\n    graphql_query: |\n      {\n        prefixes(tag: \"vm-addresses\") {\n          prefix\n          id\n        }\n      }\n  tasks:\n    - name: \"BLOCK: GET AND USE NEXT AVAILABLE IP, LIMIT TO SINGLE EXECUTION AT A TIME\"\n      block:\n        - name: \"100: GET PREFIXES AVAILABLE\"\n          networktocode.nautobot.query_graphql:\n            url: \"{{ nautobot_url }}\"\n            token: \"{{ nautobot_token }}\"\n            query: \"{{ graphql_query }}\"\n          register: query_response\n\n        - debug:\n            msg: \"{{ query_response }}\"\n\n        - name: \"200: FIND FIRST AVAILABLE IP ADDRESS\"\n          include_tasks: \"find_available2.yml\"\n          loop: \"{{ query_response['data']['prefixes'] }}\"\n          loop_control:\n            index_var: \"my_idx\"\n          when: not found_available_address\n</code></pre> <p>Task 100 executes the GraphQL query to get the response. From here the data has just what we need to continue on.</p> <p>Task 200 executes the following tasks that are defined in the <code>find_available.yml</code> file. This is a separate file so that the group of tasks can be executed within a loop.</p> <pre><code># find_available.yml\n---\n- name: \"debug var\"\n  debug:\n    msg:\n      - \"{{ my_idx }}: ID={{ item['id'] }}, Prefix={{ item['prefix'] }}\"\n      - \"{{ found_available_address }}\"\n\n- name: \"300: GET NEXT AVAILABLE IP ADDRESS AND ASSIGN IT\"\n  networktocode.nautobot.ip_address:\n    url: \"{{ nautobot_url }}\"\n    token: \"{{ nautobot_token }}\"\n    prefix: \"{{ item['prefix'] }}\"\n    status: \"Active\"\n    state: new\n  register: nautobot_ip_state\n  when: \"not found_available_address\" # A second when check to not keep assigning the IP address\n\n- name: \"310: ASSIGN found_available_address TO TRUE\"\n  ansible.builtin.set_fact:\n    found_available_address: True\n  when: nautobot_ip_state.changed\n</code></pre> <p>Task 300 uses the Ansible module <code>networktocode.nautobot.ip_address</code> to get the next available address in the prefix and registers it to the variable <code>nautobot_ip_state</code>. In Task 310 the <code>nautobot_ip_state</code> is checked to determined if the IP address has changed. If it has, it will set the variable <code>found_available_address</code> to <code>True</code>, so that the system knows that it no longer needs to check for an address because an address has been found and set.</p> <p>The looping condition from Task 200 to keep going through the playbook allows for more tasks to be added on after Task 200 and then the 300 series tasks. If this playbook were to be done at this point and nothing further is to be executed you could change task 310 to be an <code>ansible.builtin.meta</code> task with the directive of <code>end_play</code> to stop the looping and the Play itself. That is how you would look to chain multiple plays together in a Playbook, that the first play of finding the address is completed and the next play of adding the business logic would take place.</p> <p>Before the first execution, here is what Nautobot looks like without an address assigned:</p> <p></p> <p>The first execution of the playbook you get the following result:</p> <pre><code>\u276f ansible-playbook get_and_set_next_available.yml\n[WARNING]: No inventory was parsed, only implicit localhost is available\n[WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all'\n\nPLAY [GET AND USE NEXT AVAILABLE ADDRESS] ******************************************************************************************************\n\nTASK [100: GET PREFIXES AVAILABLE] *************************************************************************************************************\nok: [localhost]\n\nTASK [debug] ***********************************************************************************************************************************\nok: [localhost] =&gt; {\n    \"msg\": {\n        \"changed\": false,\n        \"data\": {\n            \"prefixes\": [\n                {\n                    \"id\": \"819f71d5-d761-4011-a3d3-d61f9ea15fec\",\n                    \"prefix\": \"198.51.100.0/30\"\n                },\n                {\n                    \"id\": \"f23de4a0-7a5d-4921-9cbb-5f2da6eb419f\",\n                    \"prefix\": \"198.51.100.192/26\"\n                },\n                {\n                    \"id\": \"7c464ee3-1ce2-47c1-b1aa-5628ab421e83\",\n                    \"prefix\": \"203.0.113.0/30\"\n                }\n            ]\n        },\n        \"failed\": false,\n        \"graph_variables\": null,\n        \"query\": \"{\\n  prefixes(tag: \\\"vm-addresses\\\") {\\n    prefix\\n    id\\n  }\\n}\\n\",\n        \"url\": \"https://demo.nautobot.com\"\n    }\n}\n\nTASK [200: FIND FIRST AVAILABLE IP ADDRESS] ****************************************************************************************************\nincluded: ./automationday_demos/find_available.yml for localhost =&gt; (item={'prefix': '198.51.100.0/30', 'id': '819f71d5-d761-4011-a3d3-d61f9ea15fec'})\nincluded: ./automationday_demos/find_available.yml for localhost =&gt; (item={'prefix': '198.51.100.192/26', 'id': 'f23de4a0-7a5d-4921-9cbb-5f2da6eb419f'})\nincluded: ./automationday_demos/find_available.yml for localhost =&gt; (item={'prefix': '203.0.113.0/30', 'id': '7c464ee3-1ce2-47c1-b1aa-5628ab421e83'})\n\nTASK [debug var] *******************************************************************************************************************************\nok: [localhost] =&gt; {\n    \"msg\": [\n        \"0: ID=819f71d5-d761-4011-a3d3-d61f9ea15fec, Prefix=198.51.100.0/30\",\n        false\n    ]\n}\n\nTASK [300: GET NEXT AVAILABLE IP ADDRESS AND ASSIGN IT] ****************************************************************************************\nok: [localhost]\n\nTASK [310: ASSIGN found_available_address TO TRUE] *********************************************************************************************\nskipping: [localhost]\n\nTASK [debug var] *******************************************************************************************************************************\nok: [localhost] =&gt; {\n    \"msg\": [\n        \"1: ID=f23de4a0-7a5d-4921-9cbb-5f2da6eb419f, Prefix=198.51.100.192/26\",\n        false\n    ]\n}\n\nTASK [300: GET NEXT AVAILABLE IP ADDRESS AND ASSIGN IT] ****************************************************************************************\nchanged: [localhost]\n\nTASK [310: ASSIGN found_available_address TO TRUE] *********************************************************************************************\nok: [localhost]\n\nTASK [debug var] *******************************************************************************************************************************\nok: [localhost] =&gt; {\n    \"msg\": [\n        \"2: ID=7c464ee3-1ce2-47c1-b1aa-5628ab421e83, Prefix=203.0.113.0/30\",\n        true\n    ]\n}\n\nTASK [300: GET NEXT AVAILABLE IP ADDRESS AND ASSIGN IT] ****************************************************************************************\nskipping: [localhost]\n\nTASK [310: ASSIGN found_available_address TO TRUE] *********************************************************************************************\nskipping: [localhost]\n\nPLAY RECAP *************************************************************************************************************************************\nlocalhost                  : ok=11   changed=1    unreachable=0    failed=0    skipped=3    rescued=0    ignored=0   \n</code></pre> <p>You can see on the first run of TASK 310 that the task has been skipped because there was not an available address (I purposely assigned addresses to fill up the first IP prefix space). The second loop through there was an IP address assigned and created. Then the last iteration skipped since the IP address was already assigned. Looking in the Nautobot UI, you now see an IP address assigned by the automation.</p> <p></p>"},{"location":"nautobot-ip-provisioning/#summary","title":"Summary","text":"<p>Combining the power of Nautobot as your source of truth about network data with the tools that the systems teams are using such as Ansible provides for a powerful combination. The Nautobot ecosystem is continuing to grow and the amount of capabilities enabled by Nautobot is tremendous. In an upcoming post, I will put together a Nautobot Job that will be able to complete the same activity and allow for an API call to be made by systems if you are not leveraging Ansible in your environment today.</p> <p>Let me know what you think on the social media at LinkedIn or Twitter.</p>"},{"location":"moving-to-hugo/","title":"Moving to Hugo","text":"<p>In this post I dive into more about my migration of the blog site to Hugo static content system. I will dive into primarily the why and how during this post. This also dives into the few changes that I had to make in order to make the change over from a Jekyll site to the Hugo site.</p>"},{"location":"moving-to-hugo/#why-migrate","title":"Why Migrate?","text":"<p>While I do not have a ton of posts, I do have a few that I like to get out into the wild periodically. As I looked around the landscape of the blog pages these days, I was starting to see my page a bit dated. I had previously went with the Jekyll Minimal Mistakes theme. The theme itself was wonderful and quite extensible. However, one of my concerns besides the look was the lack of updates coming out on the theme. Not that I was going to be taking advantage of every new feature, there seems to have been a slow down.</p> <p>The second reason is that I've been experimenting some more with GoLang and recently found a better understanding of how GoLang's templating engine works. Although admittedly as I write this, I know that I will need to re-learn again this as I have not put it into practice, and it is not completely native in my mind. But knowing how the templating language works, will allow me to be able to be more extensible for the site in the future. And when comparing wanting to learn Ruby templating vs GoLang templating. GoLang is definitely further on the list.</p>"},{"location":"moving-to-hugo/#migration-activities","title":"Migration Activities","text":"<p>To make the migration, there were several steps that I had to take and one optional step that I took along the way.</p> <ul> <li>Migrate Jekyll to Hugo repo</li> <li>There is a helpful Hugo migration script that will read a Jekyll blog organization and update the structure to Hugo</li> <li>There were a few minor challenges with the migration, that it didn't take a few things into account that VS Code search was helpful for</li> <li>Update the existing markdown files for the new syntax</li> <li>Migration from GitLab Pages to GitHub Pages (Optional)</li> </ul> <p>Let's go into a few of these topics.</p>"},{"location":"moving-to-hugo/#migration-from-jekyll-to-hugo","title":"Migration from Jekyll to Hugo","text":"<p>I ended up doing a two parts on this. I followed the post here about doing so, which generally went well -  https://chenhuijing.com/blog/migrating-from-jekyll-to-hugo/. I stopped after making the import to a repo named <code>migrate</code>. By doing this I had all of my data from the previous blog into a format for Hugo. On the repo that would become my GitHub Pages repository, I set up a new Hugo site using the <code>hugo</code> CLI tool. So that I would start fresh.</p> <p>My fresh site I decided to go with the Congo theme. I had done some research on what themes were available and went with Congo for it's freshness and sharpness to it. The documentation provided as well for each of the settings went deep. In checking the repository I also found that it was being actively maintained. I definitely liked the idea that it was being updated.</p>"},{"location":"moving-to-hugo/#blog-posts-base","title":"Blog Posts - Base","text":"<p>From there I moved a few of the markdown files into the new repository and ran the command <code>hugo server</code> to get a local instance up and running. Taking a look at the posts, things started working very quickly. I was able to see my latest posts and was able to explore changes to the layout rapidly.</p>"},{"location":"moving-to-hugo/#blog-posts-images","title":"Blog Posts - Images","text":"<p>One of the biggest changes for me is that I went ahead and moved the images from the <code>assets</code> directory in the Jekyll world into the <code>static/images/</code> directory. This allowed me to change the URL path on the images to being <code>/images/{{ imageFilePath/Name }}</code>. Thankfully VS Code has a global find and replace that I was then able to replace all of the previous paths with that of the new path. And just like that the images were up and running.</p>"},{"location":"moving-to-hugo/#blog-posts-code-highlighting","title":"Blog Posts - Code Highlighting","text":"<p>Code highlighting was one of the bigger changes that I needed to accomplish in making the migration over from Jekyll. I had previously created some syntax highlighting with line numbers that were part of a Jekyll plugin. So my code looked like:</p> <pre><code>{% highlight yaml linenos %}\n...YAML HERE ...\n{% endhighlight %}\n</code></pre> <p>I used VS Code's Regex find and replace to help with this. I searched for:</p> <pre><code>\\{% highlight (\\w+) linenos %\\}\n</code></pre> <p>And replaced it with the following, without the spaces between the braces (I need to figure out the Hugo escape method):</p> <pre><code> { {&lt; highlight $1 \"linenos=table\" &gt;} }\n</code></pre> <p>The endhighlight section was much easier. That was done with just a find and replace of finding <code>{% endhighlight %}</code> with the replacement of the <code>{ {&lt; /endhighlight &gt;} }</code> command (again without the spaces between the braces).</p> <p>At that point the code highlighting for multiple languages is complete.</p>"},{"location":"moving-to-hugo/#blog-posts-jinja-formatting","title":"Blog Posts - Jinja Formatting","text":"<p>As part of the Jekyll formatting of the blog posts, the Ansible and Python Jinja formatting would get interpreted as a Jekyll template code. In order to get around that Jekyll had a <code>{% raw %}</code> with a corresponding {% endraw %} to allow for the Jinja formatting to show up. Well, now in Hugo and GoLang it would show up in the output. So this was a find and replace of both of those combinations and to remove them.</p>"},{"location":"moving-to-hugo/#migration-from-gitlab-pages-to-github-pages","title":"Migration from GitLab Pages to GitHub Pages","text":"<p>Why I did this, this was more administrative than anything else. The rest of my world is done within GitHub with the open source projects that I work on. So this was a move that had a small bit of challenges. The primary piece was using GitHub Actions as the source of the GitHub Pages. I tried a few different combinations and in the end using the GitHub recommended Actions page when following the GitHub Pages set up, then GitHub Pages would get published.</p> <p>My original reason for hosting on GitLab Pages was that GitLab Pages had supported HTTPS and custom domains. Both of these features have made their way into GitHub Pages at this point.</p>"},{"location":"moving-to-hugo/#summary","title":"Summary","text":"<p>So far I'm back pretty happy with the initial migration. The process is familiar and works well. The previewing capabilities are terrific. And I'm pretty happy with the cleaner interface.</p> <p>Happy automating!</p> <p>Josh</p>"},{"location":"nautobot-jobs-root-in-root/","title":"Nautobot Jobs in Jobs Root","text":"<p>Today I was working to demonstrate how to get started with Nautobot Jobs within the Jobs root of Nautobot. This is not a pattern that I develop often, as I am typically developing Jobs within a plugin as my development standard. More to come on that later. During this case, the ask was to build a Job that would connect to a network device. I had a few troubles that I didn't want to have to work through on a call that had limited time and that was a screen share. So I am taking to working on this via a blog post to share, and hopefully will be helpful for others as well.</p>"},{"location":"nautobot-jobs-root-in-root/#nautobot-documentation","title":"Nautobot Documentation","text":"<p>The Nautobot documentation is pretty straight on. But it has grown organically over time. As such, this is going to be a small walkthrough in an opinionated way. This post will dive into:</p> <ul> <li>Providing a Job form that will allow you to select a Device</li> <li>Connect to the device</li> <li>Execute a <code>show version</code> on the device as a method of connecting to the device</li> </ul> <p>In this case I will be connecting to a Cisco device over SSH with Netmiko.</p>"},{"location":"nautobot-jobs-root-in-root/#creating-the-job","title":"Creating the Job","text":"<p>The default Jobs root path is <code>$NAUTOBOT_ROOT/jobs/</code>, which following the default install instructions for Nautobot is then <code>/opt/nautobot/jobs/</code> on a virtual machine install. With the default install the jobs directory comes with an <code>__init__.py</code> file:</p> <pre><code>nautobot@nautobot-host:~/jobs$ tree\n.\n\u2514\u2500\u2500 __init__.py\n</code></pre> <p>Let's build out the structure. The Nautobot repository has an Example file (or use this as a starting point) to get started. In the repository there is an example plugin that has jobs in it as a reference. You can find the jobs information then here https://github.com/nautobot/nautobot/blob/develop/examples/example_plugin/example_plugin/jobs.py.</p> <p>Here I'm executing <code>vim demo_jobs.py</code> from the <code>/opt/nautobot/jobs/</code> directory. In the end, the file just needs to be in the directory.</p> <p>This will be a multiple step process. I like to debug the job along the way in order to know what I am working with. So the first iteration I include the following code to verify what is being sent in from the device form. Here is what the code looks like.</p> <pre><code>from django.conf import settings\n\n# Importing Nautobot DCIM device model in order to be able to choose what device I want to connect to\nfrom nautobot.dcim.models import Device\n\n# The import of Job is needed to inherit the Job class. This is the magic sauce for Nautobot Jobs.\n# ObjectVar import will be used to select a device\nfrom nautobot.extras.jobs import Job, ObjectVar\n\n# Import Netmiko to connect to the device and execute commands\nfrom netmiko import ConnectHandler\n\n# Setting the name here gives a category for these jobs to be categorized into\nname = \"josh-v.com Demo jobs\"\n\nclass GetShowVersion(Job):\n    device = ObjectVar(\n        model=Device, # Using the Device model imported to say I want to select devices\n        query_params={\n            # Using this as a method to make sure that the device has a primary IP address\n            \"has_primary_ip\": True, \n            \"status\": \"active\", # Used to make sure that the device is active\n            }\n        )\n\n    # I like to describe the class Meta as what information about the Job to pass into Nautobot to help describe the Job\n    class Meta:\n        name = \"Get show version\"\n        description = \"Get the version information from a device\"\n        # Define task queues that this can run in.\n        task_queues = [\n            settings.CELERY_TASK_DEFAULT_QUEUE,\n            \"priority\",\n            \"bulk\",\n            ]\n\n    # The code execution, all things for the job are here.\n    def run(self, data, commit):\n        device = data[\"device\"]\n\n        self.log_debug(device)\n\n\njobs = (GetShowVersion)\n</code></pre> <p>At this point, you can exit out and then execute <code>nautobot-server post_upgrade</code> and restart the services <code>sudo systemctl restart nautobot nautobot-worker nautobot-scheduler</code> from the server CLI (usually not the Nautobot user). During this process you should see the message:</p> <pre><code>01:49:20.083 INFO    nautobot.extras.utils :\n  Created Job \"josh-v.com Demo jobs: Get show version\" from &lt;local: GetShowVersion&gt;\n01:49:20.089 INFO    nautobot.extras.utils :\n  Refreshed Job \"josh-v.com Demo jobs: Get show version\" from &lt;local: GetShowVersion&gt;\n</code></pre> <p>Then going into the UI menu of Jobs &gt;&gt;&gt; Jobs you get the following result with the Job Enabled column having a red X on it.</p> <p></p> <p>To enable the Job for execution, take a look at the docs here on the Nautobot Docs page for enabling a Job.</p> <p>When you execute the Job for a device, you now get a basic \"Hello World\" execution. You can see the result with the device name displayed.</p> <p></p>"},{"location":"nautobot-jobs-root-in-root/#get-the-device-information-from-nautobot","title":"Get the Device Information From Nautobot","text":"<p>The best way to get the information available within a Nautobot object is to work within the shell_plus environment. On your Nautobot server as the Nautobot user, enter the command <code>nautobot-server shell_plus</code>. This will bring you into an interactive shell, hopefully an iPython like environment. If you do not get an iPython shell and you are on a development host (not production), then you can do a pip install to get iPython installed (<code>pip install ipython</code>).</p> <pre><code>In [1]: mydevice = Device.objects.first()\n\nIn [2]: mydevice.name\nOut[2]: 'er01'\n\nIn [3]: mydevice.platform\nOut[3]: &lt;Platform: Cisco IOS&gt;\n\nIn [4]: mydevice.platform.name\nOut[4]: 'Cisco IOS'\n\nIn [5]: mydevice.platform.slug\nOut[5]: 'cisco-ios'\n\nIn [6]: mydevice.platform.napalm_driver\nOut[6]: 'ios'\n\nIn [7]: mydevice.primary_ip\nOut[7]: &lt;IPAddress: 203.0.113.65/27&gt;\n\nIn [8]: str(mydevice.primary_ip)\nOut[8]: '203.0.113.65/27'\n\nIn [9]: mydevice.primary_ip.host\nOut[9]: '203.0.113.65'\n</code></pre> <p>In the exploration with the shell, on lines 3 and 4 you see the verification that I'm working with the particular device that I wanted to work with. Lines 6 and 7 show that the platform name will come out as <code>Cisco IOS</code>. This will not work for using Netmiko connection to the device, so I then went exploring further of the data. I then saw on lines 15 and 16 something that I can work with for Netmiko. So I am going to use the NAPALM driver. If you need to do some conversions of NAPALM drivers over to Netmiko, there are some mapping utilities in the NetUtils library that can help. Lastly I explored how to get the IP address that I wanted to connect to. <code>mydevice.primary_ip</code> is a Nautobot object. That cannot be used in its own to connect to the host. I checked the string representation of the object, but that doesn't just get us the IP address. But I do know that the object has a separate <code>host</code> and <code>mask_length</code> objects. So I grabbed just the host for the Job.</p> <p>The run method now looks like this:</p> <pre><code>import os\n\n# Used to convert NAPALM type to Netmiko type\nfrom netutils.lib_mapper import NAPALM_LIB_MAPPER\n\n# CODE OMITTED FOR BREVITY #\n    # The code execution, all things for the job are here.\n    def run(self, data, commit):\n        device = data[\"device\"]\n\n        self.log_debug(device.name)\n\n        net_device_info = {\n            \"device_type\": NAPALM_LIB_MAPPER.get(device.platform.napalm_driver),\n            \"ip\": device.primary_ip.host,\n            \"username\": os.getenv(\"NAUTOBOT_NAPALM_USERNAME\"),\n            \"password\": os.getenv(\"NAUTOBOT_NAPALM_PASSWORD\"),\n        }\n\n        self.log_debug(net_device_info['device_type'])\n        net_device = ConnectHandler(**net_device_info)\n\n        output = net_device.send_command(\"show version\")\n\n        self.log_debug(output)\n</code></pre> <p>On line 1 of the above example, the <code>import os</code> was added to get the environment variables to get the credential to connect to the device. This could be set, or you may want to use another method.</p> <p>Note</p> <p>This is an example only. Environment variables have some considerations with that go along with them. There are also methods to work with the Nautobot Secrets providers that would be of better consideration here. But those were not added for the example of what it is that I'm trying to show in this post, how to connect and execute Netmiko Python scripts against a network device.</p> <p>The import on line 4 is used for converting the NAPALM platform into a usable Netmiko platform.</p> <p>When connecting to a network device in a multi-vendor environment there are a few considerations that need to be made. You need to know what the commands are needed to do various things. This is where the possibility of using NAPALM as a method to connect to devices and their use of \"getters\" would come in handy. This may be an idea for a future blog post. You can see the example on line 14 of the run method above the use of converting \"ios\" to \"cisco_ios\".</p>"},{"location":"nautobot-jobs-root-in-root/#executing-the-job","title":"Executing the Job","text":"<p>Now when you go to execute the job, you are able to get the <code>show version</code> output from the device to the screen. Take a look at the example:</p> <p></p>"},{"location":"nautobot-jobs-root-in-root/#summary","title":"Summary","text":"<p>With Nautobot, there are many ways that you can get started with executing your own custom Jobs. By using Nautobot Jobs you can centralize your power scripts into one place, allowing for those power scripts to be put to use and then some by the entire organization, not just a select few. Take some design caution in what you do make available though. Nautobot centralizes, but also provides for authentication, user logging, and creates an API that is available for use to others. Looking for more on how the Jobs are an API endpoint, take a look at my previous post on creating that API endpoint.</p> <p>Thanks for the read and Happy Automating!</p>"},{"location":"nautobot-jobs-root-in-root/#appendix-full-code","title":"Appendix - Full Code","text":"<p>Here is the full code block:</p> <pre><code>import os\nfrom django.conf import settings\n\n# Importing Nautobot DCIM device model in order to be able to choose what device I want to connect to\nfrom nautobot.dcim.models import Device\n\n# The import of Job is needed to inherit the Job class. This is the magic sauce for Nautobot Jobs.\n# ObjectVar import will be used to select a device\nfrom nautobot.extras.jobs import Job, ObjectVar\n\n# Import Netmiko to connect to the device and execute commands\nfrom netmiko import ConnectHandler\n\n# Used to convert NAPALM type to Netmiko type\nfrom netutils.lib_mapper import NAPALM_LIB_MAPPER\n\n# Setting the name here gives a category for these jobs to be categorized into\nname = \"josh-v.com Demo jobs\"\n\nclass GetShowVersion(Job):\n    device = ObjectVar(\n        model=Device, # Using the Device model imported to say I want to select devices\n        query_params={\n            \"has_primary_ip\": True, # Using this as a method to make sure that the device has a primary IP address\n            \"status\": \"active\", # Used to make sure that the device is active\n            }\n        )\n\n    # I like to describe the class Meta as what information about the Job to pass into Nautobot to help describe the Job\n    class Meta:\n        name = \"Get show version\"\n        description = \"Get the version information from a device\"\n        # Define task queues that this can run in.\n        task_queues = [\n            settings.CELERY_TASK_DEFAULT_QUEUE,\n            \"priority\",\n            \"bulk\",\n            ]\n\n    # The code execution, all things for the job are here.\n    def run(self, data, commit):\n        device = data[\"device\"]\n\n        self.log_debug(device.name)\n\n        net_device_info = {\n            \"device_type\": NAPALM_LIB_MAPPER.get(device.platform.napalm_driver),\n            \"ip\": device.primary_ip.host,\n            \"username\": os.getenv(\"NAUTOBOT_NAPALM_USERNAME\"),\n            \"password\": os.getenv(\"NAUTOBOT_NAPALM_PASSWORD\"),\n        }\n\n        self.log_debug(net_device_info['device_type'])\n        net_device = ConnectHandler(**net_device_info)\n\n        output = net_device.send_command(\"show version\")\n\n        self.log_debug(output)\n\n\njobs = (GetShowVersion)\n</code></pre>"},{"location":"2023-automation-review-top-3/","title":"2023 Automation Review: Top 3","text":"<p>The year of 2023 I think may have had some of the biggest leaps in the Network Automation capabilities that are being delivered by some of the best in the business. With Nautobot's Golden Config App adding the ability to complete configuration remediation and Ansible release Event Driven Ansible, there are a couple of powerful tools to help you with your Network Automation. And all with a great new conference addition specific to Network Automation.</p>"},{"location":"2023-automation-review-top-3/#nautobot-golden-config-configuration-remediation","title":"Nautobot Golden Config - Configuration Remediation","text":"<p>On the surface configuration remediation may not sound like a big deal. But I was astonished when the team that worked on the feature within Nautobot Golden Config demo'd the features and capabilities internally before the webinar releasing it. Initially, ok this is going to be pushing configuration. To me that is generally a solved problem, and wouldn't be a big deal.</p> <p>What the tool brings to the automation capabilities the ability to add an approval workflow to the configuration remediation effort. This is a big time process improvement in its own. On top of this, you are able to correlate which devices are associated to the configuration plan. And this all comes with the templating capabilities of the data that is in your SOT. Check out the YouTube Video for a deeper introduction.</p> <p>Info</p> <p>This is coming from a project that I am employed at. I still believe in the statements of this being game changing for configuration management. I would like to think that I am able to still be impartial, but I was truly impressed seeing this.</p>"},{"location":"2023-automation-review-top-3/#event-driven-ansible","title":"Event Driven Ansible","text":"<p>Next up we really started to see a lot more Event Driven Ansible (EDA). Technically announced in 2022, but it really started to gain conversation a lot more in 2023. EDA brings the ease of getting started with Ansible to Events, which are things that happen in the environment. What does this mean? Well, out of the box one of the event driven capabilities with EDA is to be a webhook receiver. This means that you can set up EDA to listen for an incoming webhook, and then launch an Ansible playbook based on conditions received in the webhook. This is all configured via rule books.</p> <p>Other capabilities natively as part of the system include the capability to listen to Kafka topics. Kafka is one of the leading pub/sub messaging systems that allow for providing messages to various systems. So you can have EDA subscribe to a particular topic on Kafka, and then kick off an Ansible Playbook from the message, again based on the conditions that are in the rule book.</p>"},{"location":"2023-automation-review-top-3/#network-automation-forum-autocon0","title":"Network Automation Forum - autocon0","text":"<p>This is not going to be a review, but a kudos and thank you to those at Network Automation Forum that brought autocon0 to being. This was a very well attended conference in the fall of 2023, and with a good number of attendees, it shows that the topic of Network Automation is still hot. </p>"},{"location":"2023-automation-review-top-3/#summary","title":"Summary","text":"<p>2023 was an awesome year for Network Automation, and I foresee even more coming in 2024. There will be even more built on top of what currently exists in open source, and I foresee even more new capabilities being brought and talked about all together.</p> <p>-Josh</p>"},{"location":"debian-finger-print-login/","title":"Debian Finger Print Login","text":"<p>As a long time MacBook user and of recent years on the M1 using the TouchID system that allows for fingerprint authentication, this is something that I wanted to get to work pretty quickly for myself. I had tried a couple of different options to get fingerprint reading to work. Through the 3 methods, I finally have one that works, and I figured it would be worth the share.</p>"},{"location":"debian-finger-print-login/#attempted-routes","title":"Attempted Routes","text":"<p>Now some of these may seem foolish as I write after the fact. But I had some reason to think that there would be success. The three options that I tried (only the last/third one worked):</p> <ul> <li>Kensington VeriMark</li> <li>Yubikey Bio</li> <li>DigitalPersona 4500</li> </ul>"},{"location":"debian-finger-print-login/#kensington-verimark","title":"Kensington Verimark","text":"<p>Why did I try this? Well, when I searched in the Amazon search box, I searched <code>Linux Desktop Fingerprint Reader</code>. This was Amazon's top choice. I went with it before researching further - my mistake. Once I received the tiny USB device, plugged it in, and nothing happened I went looking a bit further. Upon further research, this only had drivers for Windows and Mac. Makes sense. Those are the two biggest players. So this was a no go.</p>"},{"location":"debian-finger-print-login/#yubikey-bio","title":"Yubikey Bio","text":"<p>I thought to myself, let's try the Yubikey Bio, since there is a fingerprint component to it, maybe it would work for fingerprint auth. I could use a Yubikey anyhow myself. This didn't work, kind of obvious with having a third attempt upcoming.</p>"},{"location":"debian-finger-print-login/#digitalpersona-4500","title":"DigitalPersona 4500","text":"<p>Next I finally found the Debian docs on supporting fingerprint authentication. In the page there is a list of supported devices which gave me a few options to run down. I came across the Digital Persona device, which looks a like many of the finger print readers at various medical providers around. I felt better and gave this a go. I went with the Digital Persona 4500 off of Amazon. Instant success. I was able to get the finger print reader up and working. I had followed some other online docs that indicated that you only needed to install <code>fprintd</code>, this is wrong. You do need <code>fprintd</code> and <code>libpam-fprintd</code>, which is clearly listed on the Debian docs.</p>"},{"location":"debian-finger-print-login/#reading-in-the-fingerprint","title":"Reading in the FingerPrint","text":"<p>To read in the finger print on Debian, go to your Activities and search for <code>Users</code>. This will bring up the Users setting panel. Now you should have an option of <code>Fingerprint Login</code>, which may be set to off by default. Enable this and then you can go to adding some finger prints. You can see that I have a few finger prints already added.</p> <p></p> <p>Now to add a new finger print, you select <code>Scan new fingerprint</code>. What is not as obvious in this next screen is the process to add your finger print. Do you just put the finger on one time and let it sit? Do I need to use the process that Apple (and likely Android devices) have around putting your finger on multiple times? I found that if I just leave my finger on the scanner, it doesn't do much. So I do recommend the process of adding and removing your finger from the finger print reader. You will see the finger print icon change from grey to blue through the process.</p> <p></p> <p>Once it is complete, instead of blue or grey finger print icon you now get a green icon with complete indicated.</p> <p></p>"},{"location":"debian-finger-print-login/#summary","title":"Summary","text":"<p>Finger print authentication is coming along for those with a Debian desktop, not a laptop that has the finger print authentication built in. I'd ideally get this to work next with 1Password and other systems to not have to enter credentials. I believe this will be coming soon. Until then I do like the final choice of the Digital Persona 4500 to handle finger print authentication on my Linux desktop.</p>"},{"location":"designing-wan-availability/","title":"Designing WAN Availability","text":"<p>One of my hot topics in my past that I haven't seen written about often is the calculation of WAN availability and what the design is built for. There is often the number of 9's whether that is 5 9s or 3 9s or otherwise, where do you start? Well, in the past the post by EventHelix.com outlines system reliability. It talks about designing systems in parallel and in serial. What does that mean. Well, I am going to take the system availability and bring it into the Wide Area Network, which really could be brought to any environment that has a system uptime requirement as a way to calculate and validate the dollars that you are requesting. I am hoping that this will help you to be able to answer questions such as \"What if we added another service provider?\" or \"What if we changed out hardware for a smaller/larger hardware choice?\".</p>"},{"location":"designing-wan-availability/#the-nines","title":"The Nines","text":"<p>The first topic is to take a look at exactly what it means from a design availability perspective. When talking about availability the nines look like the following of availability from 525,600 minutes in a year (365 x 24 x 60):</p> Availability Downtime 99% 5,256 minutes (3.65 days) 99.9% 525.6 minutes 99.99% 52.6 minutes 99.999% 5.26 minutes <p>As you can see from the chart, the more nines of availability that you must target, the much more aggressive amount of downtime that is acceptable. Many data centers have had a target of 5 nines of availability for many years, and that target may always be changing as well.</p> <p>Historically in the WAN space you may get asked to design a WAN, validate the current design, or be asked what it would look like if you changed the design. By using a calculation of system availability you can then tie the WAN design back to real figures. So how do I build a design? Let's start by taking a look at a few of the components to do this.</p>"},{"location":"designing-wan-availability/#calculating-systems-availability","title":"Calculating Systems Availability","text":"<p>Before we get into a few examples that will back this up, let's first talk about the components and their calculations that will need to come into play in this math problem.</p>"},{"location":"designing-wan-availability/#circuits-mpls-networks","title":"Circuits / MPLS Networks","text":"<p>The first part is looking at the circuits and MPLS networks (or other network types) that come into play. These are the easier part actually. The circuits and network from providers will come with an availability SLA. Now there are other factors that come into play on whether or not the carrier/provider meet the SLAs. But that is the percentage number that will be used in calculating.</p>"},{"location":"designing-wan-availability/#device-availability","title":"Device Availability","text":"<p>The calculation provided by EventHelix's post makes sense here. The availability of a network device is calculated from having two numbers. MTBF (Mean Time Between Failure) and MTTR (Mean Time To Recovery). MTBF is a figure that is provided about how many hours that a device is expected to operate between failures. This is a number that is often available from your hardware vendors, and increasingly more difficult to find publicly. The MTTR is a number that you are expecting for how long it may take to replace the device or recover from a failure manually. In some environments this may be an SLA from a provider as well or you may have to put an educated guess to this.</p>"},{"location":"designing-wan-availability/#system-availability","title":"System Availability","text":"<p>From the calculation documented before, there are two parts that I do when calculating a WAN availability. First I flatten down to get the components into a series as much as possible. So if you have two routers providing WAN edge services to an MPLS cloud, the first thing you calculate is the flattened availability of the WAN edge. Once you have everything in a series, the math becomes pretty quick. </p> <p>Availability = (MTBF) / (MTBF + MTTR)</p>"},{"location":"designing-wan-availability/#parallel-system-availability","title":"Parallel System Availability","text":"<p>At each of the layers, you break things down into just a single availability for each of the systems. This means that you need to be able to calculate for a system that has parallel availability. You will see more in the second example below. So for now, the calculation that you need to know is that for finding availability of a system component that has a parallel option is:</p> <p>Availability = 1 - (1 - Ax) * (1 - Ay)</p> <p>Where <code>Ax</code> is the availability of component x (first circuit) and <code>Ay</code> is the availability of component y (secondary circuit).</p>"},{"location":"designing-wan-availability/#wan-design-availability-examples","title":"WAN Design Availability Examples","text":"<p>Let's start with some designs as a way to calculate the expected availability of a WAN environment. For the device types, I am going to use various Meraki devices as a method to help show since there is a post that is publicly available of what the MTBF is for the devices. Take a look here for the numbers that are posted (into a table below). </p> <p>Warning</p> <p>One assumption that does also need to be taken into account is power as well. That will be the one component that needs to get calculated in that for the sake of the blog we will assume will be 100% availability.</p> Device MTBF MR42 450,000h MX64 1,273,000h MX84 925,000h MX100 389,000h MX250 336,800h MX450 336,800h"},{"location":"designing-wan-availability/#basic-two-routers-connected-via-point-to-point-circuit","title":"Basic, Two Routers Connected Via Point to Point Circuit","text":"<p>This is the design basic components. There is only one component at each level:</p> <p></p> <p>Ok, so there are a few options now, other than the circuits having a single availability percentage. Let's take a look at a table of what these may look like for site availability. The WAN circuit will have an SLA of 99.5% availability and let's explore using different pieces of hardware.</p> WAN Edge Device Site Edge Device Edge Repair Time Site Repair Time WAN Edge Availability Site Edge Availability WAN Availability Total Availability MX84 MR42 8h 24h 99.99914% 99.99822% 99.5% 99.497% (2642 minutes of downtime) MX84 MX64 8h 24h 99.99914% 99.99937% 99.5% 99.499% (2636 minutes of downtime) MX64 MX64 8h 24h 99.99937% 99.99937% 99.5% 99.499% (2635 minutes of downtime) <p>What we see here is that while hardware absolutely comes into play for helping on the availability. The MX64 which has a much longer MTBF, 4 times as much, will still only get an expected availability of 7 minutes more of up time. The limiter is still the single WAN circuit availability of 99.5%.</p>"},{"location":"designing-wan-availability/#small-wan-adding-a-backup-link","title":"Small WAN, Adding a Backup Link","text":"<p>Now it has been a while since I have been ordering circuits, so I'm going to go with an extreme light example as a backup circuit. I don't know that cellular providers are providing a SLA of service availability, but let's add a secondary circuit that would be cellular or broadband provider. Let's then assume that the second circuit has an availability of 99% along with it, which would allow for 3 days+ per year of service outage.</p> <p>Using the calculation above for calculating the availability of a secondary circuit you now get a WAN availability of 99.99% - four nines by just adding an inexpensive WAN circuit. Let's see what that does to the calculations of availability:</p> WAN Edge Device Site Edge Device Edge Repair Time Site Repair Time WAN Edge Availability Site Edge Availability WAN Availability Total Availability MX84 MR42 8h 24h 99.99914% 99.99822% 99.995% 99.992% (40 minutes of downtime) MX84 MX64 8h 24h 99.99914% 99.99937% 99.995% 99.99% (34 minutes of downtime) MX64 MX64 8h 24h 99.99937% 99.99937% 99.995% 99.998% (33 minutes of downtime) <p>Very quickly are we getting to diminishing returns on what we can add to the system to make it more available before it gets too complex. Let's throw a second backup link into the mix before we start to explore adding in additional components at the WAN Edge layer.</p>"},{"location":"designing-wan-availability/#small-wan-two-backup-links","title":"Small WAN, Two Backup Links","text":"<p>This is where the design may start to get a bit unwieldy, and there is a cost for the complexity of hte network. That is just much more difficult to quantify. When adding a third circuit that has a 99% availability number with it, you get to having 6 nines and a 5. The 5 comes from the original circuit of 99.5% availability.</p> WAN Edge Device Site Edge Device Edge Repair Time Site Repair Time WAN Edge Availability Site Edge Availability WAN Availability Total Availability MX84 MR42 8h 24h 99.99914% 99.99822% 99.99995% 99.997% (14 minutes of downtime) MX84 MX64 8h 24h 99.99914% 99.99937% 99.99995% 99.998% (8 minutes of downtime) MX64 MX64 8h 24h 99.99937% 99.99937% 99.99995% 99.999% (7 minutes of downtime) <p>The third circuit gets your site availability expectation to 7 minutes of downtime where both ends of the circuit are having MX64 devices terminate the circuits. That is getting pretty darn good on the availability. </p>"},{"location":"designing-wan-availability/#mpls-network-with-two-uplinks-at-edge-site-with-one-and-backup","title":"MPLS Network With Two Uplinks At Edge, Site with One and Backup","text":"<p>One of the common practices now days is to have multiple edge routers at the WAN edge in the data center that provides connectivity to the cloud portion of the WAN run by the providers. Let's take a look at this design:</p> <p></p> <p>Now you can see that the layers have expanded here significantly. There are now five layers to take into consideration. Rather than include the repair times in the table, I'm removing those for brevity here. </p> Site Edge Availability Site WAN Availability (Not the Backup Link) MPLS Availability WAN Edge Availability to Provider Backup Network Availability WAN Edge Availability Total Availability 99.99937% 99.5% 99.95% 99.9975% 99% 99.99874% 99.9925% (39 minutes) <p>Warning</p> <p>This shows that the availability number here is slightly worse than what you may get from having multiple back ups earlier above. This is where the network architecture also needs to take into account the criticality of the path. If this is the design for a single site to single site it may not be optimal (depending on the service profile).</p>"},{"location":"designing-wan-availability/#summary","title":"Summary","text":"<p>When it comes to WAN design, there are multiple options and methods to design your Wide Area Network. You are able to put together calculations to be able to affirm that you have the right design to meet the business requirements for the WAN. The next challenge is then to get the appropriate uptime metric as often you may get \"The WAN needs to be online all of the time\". At this point you can put together general costs of providing the services at different service levels. This has been one of my favorite topics over the years and wanted to share. Hope that some may have found this helpful in articulating WAN design and costs!</p> <p>-Josh</p>"},{"location":"desktop-build-2023/","title":"Desktop Build 2023","text":"<p>Here I'm going to dive into what I'm planning to build out for my next desktop here in 2023. Prime Day is nearly upon us, and I'm anticipating (but do not know for sure) that prices on some of the gear that I'm looking for will be available at a good price. I'm also looking to build out a bigger system in order to run some intense VMs up coming.</p> <p>My goals: - Build a system that will last for 3-4 years at a minimum - Max out the RAM, that is my most limiting factor in my environments - Give Linux a try as the desktop OS, still a bit of debate in this, considering options:   - Debian 12   - POP OS   - Linux Mint</p>"},{"location":"desktop-build-2023/#the-system","title":"The System","text":"<p>What I'm going with are the following:</p> Component Item Amazon Link Processor Intel i5-12600K https://amzn.to/3XLsi3H Motherboard ~~ASUS Prime B760M-A~~ Gigabyte B765M DS3H AX https://amzn.to/3OhpdEe Memory Qty 2, 2 x 32 GB TEAMGROUP Elite DDR5 https://amzn.to/3rp0RAR Storage Samsung 980 Pro SSD NVMe https://amzn.to/3PS3Fk3 Case Cooler Master MasterBox Q300L https://amzn.to/43kHF4j Power Supply Thermaltake SMART 600W https://amzn.to/3NPuCCt CPU Cooler DeepCool AK400 https://amzn.to/3rrpM6y GPU AISURIX Radeon RX 580 https://amzn.to/3Yf4B4a <p>Note</p> <p>These are all affiliate links. I am not a big affiliate person today, but trying it out.</p> <p>Note</p> <p>I am making some minor updates based on changes that I had made in troubleshooting. I have swapped out what originally was an ASUS Prime B760M motherboard for a Gigabyte B765M DS3H AX. This provides additional networking capabilities (Wifi/Bluetooth and 2.5 Gbps NIC) and an additional M.2 slot. I have also swapped out the GPU and will stick with the new GPU at this point.</p>"},{"location":"desktop-build-2023/#processor","title":"Processor","text":"<p>I was in between the i5 and i7-12700K processor for this. As I look at my systems though, I don't really ever touch the CPU and looking at Passmark on power usage, the i7 (as expected) has a higher TDP, at which point for the amount of idle time that I do expect on the system I stuck with the i5-12600K. It has plenty of speed, and should support what I'm looking to do no issue.</p>"},{"location":"desktop-build-2023/#motherboard","title":"Motherboard","text":"<p>I was originally looking at the most inexpensive 128 GB DDR5 capable motherboard that I could find. In the end I went with a little bit better one to get a 2.5 Gbps NIC, not that I will need it. But its available for the future.</p>"},{"location":"desktop-build-2023/#memory","title":"Memory","text":"<p>This was what I needed. All of my systems that I have are feeling the pinch when it comes to allocation of RAM. The CPUs are not being touched, but the memory is definitely running pretty hot. So this is where I am maxing out the memory with some DDR5-4800, which is what the CPU recommends. I'm not looking to overclock anything, but this works.</p>"},{"location":"desktop-build-2023/#storage","title":"Storage","text":"<p>Nothing too fancy here. Going with a good amount of memory. I utilize SAN connectivity often and will be the case here as well that I will mount a few folders on my SAN to account for anything that starts to seem excessive.</p>"},{"location":"desktop-build-2023/#case","title":"Case","text":"<p>This is where I go small and don't need much. I just need the case to be there to house the gear and protect it. The Cooler Master MasterBox is small enough and should be just what I need.</p>"},{"location":"desktop-build-2023/#power-supply","title":"Power Supply","text":"<p>Similar story as the others, that the system didn't need to be too heavy. So going with something light on the CPU should be just fine for me. I could see a reverse course on this decision at some point.</p>"},{"location":"desktop-build-2023/#cpu-cooler","title":"CPU Cooler","text":"<p>I have seen good reviews on this and my goal is for quiet. Being a bit more of a budget PC yet, that is what I have decided to go with on an inexpensive front.</p>"},{"location":"desktop-build-2023/#graphics","title":"Graphics","text":"<p>Since I'm not doing a ton of gaming, I am likely to either just use the integrated graphics or leverage an existing GPU that I have in the house. With this becoming my primary desktop I will move an NVIDIA card that I already have into the new unit, and take an older card to replace it in my previous desktop that will eventually become another Proxmox node.</p>"},{"location":"desktop-build-2023/#what-will-i-do-with-the-system","title":"What Will I Do With The System?","text":"<p>So what will I be putting onto this system?</p> <ul> <li>Cisco CML</li> <li>With the expanded memory I should be able to run at least one IOS-XR device</li> <li>Will run plenty of Arista / Nexus switches to better simulate a DC</li> <li>Dev Host</li> <li>Possibly some GitHub Actions runners</li> <li>Few other VMs that are OK to be rebooted periodically</li> <li>For those that I want to have more generally available I have a few NUCs that are running that do the same thing</li> <li>Attempting to integrate Libvirt and Proxmox together</li> </ul>"},{"location":"desktop-build-2023/#happy-prime-day","title":"Happy Prime Day","text":"<p>With Prime Day going on, there are plenty of deals to be had with these items. I in fact had purchased a few of these before thinking that they were already a good price. But then Prime Day deals came in even better. Look for more posts to come in the future!</p> <p>Josh</p>"},{"location":"devnet-expert-workstation-on-debian/","title":"DevNet Expert Workstation On Debian","text":"<p>As part of my journey of using my Debian based Dev Workstation, as well as my studies towards completion of the DevNet Expert, I wanted to get up and running with the DevNet Workstation example that would help to become familiar with the environment that would be found at the live exam. There were a few small quirks along the way, so I thought I would go ahead and create a post about how to get started.</p>"},{"location":"devnet-expert-workstation-on-debian/#getting-started","title":"Getting Started","text":"<p>The process will look like the following:</p> <ul> <li>Download the image file</li> <li>Import the image into a new VM</li> <li>Validate the access, check for Internet access</li> <li>Modify the Netplan</li> <li>Restart and enjoy!</li> </ul>"},{"location":"devnet-expert-workstation-on-debian/#download-the-image","title":"Download the Image","text":"<p>The first part of the process is to get the qcow2 file downloaded from the Equipment and Software List in the <code>Candidate Workstation</code> section. Once the file is downloaded, I recommend moving the file into a safe location that is not the Downloads folder. That seems to be a place that tends to get overwhelmed with data and then periodically purged.</p> <p>Once downloaded, then you can create a new VM, I will be using Virtual Machine Manager as a UI to the KVM system. I won't dive into deep details of the under the hood of what is going on, but get into how I am able to get the host available and online.</p> <p>Info</p> <p>I recommend taking a look at Chris Titus's Tech blog (and likely his YouTube video associated with it) for getting Qemu/KVM and Virtual Machine Manager going.</p>"},{"location":"devnet-expert-workstation-on-debian/#import-the-image-to-a-new-vm","title":"Import the Image to a New VM","text":"<p>Now the fun part, which is pretty quick to get going. From the base VMM screen, select new on the upper left hand corner.</p> <p></p> <p>On the new window pop up, then select import existing disk image.</p> <p></p> <p>Browse to the location of the disk, then on the bottom section type out <code>Ubuntu 20</code> to find the OS type of Ubuntu, select Forward.</p> <p></p> <p>Select the resources that you want to make available to the host. This is something that you may want to consider giving a few more to than the default, however Linux OS do seem to handle limited resources quite well.</p> <p></p> <p>On step 4 you get the option to name the virtual machine. Here it is good to go ahead and give it a meaningful name. Be sure to expand the <code>Network selection</code> section, and have this assigned to the bridge interface that is created during the set up of the system for Qemu.</p> <p></p> <p>Once you have completed this, now the machine will boot up. The login based on the docs (and thank you Jeff) is <code>1234QWer!</code>. </p>"},{"location":"devnet-expert-workstation-on-debian/#validate-internet-access","title":"Validate Internet Access","text":"<p>To verify what I had seen previously, I test a <code>ping 1.1.1.1</code> to see if there is Internet access and there is not.</p> <p></p>"},{"location":"devnet-expert-workstation-on-debian/#netplan","title":"Netplan","text":"<p>Taking a look at the network connections I run the command <code>ip add</code> and I get the following output, where the interface name is highlighted on line 8.</p> <pre><code>(main) expert@expert-cws:~$ ip add\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host \n       valid_lft forever preferred_lft forever\n2: enp1s0: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000\n    link/ether 52:54:00:17:69:d0 brd ff:ff:ff:ff:ff:ff\n3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default \n    link/ether 02:42:74:e1:0a:fc brd ff:ff:ff:ff:ff:ff\n    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0\n       valid_lft forever preferred_lft forever\n</code></pre> <p>On this same host, showing the netplan with the command <code>cat /etc/netplan/00-cws-dhcp-config.yaml</code> (tab complete after <code>/etc/netplan</code> gets you the file name) you can see that on line 6 that the default workstation interface name has <code>ens3</code> rather than the <code>enp1s0</code> that you saw on the previous command:</p> <pre><code># Configure ens160 for DHCP\nnetwork:\n    version: 2\n    renderer: networkd\n    ethernets:\n        ens3:\n            dhcp4: true\n\n# Reference: Configuring Static IP address\n# network:\n#     version: 2\n#     renderer: networkd\n#     ethernets:\n#         ens160:\n#             addresses:\n#                 - 10.10.10.2/24\n#             nameservers:\n#                 search: [mydomain, otherdomain]\n#                 addresses: [10.10.10.1, 1.1.1.1]\n#             routes:\n#                 - to: default\n#                   via: 10.10.10.1\n</code></pre>"},{"location":"devnet-expert-workstation-on-debian/#updating-netplan","title":"Updating NetPlan","text":"<p>Now, this is where a little bit of Linux commands come out. The user <code>expert</code> is not in the sudoers list, so you are not able to use <code>sudo</code> to modify the file. However, I found that bringing up the root shell of the root user with <code>su -</code> does in fact get you into the root user prompt.</p> <pre><code>(main) expert@expert-cws:~$ su -\nPassword: \nroot@expert-cws:~# \n</code></pre> <p>With the prompt you see that you are now the root user by the username at the front and the <code>#</code> prompt.</p> <p>Now you can modify the Netplan using vi or nano, or whatever your favorite text editor is from the CLI. While modifying the file, swap out the <code>ens3:</code> for whatever interface name that you found on the <code>ip add</code> command earlier. In this case making the interface name <code>enp1s0</code>. So the output looks like:</p> <pre><code>root@expert-cws:~# cat /etc/netplan/00-cws-dhcp-config.yaml \n# Source: https://netplan.io/examples/\n\n# Configure ens160 for DHCP\nnetwork:\n    version: 2\n    renderer: networkd\n    ethernets:\n        enp1s0:\n            dhcp4: true\n</code></pre> <p>Once the file is saved, now execute <code>netplan apply</code>. You should have no output and bring you back to the same prompt. Now you can ping an address on the internet and get success:</p> <pre><code>root@expert-cws:~# ping 1.1.1.1 -c 4\nPING 1.1.1.1 (1.1.1.1) 56(84) bytes of data.\n64 bytes from 1.1.1.1: icmp_seq=1 ttl=55 time=16.2 ms\n64 bytes from 1.1.1.1: icmp_seq=2 ttl=55 time=15.8 ms\n64 bytes from 1.1.1.1: icmp_seq=3 ttl=55 time=14.8 ms\n64 bytes from 1.1.1.1: icmp_seq=4 ttl=55 time=15.4 ms\n\n--- 1.1.1.1 ping statistics ---\n4 packets transmitted, 4 received, 0% packet loss, time 3005ms\nrtt min/avg/max/mdev = 14.796/15.568/16.226/0.525 ms\n</code></pre>"},{"location":"devnet-expert-workstation-on-debian/#summary","title":"Summary","text":"<p>At this point you now have another device on the network for you to use. You can set up VNC or other system to remote into the environment and use, or just use the Virtual Machine Manager display to work within the workstation. This is something that took me a small bit of time and I was glad to be able to figure out how to accomplish getting the system up and running. What will you do with the DevNet Workstation? Let me know in comments below or on social media!</p> <p>Josh</p>"},{"location":"nautobot-remote-validation/","title":"Nautobot Remote Validation","text":"<p>In this post I'm going to dive into a bit more on the Nautobot custom validators. This is a powerful validation tool that will allow for you to write your own validation capability, including in this demonstration on how to complete a validation against a remote API endpoint. The custom validators are a part of the Nautobot App extension capability. This allows for custom code to be written to validate data upon the <code>clean()</code> method being called, which is used in the majority of API calls and form inputs of Nautobot.</p> <p>I will look to accomplish four different objectives in this post from my point of view. This will help to get some targeted experiences with what I believe the DevNet Expert exam has in mind for Web Services, working with Flask. In my day to day I deal more so within the Django Web Framework to build web applications that are part of the Nautobot ecosystem. So the need to write some Flask applications is a good way to branch out some.</p> <p>The goals that I have for this post:</p> <ul> <li>Creating a Web Services endpoint using Flask</li> <li>Creating multiple endpoints for validation</li> <li>Process the HTTP Request</li> <li>Provide a response</li> <li>Provide additional details around the Nautobot custom validation engine</li> </ul>"},{"location":"nautobot-remote-validation/#nautobot-custom-validators","title":"Nautobot Custom Validators","text":"<p>The Nautobot extensibility features are quite awesome. It is what makes Nautobot a platform worth investing in. The Nautobot Custom Validators is no exception here. The example from the link previous is that a validator may set the requirement that every site must have a region:</p> <pre><code># custom_validators.py\nfrom nautobot.apps.models import CustomValidator\n\n\nclass SiteValidator(CustomValidator):\n    \"\"\"Custom validator for Sites to enforce that they must have a Region.\"\"\"\n\n    model = 'dcim.site'\n\n    def clean(self):\n        if self.context['object'].region is None:\n            # Enforce that all sites must be assigned to a region\n            self.validation_error({\n                \"region\": \"All sites must be assigned to a region\"\n            })\n\n\ncustom_validators = [SiteValidator]\n</code></pre> <p>More information about the details of this can be found on the link provided, including the most up to date information on writing custom validators.</p>"},{"location":"nautobot-remote-validation/#flask-app","title":"Flask App","text":"<p>First the Flask application. In this instance, I am using the Flask extension Flask-RESTX to help in handling a REST API endpoint that I plan on extending in future iterations. After installing Flask and Flask-RESTX into a new Poetry virtual environment I built the following API endpoint that will check that the hostname is all lower case:</p> <pre><code>from flask import Flask\nfrom flask_restx import Api, Resource\n\napp = Flask(__name__)\napi = Api(app)\n\n\n@api.route(\"/validate_name/\")\nclass HelloWorld(Resource):\n    def post(self):\n        \"\"\"Handles POST request.\n\n        Data comes in via api.payload object that can be interacted upon. This is a type dictionary.\n        \"\"\"\n        # Get the data that is coming in, expecting the key of proposed_device_name\n        proposed_device_name = api.payload.get(\"proposed_name\")\n        print(proposed_device_name)\n        print(api.payload)\n\n        if proposed_device_name is None:\n            return {\n                \"valid\": False,\n                \"details\": \"Proposed Name was not sent appropriately\",\n            }, 422\n\n        # Verify the case\n        if proposed_device_name != proposed_device_name.lower():\n            return {\n                \"valid\": False,\n                \"details\": f\"{proposed_device_name} is not all lower case.\",\n            }, 201\n\n        return {\n            \"valid\": True,\n            \"details\": f\"{proposed_device_name} passes validation\",\n        }, 201\n\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n</code></pre> <p>Abstract</p> <p>Lines 17 &amp; 18 are just the debugging code that will print out to the console the details to help understand what is coming in.</p>"},{"location":"nautobot-remote-validation/#testing-flask-endpoints-manual","title":"Testing Flask Endpoints (Manual)","text":"<p>Some manual tests of the Flask endpoint to verify that the data is working as expected get the following results.</p>"},{"location":"nautobot-remote-validation/#good-test","title":"Good Test","text":"<pre><code>curl localhost:5000/validate_name/ -d '{\"proposed_name\":\"goodname\"}' -X POST -H \"Content-Type: application/json\"\n</code></pre> <p>The response:</p> <pre><code>{\n    \"valid\": true,\n    \"details\": \"goodname passes validation\"\n}\n</code></pre>"},{"location":"nautobot-remote-validation/#bad-test","title":"Bad Test","text":"<pre><code>curl localhost:5000/validate_name/ -d '{\"proposed_name\":\"Badname01\"}' -X POST -H \"Content-Type: application/json\"\n</code></pre> <pre><code>{\n    \"valid\": false,\n    \"details\": \"Badname01 is not all lower case.\"\n}\n</code></pre>"},{"location":"nautobot-remote-validation/#returns","title":"Returns","text":"<p>In working with an endpoint, it is imperative from a backwards compatibility perspective that you maintain the keys that you start with. If you are adopting a micro services approach like this with an endpoint that is going to provide data back, then you must maintain some consistency with the keys that you are using. If you all of a sudden change \"valid\" to \"status\", then you are going to have extra work to do. Maintain the keys, and maybe expand the data structure to maintain \"valid\" as a key. You can add more keys into the base of the structure response.</p>"},{"location":"nautobot-remote-validation/#nautobot-custom-validator","title":"Nautobot Custom Validator","text":"<p>Info</p> <p>I have a \"sandbox\" Nautobot App that I use to do tests like this. It is its own standalone plugin that I have built from the structure that built many of the Network to Code sponsored Nautobot Apps. You may want to build a Sandbox app for yourself to work from as well.</p> <p>The Nautobot Custom Validator is quite boring actually compared to the code that is put into the remote API. The code is the following and then I'll explain a few of the core parts:</p> <pre><code>\"\"\"Custom validators.\"\"\"\nimport requests\n\nfrom nautobot.apps.models import CustomValidator\n\n\nclass DeviceValidator(CustomValidator):\n    \"\"\"Custom validator for Device names to be validated remotely.\"\"\"\n\n    model = \"dcim.device\"\n\n    def clean(self):\n        response = requests.post(\n            url=\"http://validator_svc/validate_name/\",\n            json={\"proposed_name\": self.context[\"object\"].name},\n            headers={\"Content-Type\": \"application/json\"},\n        )\n\n        if not response.json().get(\"valid\"):\n            self.validation_error({\"name\": response.json().get(\"details\", \"Error in testing.\")})\n\n\ncustom_validators = [DeviceValidator]\n</code></pre> <p>The validator is going to make an API request out and Python Requests is the best library for single threaded requests, so that is imported. If you do not have the Requests module in your Nautobot environment, you need to make sure to add the Python Requests library to the pip install in the Nautobot environment.</p> <p>Line 10 shows the model that is having the validation applied to. This is required for the application to know what to validate.</p> <p>Within the clean method the first action is to make a POST request to the validator service. This provides a response back as part of the API definition that was set up.</p> <p>Lines 19 and 20 are the components that are required by Nautobot and the custom validator to provide a negative response if there is an error in the validation and to provide a message back. The dictionary key is the field on the form that is having an error. The value in the dictionary is the details to present back to the form.</p> <p>The last line on line 23 assigns a list of the single class to do validation against. This then gets loaded into the validation engine for Nautobot.</p>"},{"location":"nautobot-remote-validation/#how-would-i-do-this-differently","title":"How Would I Do This Differently?","text":"<p>If all I was doing was using case or another field that could be done with Regex, I would look at using the Nautobot Data Validation app. That was built with this in mind and to provide a methodology. The second part I would do would be to not put the validation into a separate application, at least not immediately. With the minimal amount of code, that is something that could have just been done in the validator itself. I completed it this way for demonstration purposes and a start for some personal learning of Flask for my own studies.</p>"},{"location":"nautobot-remote-validation/#summary","title":"Summary","text":"<p>All together, I'm looking at having the Nautobot custom validator as part of my practices as it is needed, but is something that should probably be added sooner than later. </p> <p>To get started with a custom validator for your business logic, create your own Nautobot App, then install it into your environment. You don't need to have new models in order to make a Nautobot App. The best way to do this at the moment is to copy another plugin, such as the Nautobot BGP Plugin. Remove all of the information in the models and then put the code pieces together. That is not a great answer, and there will be more coming soon on this!</p> <p>Josh</p>"},{"location":"nautobot-environment-file/","title":"Nautobot Environment File","text":"<p>Within Nautobot there are many ways to be able to get the Nautobot environment running. Environment variables are used quite a bit in the Docker environment following best practice principles set forth in the 12 Factor App. The use of environment variables is helpful for working through the various stages of an application to production. The installation instructions leverage a single environment variable <code>NAUTOBOT_ROOT</code> and that is set in the SystemD files shown below.</p> <pre><code>/etc/systemd/system/nautobot.service\n[Unit]\nDescription=Nautobot WSGI Service\nDocumentation=https://docs.nautobot.com/projects/core/en/stable/\nAfter=network-online.target\nWants=network-online.target\n\n[Service]\nType=simple\nEnvironment=\"NAUTOBOT_ROOT=/opt/nautobot\"\n\nUser=nautobot\nGroup=nautobot\nPIDFile=/var/tmp/nautobot.pid\nWorkingDirectory=/opt/nautobot\n\nExecStart=/opt/nautobot/bin/nautobot-server start --pidfile /var/tmp/nautobot.pid --ini /opt/nautobot/uwsgi.ini\nExecStop=/opt/nautobot/bin/nautobot-server start --stop /var/tmp/nautobot.pid\nExecReload=/opt/nautobot/bin/nautobot-server start --reload /var/tmp/nautobot.pid\n\nRestart=on-failure\nRestartSec=30\nPrivateTmp=true\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>This is great if there are only a few environment variables, and since SystemD files are only available via the root user, there is some protection.</p>"},{"location":"nautobot-environment-file/#environment-file","title":"Environment File","text":"<p>There is also a method that is supported of using an environment file. The environment file allows for putting several variables into a single file that can then be loaded by the application. Let's start with the file format itself.</p> <p>The file itself is recommended to live at the Nautobot root and be named <code>.env</code>, <code>/opt/nautobot/.env</code>. It really could be named anything, as long as it is known to you and your organization.</p>"},{"location":"nautobot-environment-file/#environment-file-format","title":"Environment File Format","text":"<p>The format is an environment variable per line matching the syntax. So if you want to add to the environment NAPALM credentials for example, then you would have the following in the file, where the <code>#</code> is a comment when loading. You should be creating this file as the Nautobot user:</p> <pre><code># Change to the Nautobot User\nsudo -iu nautobot\n\n# Create the file, you can use Nano or other text editors if you choose.\nvim .env\n</code></pre> <pre><code># /opt/nautobot/.env\n\n# NAPALM Credentials\nNAPALM_USERNAME=my_user\nNAPALM_PASSWORD=what_is_that_password_again\n</code></pre> <p>Note on the environment variables show there are no quotes. You could also put quotes into the environment variable. Use the comment character to help to organize your credentials. So now there is a file with environment items, which may include credentials, now what?</p>"},{"location":"nautobot-environment-file/#file-permissions","title":"File Permissions","text":"<p>It is a good practice to restrict the permissions on the file to that of the Nautobot user. So that only those that can get to the Nautobot user on the system are able to read the file. To update this to being only readable (and editable) to the Nautobot user. So this is executed as the Nautobot user again.</p> <pre><code>chmod 0600 .env\n</code></pre>"},{"location":"nautobot-environment-file/#using-the-environment-file","title":"Using the Environment File","text":"<p>The last step in using the <code>.env</code> file that was created is to now reference that in the SystemD files. Note that you will need to make this change for each of the SystemD files including if using the core docs of <code>nautobot</code>, <code>nautobot-worker</code>, <code>nautobot-scheduler</code>. If there are any other files that you have added as well, you will need to update these. These files should be updated as the root user:</p> <pre><code>sudo vi /etc/systemd/system/nautobot.service\n</code></pre> <pre><code># /etc/systemd/system/nautobot.service\n[Unit]\nDescription=Nautobot WSGI Service\nDocumentation=https://docs.nautobot.com/projects/core/en/stable/\nAfter=network-online.target\nWants=network-online.target\n\n[Service]\nType=simple\nEnvironment=\"NAUTOBOT_ROOT=/opt/nautobot\"\nEnvironmentFile=/opt/nautobot/.env\n\nUser=nautobot\nGroup=nautobot\nPIDFile=/var/tmp/nautobot.pid\nWorkingDirectory=/opt/nautobot\n\nExecStart=/opt/nautobot/bin/nautobot-server start --pidfile /var/tmp/nautobot.pid --ini /opt/nautobot/uwsgi.ini\nExecStop=/opt/nautobot/bin/nautobot-server start --stop /var/tmp/nautobot.pid\nExecReload=/opt/nautobot/bin/nautobot-server start --reload /var/tmp/nautobot.pid\n\nRestart=on-failure\nRestartSec=30\nPrivateTmp=true\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Note</p> <p>It is important to note that there are no quotes/double quotes around the file path. If you put these in, there will be issues. I had invested a fair bit of time the first time troubleshooting why my environment variables are not loading.</p> <p>Once all of the files have been updated, you should complete a daemon reload:</p> <pre><code>sudo systemctl daemon-reload\n</code></pre>"},{"location":"nautobot-environment-file/#loading-the-environment-variables-on-login-of-nautobot","title":"Loading the Environment Variables on Login of Nautobot","text":"<p>Now there are variables in the file that by default do not get loaded. If you try to run <code>source /opt/nautobot/.env</code> then you will not have the proper format to load these. As the Nautobot user, modify the <code>/opt/nautobot/.bashrc</code> file, adding the following to the end of the file (from this gist). The highlighted line the <code>.env</code> file should match what you name the file.</p> <pre><code>set -o allexport\nsource /opt/nautobot/.env\nset +o allexport\n</code></pre> <p>Now whenever you enter the bash prompt for the Nautobot user, then the environment is loaded. This is especially helpful if the database credentials are being controlled via the environment.</p>"},{"location":"nautobot-environment-file/#summary","title":"Summary","text":"<p>I hope this helps out some folks over time on getting rolling with Nautobot and some of the capabilities. I myself have had to research this several times and wanted to get something out to be able to be a reference in order to get additional capabilities going. Let me know in the comments or on a social media link if you found this helpful!</p> <p>Josh</p>"},{"location":"nautobot-get-ip-address-info/","title":"Nautobot: Get IP Addresses From Nautobot","text":"<p>One of Nautobot's primary functions is to serve as an IPAM solution. Within that realm, the application needs to provide a method to get at IP address data for a device, quickly and easily. In this post I will review three prominent methods to get an IP address from Nautobot. It will demonstrate getting the address via:</p> <ul> <li>Nautobot REST API</li> <li>curl</li> <li>Python Requests</li> <li>GoLang HTTP</li> <li>pynautobot</li> <li>Ansible Lookup</li> <li>Nautobot GraphQL API</li> <li>curl</li> <li>Python Requests</li> <li>GoLang HTTP</li> <li>pynautobot</li> <li>Ansible Lookup</li> </ul> <p>Each method I will demonstrate how to get the IP address for Loopback0 on the device <code>bre01-edge-01</code> within the demo instance of Nautobot. This device has 62 interfaces, so being able to filter down to which interface IP address we are looking for makes sense.</p>"},{"location":"nautobot-get-ip-address-info/#getting-an-ip-address-from-the-api","title":"Getting an IP Address From The API","text":"<p>For this set up, the environment variables of <code>NAUTOBOT_URL</code> and <code>NAUTOBOT_TOKEN</code> will be used. Set those with</p> <pre><code>export NAUTOBOT_URL=https://demo.nautobot.com\nexport NAUTOBOT_TOKEN=secretTokenHere\n</code></pre>"},{"location":"nautobot-get-ip-address-info/#curl","title":"curl","text":"<p>The first straight forward method is going to be using the curl application to accomplish the goal.</p> <pre><code>curl -X \"GET\" \\\n  \"$NAUTOBOT_URL/api/ipam/ip-addresses/?device=bre01-edge-01&amp;interface=Loopback0\" \\\n  -H \"accept: application/json\" \\\n  -H \"Authorization: Token $NAUTOBOT_TOKEN\"\n</code></pre> <p>This requires the quotes to be double quotes. Bash and shell prompts will not expand it if it is single quotes.</p> <p>This is the full API response that is provided then:</p> <pre><code>{\n  \"count\": 1,\n  \"next\": null,\n  \"previous\": null,\n  \"results\": [\n    {\n      \"id\": \"77371932-3b7f-4e94-9179-d1d4290695d9\",\n      \"display\": \"10.30.128.1/32\",\n      \"url\": \"https://demo.nautobot.com/api/ipam/ip-addresses/77371932-3b7f-4e94-9179-d1d4290695d9/\",\n      \"family\": {\n        \"value\": 4,\n        \"label\": \"IPv4\"\n      },\n      \"address\": \"10.30.128.1/32\",\n      \"vrf\": null,\n      \"tenant\": {\n        \"display\": \"Nautobot Baseball Stadiums\",\n        \"id\": \"a39f2dd8-84c8-4816-9e6f-4a7c46e91a77\",\n        \"url\": \"https://demo.nautobot.com/api/tenancy/tenants/a39f2dd8-84c8-4816-9e6f-4a7c46e91a77/\",\n        \"name\": \"Nautobot Baseball Stadiums\",\n        \"slug\": \"nautobot-baseball-stadiums\"\n      },\n      \"status\": {\n        \"value\": \"active\",\n        \"label\": \"Active\"\n      },\n      \"role\": null,\n      \"assigned_object_type\": \"dcim.interface\",\n      \"assigned_object_id\": \"6ecee964-e4e0-4a0a-83b7-b7485633fc78\",\n      \"assigned_object\": {\n        \"display\": \"Loopback0\",\n        \"id\": \"6ecee964-e4e0-4a0a-83b7-b7485633fc78\",\n        \"url\": \"https://demo.nautobot.com/api/dcim/interfaces/6ecee964-e4e0-4a0a-83b7-b7485633fc78/\",\n        \"device\": {\n          \"display\": \"bre01-edge-01\",\n          \"id\": \"5e7c0bdd-254b-44cb-bf7c-2f2560082f6d\",\n          \"url\": \"https://demo.nautobot.com/api/dcim/devices/5e7c0bdd-254b-44cb-bf7c-2f2560082f6d/\",\n          \"name\": \"bre01-edge-01\"\n        },\n        \"name\": \"Loopback0\",\n        \"cable\": null\n      },\n      \"nat_inside\": null,\n      \"nat_outside\": null,\n      \"dns_name\": \"edge-01.bre01.mlb.nautobot.com\",\n      \"description\": \"\",\n      \"created\": \"2022-11-09\",\n      \"last_updated\": \"2022-11-09T15:11:51.606550Z\",\n      \"tags\": [],\n      \"notes_url\": \"https://demo.nautobot.com/api/ipam/ip-addresses/77371932-3b7f-4e94-9179-d1d4290695d9/notes/\",\n      \"custom_fields\": {}\n    }\n  ]\n}\n</code></pre>"},{"location":"nautobot-get-ip-address-info/#python-requests","title":"Python Requests","text":"<pre><code>from requests import Session\nimport json\nimport os\n\nurl = f\"{os.getenv('NAUTOBOT_URL')}/api/ipam/ip-addresses/?device=bre01-edge-01&amp;interface=Loopback0\"\n\nsession = Session()\nsession.headers = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Token {os.getenv('NAUTOBOT_TOKEN')}\",\n}\n\nresponse = session.get(url)\nip_address = response.json()[\"results\"][0][\"address\"]\n\nprint(ip_address)\n</code></pre> <p>In this example I've used the requests Session method to store the headers instead of passing it in with requests.get(). They both work, but it is good habit to utilize a session when applicable and especially when making multiple API calls. The output is:</p> <pre><code>\u276f python get_ip_address.py\n10.30.128.1/32\n</code></pre>"},{"location":"nautobot-get-ip-address-info/#golang-http","title":"GoLang HTTP","text":"<pre><code>package main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"io/ioutil\"\n    \"net/http\"\n    \"os\"\n    \"strings\"\n)\n\ntype IPAddress struct {\n    Address string `json:\"address\"`\n}\n\ntype Response struct {\n    Results []IPAddress `json:\"results\"`\n}\n\nfunc main() {\n    nautobotBase := os.Getenv(\"NAUTOBOT_URL\")\n    nautobotToken := os.Getenv(\"NAUTOBOT_TOKEN\")\n    url := fmt.Sprintf(\"%s/api/ipam/ip-addresses/?device=bre01-edge-01&amp;interface=Loopback0\", nautobotBase)\n    method := \"GET\"\n\n    payload := strings.NewReader(``)\n\n    client := &amp;http.Client{}\n    req, err := http.NewRequest(method, url, payload)\n\n    if err != nil {\n        fmt.Println(err)\n        return\n    }\n    tokenString := fmt.Sprintf(\"Token %s\", nautobotToken)\n    req.Header.Add(\"Content-Type\", \"application/json\")\n    req.Header.Add(\"Authorization\", tokenString)\n\n    res, err := client.Do(req)\n    if err != nil {\n        fmt.Println(err)\n        return\n    }\n    defer res.Body.Close()\n\n    body, err := ioutil.ReadAll(res.Body)\n    if err != nil {\n        fmt.Println(err)\n        return\n    }\n\n    // Parse the JSON response\n    var response Response\n    err = json.Unmarshal(body, &amp;response)\n    if err != nil {\n        fmt.Println(err)\n        return\n    }\n\n    // Print the desired value\n    if len(response.Results) &gt; 0 {\n        fmt.Println(\"IP Address:\", response.Results[0].Address)\n    } else {\n        fmt.Println(\"No IP addresses found.\")\n    }\n}\n</code></pre> <p>This provides the same output as seen in the Python version:</p> <pre><code>\u276f go run get_ip.go\nIP Address: 10.30.128.1/32\n</code></pre>"},{"location":"nautobot-get-ip-address-info/#pynautobot","title":"pynautobot","text":"<p>The Python SDK that works to turn the Nautobot API into a Python object you can get the IP address data with the code below. Pynautobot examples are falling under the REST API section at the moment as that is where it fits the best.</p> <pre><code>import os\nimport pynautobot\n\nnautobot = pynautobot.api(url=os.getenv(\"NAUTOBOT_URL\"), token=os.getenv(\"NAUTOBOT_TOKEN\"))\nip_address = nautobot.ipam.ip_addresses.get(interface=\"Loopback0\", device=\"bre01-edge-01\")\nprint(ip_address)\n</code></pre> <p>The execution:</p> <pre><code>\u276f python get_ip_address_sdk.py\n10.30.128.1/32\n</code></pre>"},{"location":"nautobot-get-ip-address-info/#rest-api-ansible","title":"REST API - Ansible","text":"<p>With Ansible, there are two methods available to use. You can use the native URI module that will gather data from the API endpoint. This example is with the Nautobot Ansible collection lookup plugin, which uses pynautobot under the hood. This allows for a little easier methodology of gathering data.</p> <pre><code>---\n- name: \"GET IP ADDRESS FROM NAUTOBOT\"\n  hosts: localhost\n  connection: local\n  gather_facts: no\n  vars:\n    nautobot_url: \"{{ lookup('ansible.builtin.env', 'NAUTOBOT_URL') }}\"\n    nautobot_token: \"{{ lookup('ansible.builtin.env', 'NAUTOBOT_TOKEN') }}\"\n  tasks:\n    - name: \"10: GET IP ADDRESS FROM NAUTOBOT\"\n      set_fact:\n        ip_address: \"{{ lookup('networktocode.nautobot.lookup',\n          'ip-addresses',\n          api_endpoint=nautobot_url,\n          token=nautobot_token,\n          api_filter='device=bre01-edge-01 interface=Loopback0') }}\"\n\n    - debug:\n        msg: \"{{ ip_address['value']['address'] }}\"\n</code></pre> <p>Which gives the following output.</p> <pre><code>PLAYBOOK: get_ip.yml ***********************************************************************************************************************\n1 plays in get_ip.yml\n\nPLAY [GET IP ADDRESS FROM NAUTOBOT] ********************************************************************************************************\nMETA: ran handlers\n\nTASK [10: GET IP ADDRESS FROM NAUTOBOT] ****************************************************************************************************\ntask path: /home/joshv/projects/sandbox-ansible/get_ip.yml:10\nok: [localhost] =&gt; {\n    \"ansible_facts\": {\n        \"ip_address\": {\n            \"key\": \"77371932-3b7f-4e94-9179-d1d4290695d9\",\n            \"value\": {\n                \"address\": \"10.30.128.1/32\",\n                \"assigned_object\": {\n                    \"cable\": null,\n                    \"device\": {\n                        \"display\": \"bre01-edge-01\",\n                        \"id\": \"5e7c0bdd-254b-44cb-bf7c-2f2560082f6d\",\n                        \"name\": \"bre01-edge-01\",\n                        \"url\": \"https://demo.nautobot.com/api/dcim/devices/5e7c0bdd-254b-44cb-bf7c-2f2560082f6d/\"\n                    },\n                    \"display\": \"Loopback0\",\n                    \"id\": \"6ecee964-e4e0-4a0a-83b7-b7485633fc78\",\n                    \"name\": \"Loopback0\",\n                    \"url\": \"https://demo.nautobot.com/api/dcim/interfaces/6ecee964-e4e0-4a0a-83b7-b7485633fc78/\"\n                },\n                \"assigned_object_id\": \"6ecee964-e4e0-4a0a-83b7-b7485633fc78\",\n                \"assigned_object_type\": \"dcim.interface\",\n                \"created\": \"2022-11-09\",\n                \"custom_fields\": {},\n                \"description\": \"\",\n                \"display\": \"10.30.128.1/32\",\n                \"dns_name\": \"edge-01.bre01.mlb.nautobot.com\",\n                \"family\": {\n                    \"label\": \"IPv4\",\n                    \"value\": 4\n                },\n                \"id\": \"77371932-3b7f-4e94-9179-d1d4290695d9\",\n                \"last_updated\": \"2022-11-09T15:11:51.606550Z\",\n                \"nat_inside\": null,\n                \"nat_outside\": null,\n                \"notes_url\": \"https://demo.nautobot.com/api/ipam/ip-addresses/77371932-3b7f-4e94-9179-d1d4290695d9/notes/\",\n                \"role\": null,\n                \"status\": {\n                    \"label\": \"Active\",\n                    \"value\": \"active\"\n                },\n                \"tags\": [],\n                \"tenant\": {\n                    \"display\": \"Nautobot Baseball Stadiums\",\n                    \"id\": \"a39f2dd8-84c8-4816-9e6f-4a7c46e91a77\",\n                    \"name\": \"Nautobot Baseball Stadiums\",\n                    \"slug\": \"nautobot-baseball-stadiums\",\n                    \"url\": \"https://demo.nautobot.com/api/tenancy/tenants/a39f2dd8-84c8-4816-9e6f-4a7c46e91a77/\"\n                },\n                \"url\": \"https://demo.nautobot.com/api/ipam/ip-addresses/77371932-3b7f-4e94-9179-d1d4290695d9/\",\n                \"vrf\": null\n            }\n        }\n    },\n    \"changed\": false\n}\n\nTASK [debug] *******************************************************************************************************************************\ntask path: /home/joshv/projects/sandbox-ansible/get_ip.yml:18\nok: [localhost] =&gt; {\n    \"msg\": \"10.30.128.1/32\"\n}\nMETA: ran handlers\nMETA: ran handlers\n\nPLAY RECAP *********************************************************************************************************************************\nlocalhost                  : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n</code></pre>"},{"location":"nautobot-get-ip-address-info/#graphql","title":"GraphQL","text":"<p>The second methodology, which is the preferred method to get data from Nautobot, as it provides the data you are looking for only, it doesn't get all of the additional data that comes with the REST API calls. The best methodology for discovering the GraphQL query is by using the iQL interface. This is by selecting <code>GraphQL</code> icon on the bottom right of the Nautobot instance.</p> <p>The query that is going to be used here:</p> <pre><code>query {\n  ip_addresses(device:\"bre01-edge-01\", interface: \"Loopback0\") {\n    address\n  }\n}\n</code></pre> <p>On line 2 the query is indicating to search ip_addresses from Nautobot. Then to filter on the device by name and the interface by interface name. You can turn these into variables as well within the GraphQL standards. The returned response is:</p> <pre><code>{\n    \"data\": {\n        \"ip_addresses\": [\n            {\n                \"address\": \"10.30.128.1/32\"\n            }\n        ]\n    }\n}\n</code></pre> <p>Let's take a look at how this is then accomplished with the various methods.</p>"},{"location":"nautobot-get-ip-address-info/#graphql-curl","title":"GraphQL - Curl","text":"<pre><code>curl --location \"$NAUTOBOT_URL/api/graphql/\" \\\n--header \"Content-Type: application/json\" \\\n--header \"Authorization: Token $NAUTOBOT_TOKEN\" \\\n--data '{\"query\":\"query {\\n  ip_addresses(device:\\\"bre01-edge-01\\\", interface: \\\"Loopback0\\\") {\\n    address\\n  }\\n}\\n\",\"variables\":{}}'\n</code></pre> <p>The response is back as expected, just in the printed format.</p>"},{"location":"nautobot-get-ip-address-info/#python-requests_1","title":"Python Requests","text":"<pre><code>from requests import Session\nimport json\nimport os\n\nurl = \"https://demo.nautobot.com/api/graphql/\"\n\nsession = Session()\nsession.headers = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Token {os.getenv('NAUTOBOT_TOKEN')}\",\n}\npayload = {\n    \"query\": \"\"\"\nquery {\n  ip_addresses(device:\"bre01-edge-01\", interface: \"Loopback0\") {\n    address\n  }\n}\n\"\"\"\n}\n\nresponse = session.post(url, json=payload)\nip_address = response.json()[\"data\"][\"ip_addresses\"][0][\"address\"]\n\nprint(ip_address)\n</code></pre> <p>The structure returned by GraphQL is a little bit different than the REST API, but nothing that we can't work through as seen on line 23.</p>"},{"location":"nautobot-get-ip-address-info/#golang","title":"GoLang","text":"<pre><code>package main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"io/ioutil\"\n    \"net/http\"\n    \"os\"\n    \"strings\"\n)\n\ntype IPAddressResponse struct {\n    Data struct {\n        IPAddresses []struct {\n            Address string `json:\"address\"`\n        } `json:\"ip_addresses\"`\n    } `json:\"data\"`\n}\n\n// Define a struct to represent the GraphQL query and its variables\ntype GraphQLRequest struct {\n    Query     string   `json:\"query\"`\n    Variables struct{} `json:\"variables\"`\n}\n\nfunc main() {\n    nautobotUrl := os.Getenv(\"NAUTOBOT_URL\")\n    nautobotToken := os.Getenv(\"NAUTOBOT_TOKEN\")\n    url := fmt.Sprintf(\"%s/api/graphql/\", nautobotUrl)\n    method := \"POST\"\n\n    // Create a GraphQLRequest object with the query and an empty variables object\n    graphQLRequest := GraphQLRequest{\n        Query: `query {\n            ip_addresses(device:\"bre01-edge-01\", interface: \"Loopback0\") {\n                address\n            }\n        }`,\n        Variables: struct{}{},\n    }\n\n    // Serialize the GraphQLRequest object to JSON\n    jsonData, err := json.Marshal(graphQLRequest)\n    if err != nil {\n        fmt.Println(\"Error marshaling JSON:\", err)\n        return\n    }\n    payload := strings.NewReader(string(jsonData))\n\n    client := &amp;http.Client{}\n    req, err := http.NewRequest(method, url, payload)\n\n    if err != nil {\n        fmt.Println(err)\n        return\n    }\n    req.Header.Add(\"Content-Type\", \"application/json\")\n    req.Header.Add(\"Authorization\", fmt.Sprintf(\"Token %s\", nautobotToken))\n\n    res, err := client.Do(req)\n    if err != nil {\n        fmt.Println(err)\n        return\n    }\n    defer res.Body.Close()\n\n    body, err := ioutil.ReadAll(res.Body)\n    if err != nil {\n        fmt.Println(err)\n        return\n    }\n    // Parse the response JSON\n    var ipResponse IPAddressResponse\n    err = json.Unmarshal(body, &amp;ipResponse)\n    if err != nil {\n        fmt.Println(\"Error parsing response JSON:\", err)\n        return\n    }\n\n    // Access the IP address from the response\n    ipAddresses := ipResponse.Data.IPAddresses\n    if len(ipAddresses) &gt; 0 {\n        ipAddress := ipAddresses[0].Address\n        fmt.Println(\"IP Address:\", ipAddress)\n    } else {\n        fmt.Println(\"No IP address found.\")\n    }\n}\n</code></pre>"},{"location":"nautobot-get-ip-address-info/#pynautobot_1","title":"pynautobot","text":"<p>Pynautobot also provides a helper method to be able to make GraphQL queries as well. It returns a JSON object at <code>.json</code> and can be accessed as a dictionary:</p> <pre><code>import os\nimport pynautobot\n\nquery_str = \"\"\"\nquery {\n  ip_addresses(device:\"bre01-edge-01\", interface: \"Loopback0\") {\n    address\n  }\n}\"\"\"\n\nnautobot = pynautobot.api(url=os.getenv(\"NAUTOBOT_URL\"), token=os.getenv(\"NAUTOBOT_TOKEN\"))\nprint(nautobot.graphql.query(query=query_str).json[\"data\"][\"ip_addresses\"][0][\"address\"])\n</code></pre>"},{"location":"nautobot-get-ip-address-info/#graphql-ansible","title":"GraphQL: Ansible","text":"<p>A slight modification to the Ansible playbook, adding a variable at the top for the query string, and using the action module to query instead of a lookup plugin.</p> <pre><code>---\n- name: \"GET IP ADDRESS FROM NAUTOBOT\"\n  hosts: localhost\n  connection: local\n  gather_facts: no\n  vars:\n    nautobot_url: \"{{ lookup('ansible.builtin.env', 'NAUTOBOT_URL') }}\"\n    nautobot_token: \"{{ lookup('ansible.builtin.env', 'NAUTOBOT_TOKEN') }}\"\n    query_str: |\n      query {\n        ip_addresses(device:\"bre01-edge-01\", interface: \"Loopback0\") {\n          address\n        }\n      }\n\n  tasks:\n    - name: \"10: GET IP ADDRESS FROM NAUTOBOT\"\n      networktocode.nautobot.query_graphql:\n        url: \"{{ nautobot_url }}\"\n        token: \"{{ nautobot_token }}\"\n        query: \"{{ query_str }}\"\n      register: \"query_response\"\n\n    - debug:\n        msg: \"{{ query_response['data']['ip_addresses'][0]['address'] }}\"\n</code></pre> <p>With the expected result as seen:</p> <pre><code>PLAY [GET IP ADDRESS FROM NAUTOBOT] ********************************************************************************************************\n\nTASK [10: GET IP ADDRESS FROM NAUTOBOT] ****************************************************************************************************\nok: [localhost]\n\nTASK [debug] *******************************************************************************************************************************\nok: [localhost] =&gt; {\n    \"msg\": \"10.30.128.1/32\"\n}\n\nPLAY RECAP *********************************************************************************************************************************\nlocalhost                  : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n</code></pre>"},{"location":"nautobot-get-ip-address-info/#summary","title":"Summary","text":"<p>There are several ways that you can get IP addresses out of Nautobot. With having the robust API capabilities that Nautobot has, including the GraphQL endpoints, you are able to work quite quickly to get at the data that you need to automate your network, automate your enterprise. Whether it is working with some programming languages, or working with an automation engine such as Ansible, Nautobot is available to help out!</p>"},{"location":"nautobot-how-i-use-tags-for-vms/","title":"Nautobot: How I Use Tags For VMs","text":"<p>In my home environment I am leveraging Nautobot as my source of truth. This is for the network, which is probably not all that interesting in my home environment, and my virtual machines. Why am I tracking my virtual machines in Nautobot? Simple, to help automate them. I think that this is a clever methodology to help use tags and to get automation working within the environment. This same type of thing may be applicable to your network environment as well.</p>"},{"location":"nautobot-how-i-use-tags-for-vms/#my-vm-environment","title":"My VM Environment","text":"<p>I'm still in the process at the moment of having to build my virtual machines a bit manual. I hope to automate this some more in the future yet. I'm leveraging an open source hypervisor that is running on several NUC class hosts that make up the cluster. These are running in HA mode, so if one of the hosts go down, then I should have another host that will pick up the slack and take care of running the virtual machine.</p>"},{"location":"nautobot-how-i-use-tags-for-vms/#nautobot-set-up","title":"Nautobot Set Up","text":"<p>Within Nautobot I am adding my VMs as I add them onto the host. This is what I will automate and flip around. I am setting up the system as though I were building the VM from Nautobot, and then that goes into the hypervisor system.</p> <p>Next up, I use tags to define what application systems that are going to be installed onto the environment. So I am effectively using Tags from Nautobot to create the groups for Ansible. </p> <p></p> <p>The first set of tags that I have include:</p> <ul> <li>app=node_exporter</li> <li>app=plex</li> <li>app=telegraf</li> </ul> <p>As I work through my virtual machines and devices, I now have the capability to run Ansible playbooks based on the tags. So on the virtual machines that I want to deploy the Telegraf agent to send metrics to my TSDBs, I use the tag of <code>app-telegraf</code>. When running the playbook, I have confidence that the machines that I want to have Telegraf will in fact be there.</p>"},{"location":"nautobot-how-i-use-tags-for-vms/#application-to-networks","title":"Application to Networks","text":"<p>How would I look to apply this process to networking? I would absolutely look to have this same thing set up for tags about what devices need to be in what system. Such as the following tags:</p> <ul> <li>Monitoring=SystemA</li> <li>Monitoring=SystemB</li> <li>AAA=RadiusSystemA</li> <li>AAA=TacacsSystemA</li> </ul> <p>Info</p> <p>When I make these tags I also change the slug away from the default value. Changing <code>=</code> which will be no space/character in the slugify to be a <code>__</code> double underscore.</p> <p>By leveraging tags you can filter for devices/virtual machines and allow for customization as of the automation that is being deployed. So that you can define in your automation that devices should all be part of a monitoring system. But what if you want to take the device out of monitoring for an extended period of time? Well, you can just change the tag. Or if you want to apply a custom policy, apply a tag to the object and then in the automation check to see if the tag exists.</p>"},{"location":"nautobot-how-i-use-tags-for-vms/#next-step-automation","title":"Next Step: Automation","text":"<p>The next step for me will be to find the time to set the appropriate WebHook/JobHook for when a device gets updated/created in Nautobot to update the Ansible AWX Inventory that I have. I will likely go to Nautobot JobHook to be able to send updates only when tags are applied. Why JobHook? I can add some logic into the path to be able to handle should there shouldn't there be an update based on the object update.</p>"},{"location":"nautobot-how-i-use-tags-for-vms/#summary","title":"Summary","text":"<p>Tags are an awesome and often forgotten capability with a SOT tool like Nautobot. You can leverage tags quite a bit to help control environments and the data that is being added/updated inside of Nautobot. Anything else that you use tags for? What other ideas do you have? Let me know in the comments.</p> <p>Need a step by step guide on getting started with open source network management tools? Check out my book on LeanPub or on Amazon.</p> <p>Josh</p>"},{"location":"nautobot-secrets-hashicorp-vault/","title":"Nautobot Secrets - Hashicorp Vault","text":"<p>With Nautobot, one of the things that came up was how to work with secrets. Nautobot itself is not the place to maintain secrets, as it is not a vault. There may be some good cryptographic libraries out to handle this, but by its nature, that is not the intent. So Nautobot has written methods to be able to retrieve secrets from proper vault sources and be able to leverage them. These can be tricky to get set up however. I had struggled for a while myself. So now that I have it working, I thought it would be a good time to have a quick personal blog about it.</p>"},{"location":"nautobot-secrets-hashicorp-vault/#secrets-set-up","title":"Secrets Set Up","text":"<p>In my writing within the Open Source Network Management book I showed how to get started with Hashicorp Vault for secrets within Ansible. This is a natural progression to house my secrets for my Nautobot recommendation as well. This will not cover the set up of Nautobot for secrets. That is best to be done by the provider documentation. This will however dive into how the vault is set up in my environment, and how that translates into Nautobot.</p>"},{"location":"nautobot-secrets-hashicorp-vault/#nautobot-parameters","title":"Nautobot Parameters","text":"<p>When setting up a Hashicorp Vault secret in Nautobot, you will need the following parameters:</p> <ul> <li>Path</li> <li>Key</li> <li>Mount point</li> <li>Kv version</li> </ul> <p></p>"},{"location":"nautobot-secrets-hashicorp-vault/#nautobot-parameters-vault-mount-point","title":"Nautobot Parameters: Vault Mount Point","text":"<p>So the vault that I have set up inside of Vault, is a KV store. In the image there is the <code>cubbyhole</code> which is used for local passwords, so I wouldn't use that for my vaults. The name of <code>kv</code> below will be the first item into the Nautobot secret and will map to the <code>Mount point</code> with Nautobot. The default is <code>secret</code>, and this needs to change to <code>kv</code> if that is what you have.</p> <p></p>"},{"location":"nautobot-secrets-hashicorp-vault/#nautobot-parameters-vault-path","title":"Nautobot Parameters: Vault Path","text":"<p>Next down the folder path on Hashicorp Vault is the folder/path. When you navigate into the kv link you then get the next item of the path, in this case <code>net_device</code>. This is the <code>Path</code> within the parameters form.</p> <p></p>"},{"location":"nautobot-secrets-hashicorp-vault/#nautobot-parameters-key","title":"Nautobot Parameters: Key","text":"<p>The last part you need within your Nautobot secret is the Key itself. So in the secret within Vault, you have various keys. These line up with the <code>key</code> within the parameters of the Nautobot secret.</p> <p></p> <p>Note</p> <p>It's worth noting that each key in the vault secret is its own entity. That being that you will need to set up a Nautobot Secrets Group to pair the username and password together. You need to set up a secret for both a username and password if you are having a username/password combination that is often the case.</p>"},{"location":"nautobot-secrets-hashicorp-vault/#testing-the-secret","title":"Testing the Secret","text":"<p>One of my favorite features of the Nautobot Secrets Provider is that there is the opportunity to \"test\" the secrets gathering. Once the secret is created you can find the <code>Check Secret</code> button in the upper right.</p> <p></p> <p>When you click that button and all is well you get the successful message back.</p> <p></p>"},{"location":"nautobot-secrets-hashicorp-vault/#alignment-table","title":"Alignment Table","text":"<p>Here is what the Nautobot terminology lined up:</p> Nautobot Parameter Hashicorp Vault Path Folders inside of the KV store, including the secret name itself Key Key within the secret Mount point Name of the KV store Kv version Version of the Key/Value store"},{"location":"nautobot-secrets-hashicorp-vault/#summary","title":"Summary","text":"<p>Nautobot is handling secrets right in my opinion. That secrets are not to be stored within Nautobot. These are things that there are great solutions already available for. Use those tools. Nautobot provides a good mechanism to be able to integrate with various secrets providers. This hopefully helps out someone get started with using Hashicorp Vault secrets and Nautobot. I know I will be using this post again in the future some day!</p> <p>Let me know on social media (until I get a comment system built in) what your thoughts are. </p> <p>Josh</p>"},{"location":"nornir-brief/","title":"Poetry Fix","text":"<p>The Python Poetry is our go to package management system thus far, you can see that in all of the Python projects that Network to Code open sources, such as Nautobot, pyntc, network-importer, and NTC-Templates. Lately though, I've been having some challenges when my HomeBrew updates happen and my system Python gets updated. I've been able to recover with the help of the same few pages I land on from my Google searches. But since I've done this twice now, I'm using this post to document the fix as much as for myself, but for anyone else that may come across Poetry issues.</p>"},{"location":"nornir-brief/#the-issue","title":"The Issue","text":"<p>The issue occurs when I'm updating the system Python through HomeBrew. I start to see errors where Poetry is unable to be detected, with some nasty looking MacOS shell issues. Such as:</p> <pre><code>poetry Library not loaded: /opt/homebrew/Cellar/python@3.10/3.10.12/Frameworks/Python.framework/Versions/3.10/Python\n</code></pre> <p>With this error, I made the mistake (maybe) of just removing <code>poetry</code> by finding the location with <code>which poetry</code> and then removing that file. Not recommended.</p>"},{"location":"nornir-brief/#the-fix","title":"The Fix","text":"<p>The proper way that I'm finding to fix this is to run the Poetry uninstall scripts, which are found on the Poetry docs.</p> <pre><code>curl -sSL https://install.python-poetry.org | python3 - --uninstall\ncurl -sSL https://install.python-poetry.org | POETRY_UNINSTALL=1 python3 -\n</code></pre> <p>Once it is removed, you try to re-install Poetry, but then you get the following symlinks issue:</p> <pre><code>raise Exception(\"This build of python cannot create venvs without using symlinks\")\nException: This build of python cannot create venvs without using symlinks\n</code></pre> <p>This is coming from a pyenv set up that I've also been convinced to run (however, I'm not convinced). So the next step is instead of installing to <code>python3</code>, you install to the current minor version of Python:</p> <pre><code>curl -sSL https://install.python-poetry.org | python3.10 -\n</code></pre>"},{"location":"nornir-brief/#my-preferred-set-up-for-various-versions-of-python","title":"My Preferred Set Up For Various Versions of Python","text":"<p>My preferred method of running different versions of Python is the use of containers. Then you have a fully isolated system rather than running different versions within your shell. Allow the system Python to be that, the system. Then use containers to handle the different versions.</p>"},{"location":"nornir-brief/#summary","title":"Summary","text":"<p>This is my fix thus far that I have found. If there is a better way of getting Poetry to work with pyenv (until I uninstall it), I'd love to hear what the solutions are. Until next time, happy automating!</p> <p>Josh</p>"},{"location":"slack-canvas/","title":"Slack Canvas","text":"<p>Newly released (at some point anyway) is Slack Canvas, what looks to be a little bit of on demand wiki, collaboration space, and possibly (based on marketing materials) workflow organizer. This came to light as a \"pop up\" when I went into a Slack window on my mobile. Being a curious person and someone that is willing to try out new things I jumped right in.</p>"},{"location":"slack-canvas/#the-take","title":"The Take","text":"<p>Without reading any marketing pages or anything else, I just started to do a few things with it. The first look at the canvas I immediately attempted to write with Markdown. I instinctively tried making some headings, and it actually worked, I was very surprised as I would have thought that the canvas would remain a very Slack flavor of Markdown, which hasn't really worked in the past. But the headings came right in and it even provides a collapsible menu for the headings.</p> <p>Next up was to try to make bold, italic, and ~~strikethrough~~ formatting. It followed the Slack flavor Markdown here. So that will continue to be a switch up from the standard GitHub flavored Markdown that so many are accustomed to.</p> <p>One of the features that I'm liking is the ability to have quick access to comments within the Canvas for any section, paragraph, item added. This seems very natural.</p> <p>I then went and checked out the marketing material around the feature, take a look for yourself here. I then wrote the majority of this blog post and looked at the getting started. The following quote kind of concerns me a small bit:</p> <p>All existing posts will eventually be converted to canvases, and all of your content will be saved. On free teams, your posts will be converted to read-only canvases. You can continue to edit your existing posts until they are converted. </p> <p>If I'm reading the block quote there properly, all posts are going to move to a Canvas for each message? I guess time will tell here.</p>"},{"location":"slack-canvas/#confluence","title":"Confluence?","text":"<p>This is all giving a bit of feel to having Confluence pages. Where you are able to collaborate in real time with others, leave comments, and make for good pages that can be consumed in a Wiki format. The difficult thing with Confluence is that it is a bit clunky to go between pages. You are relying on the web page loading, which has some overhead. The speed at which Canvas pages are able to load and then render text is pretty smooth within the Slack application itself.</p>"},{"location":"slack-canvas/#down-sides","title":"Down Sides?","text":"<ul> <li>Yet another place to store wiki like content</li> <li>Escape Key does not close out the Canvas window.</li> </ul>"},{"location":"slack-canvas/#am-i-going-to-use-it","title":"Am I Going to Use It?","text":"<p>I'm going to give it a try. I like having it in the messaging app, almost the new universal user interface. I believe in this concept of having work within collaboration/messaging apps. ChatOps is another example within Slack that is providing network automation to users. Everyone is in messaging apps all day, every day. So this is a good feel to have.</p> <p>Thoughts on Slack changes; maybe thoughts on having my hot takes on these? Let me know.</p> <p>Josh</p>"},{"location":"slack-power-keyboard/","title":"Slack Power Keyboard","text":"<p>Recently Slack started introducing a new UI that is bringing back the importance of knowing keyboard shortcuts. Keyboard shortcuts within Slack are immensely helpful in my day to day, and it is essential for me being able to keep up with what is going on within my organization and some of the other 20 Slack organizations that I have joined. So much so, I don't believe that once I am in the Slack UI on my machines, I am ever touching the mouse to get from one place to another. About the only time I may need to use the mouse is to scroll back in the thread, so I'm not even clicking when using the mouse.</p> <p>Note</p> <p>The keyboard shortcuts that I will be describing here will be demonstrated with <code>CMD + &lt;keyboard key&gt;</code>. These are for the Mac keyboard. If the Linux keyboard shortcuts of using the control key, should be the equivalent on the Windows platform as I understand it. If I am off on these keys, please let me know in the comments.</p>"},{"location":"slack-power-keyboard/#my-common-keyboard-shortcuts","title":"My Common Keyboard Shortcuts","text":"<p>Slack makes the keyboard shortcuts available via <code>CMD + /</code>. This gives you the full list of commands that are available. </p>"},{"location":"slack-power-keyboard/#navigating-to-channels-unread-channels","title":"Navigating To Channels - Unread Channels","text":"<p>In my typical scenario I'm navigating between channels within an organization. The first thing that I'm doing is using <code>CMD + K</code> to find my unread channels. It gives a really nice list of the channels that are unread right at the top and in bold. You can then arrow down to those unread channel that you wish to look at and then press <code>enter</code> in order to get to the channel.</p> <p>Here this has everything in an unread state, but you can see where you can start to navigate. If there were some channels that had not been read yet, they would appear here in bold.</p> <p></p>"},{"location":"slack-power-keyboard/#navigating-to-channels-just-getting-to-channel-name","title":"Navigating To Channels - Just Getting to Channel Name","text":"<p>The next step if you need to get to a particular channel, I'm following the similar start to the unread channels. I'm using <code>CMD +  k</code> to bring up the window again, but now the default is a search bar. This now allows you to just start typing. This is where I type the name of the channel that I'm looking for and then selecting it with the enter key again once it is highlighted.</p>"},{"location":"slack-power-keyboard/#navigating-to-a-direct-messages-dms","title":"Navigating to a Direct Messages (DMs)","text":"<p>The direct message interface is a little different on the channels. If I'm looking to create a new DM conversation with someone, I'm using the combination of <code>CMD + Shift + k</code> to get to the DM window. This will then provide you the ability to type various contacts to get to the DM that you wish to complete.</p> <p>This now brings up one of the newest quirks of the new Slack interface. You are taken to a whole new view. Once I am complete with my DM session that I started, I'm now trying to get back to the home view. On the Mac side of things this is accomplished with <code>Control + 1</code> to get to that view. The Linux/Windows side is a bit more complicated. There you need to use <code>Control + Shift + 1</code> to get to the home view.</p>"},{"location":"slack-power-keyboard/#workspace-switching","title":"Workspace Switching","text":"<p>This is another quirky experience that we will dive into. The new UI has hidden the workspace switcher by default. If you wish to have the workspace switcher open for whatever scenario (like you just like them, or you have too many to use keyboard shortcuts), you can do so on Mac with <code>CMD + Shift + s</code> and on the Linux/Windows world with <code>Control + Shift + s</code>. This will allow you to change workspaces via the mouse again.</p> <p>But this post is about my keyboard shortcuts. The workspace switching capability has some good shortcuts. This can be accomplished using <code>CMD + #</code> where <code>#</code> is the workspace number, similar to a tab experience on your web browsers. Here you can only go to 9 workspaces before you need to use a mouse, so this is where re-ordering workspaces is important.</p>"},{"location":"slack-power-keyboard/#summary","title":"Summary","text":"<p>Keyboard shortcuts are what empowers me to move through the day and keep up quite quickly in the Slack workspaces these days. I hope that some of this information may help you as well. Are there some other keyboard shortcuts that you are using in your day to day interaction with Slack? Start a conversation in the discussions.</p> <p>Josh</p>"},{"location":"upgrade-nautobot-python-virtual-machine/","title":"Upgrade Nautobot Python Version in Virtual Machine","text":"<p>One observation lately is that Python is moving along quickly with new versions and new EOLs. Along with needing to make these updates, the applications that Python uses will also need to be moving along. Nautobot is my favorite, and in my opinion the best SOT platform available in the open source ecosystem today. So let's dive into the updating of the Python version.</p> <p>For this post, I've created a new Rocky 8 Virtual Machine to be the host. See the note below for the reasoning. This will start off with a Nautobot install from the Nautobot docs. I won't dive into all of that, assume that is the starting point with a fresh Nautobot application.</p> <p>I originally thought just doing a DNF update on the host that I would be able to use Python 3.8 or something of that nature. I was wrong. Running the DNF update put Python3.11 on the host. So now it is on to removing Python3.11 and move to 3.8 for this post. I completed this step with a <code>sudo dnf install python3.8</code></p> <p>Note</p> <p>It is a strong recommendation to not have to follow this process. In that you should be looking to be using infrastructure that can be destroyed and recreated. Such as with Docker containers or having the build process to be able to replace virtual machines as needed.</p>"},{"location":"upgrade-nautobot-python-virtual-machine/#my-original-challenges-with-upgrading-python","title":"My Original Challenges With Upgrading Python","text":"<p>Going back a few years I had the experience that was terribly painful to update Python versions on RHEL 7 and its derivatives. All of the documentation that I had found required to install Python from source, that you were not able to use DNF/YUM to just get the latest version of Python. Thankfully since RHEL8 and its derivatives, it is much simpler. Now DNF just is able to install and off we go.</p>"},{"location":"upgrade-nautobot-python-virtual-machine/#explaining-the-upgrade-path","title":"Explaining the Upgrade Path","text":"<p>If you followed the instructions on the Nautobot installation docs for your virtual machine build, this will become pretty quick. The installation method has a virtual environment created. So in order to update the version of Python, with the help of the Python VENV capabilities, it is a short process.</p> <ol> <li>Install the desired version of Python on your system (<code>sudo dnf install python3.11</code> or <code>sudo apt install python3.11</code>)</li> <li>Verify the Python command to run the latest version (<code>python3 -V</code>)</li> <li>Make a backup of the requirements and the directory</li> <li>Remove the existing virtual environment files</li> <li>Recreate the virtual environment with the new version of Python</li> <li>Re-install the Python packages</li> <li>Restart the Nautobot services</li> </ol>"},{"location":"upgrade-nautobot-python-virtual-machine/#the-upgrade","title":"The Upgrade","text":"<p>The process will begin with saving the current pip freeze requirements to a tmp file. THis is going to provide a quick method to re-install the Python packages required for your Nautobot instance.</p> <pre><code>pip freeze &gt; /tmp/nautobot_requirements.txt\n</code></pre>"},{"location":"upgrade-nautobot-python-virtual-machine/#remove-bad-requirement","title":"Remove Bad Requirement","text":"<p>The <code>backports.zoneinfo</code> requirement that gets generated is deprecated and within Python3.9 should not be used. To accommodate for this remove any reference to backports.zoneinfo from the <code>tmp/nautobot_requirements.txt</code> file.</p>"},{"location":"upgrade-nautobot-python-virtual-machine/#create-a-backup-of-directory","title":"Create a Backup of Directory","text":"<p>With the file in place, now let's move the copy the nautobot user directory to another directory. Most likely this will not be required, but if you need to do a restore of the directory having a backup is good practice.</p> <pre><code>[user@rocky-nautobot ~]$ sudo cp -a /opt/nautobot /opt/nautobot-old\n</code></pre>"},{"location":"upgrade-nautobot-python-virtual-machine/#remove-existing-virtual-environment","title":"Remove Existing Virtual Environment","text":"<p>Now to remove the existing Nautobot virtual environment by removing the bin directory (<code>/opt/nautobot/bin</code>).</p> <pre><code>sudo -u nautobot rm -rf /opt/nautobot/bin\n</code></pre>"},{"location":"upgrade-nautobot-python-virtual-machine/#create-new-virtual-environment","title":"Create New Virtual Environment","text":"<p>Create a new virtual environment into <code>/opt/nautobot</code> by using the same command from the install instructions.</p> <pre><code>sudo -u nautobot python3 -m venv /opt/nautobot\n</code></pre> <p>Now log in as the Nautobot user to update the <code>pip</code> and <code>wheel</code> packages. And from there install the rest of the requirements.txt items that were saved off earlier.</p> <pre><code>sudo -iu nautobot\npip install --upgrade pip wheel\npip install -r /tmp/nautobot_requirements.txt\n</code></pre>"},{"location":"upgrade-nautobot-python-virtual-machine/#restart-services","title":"Restart Services","text":"<p>The last step is to restart the services for Nautobot, which will then pick up the new Python version.</p> <pre><code># Exit out of the Nautobot user\nexit\n\nsudo systemctl restart nautobot nautobot-worker nautobot-scheduler\n</code></pre>"},{"location":"upgrade-nautobot-python-virtual-machine/#summary","title":"Summary","text":"<p>While the recommended method for upgrading the Nautobot application is to create a new virtual machine, migrate the database over, and reclaim the previous host, that may not always be a viable option. This process is just tested out on my lab environment and has not been done in a production environment yet, but this should work.</p>"},{"location":"google-earth-golf/","title":"Using Google Earth for Golf","text":"<p>I'm going to diverge a small bit from the straight network automation space that I have blogged about primarily and dive a small bit into the world of using Google Earth to help prepare for your golf game. Upcoming, I'm playing in a Minnesota Golf event at two courses in late August. I'm going to put together a green book for myself and figure this would be a great topic to touch on how I'm going about this activity.</p>"},{"location":"google-earth-golf/#requirements","title":"Requirements","text":"<p>The first requirement is to have Google Earth installed on a desktop environment. This may be something that you can use the web version for, but I have had better luck using Google Earth Pro installed onto the desktop.</p> <ul> <li>Google Earth Pro </li> <li>GIMP</li> </ul> <p>The second requirement is a document (Microsoft Word, Google Doc, LibreOffice Writer) to build the content on. For this work I'm at least starting out with using Ubuntu 23 as my operating system. I will attempt to provide the same screen shot capabilities and editing for the Mac and Windows platforms as well.</p>"},{"location":"google-earth-golf/#getting-started-with-google-earth","title":"Getting Started with Google Earth","text":"<p>There are two main parts that I use when building my green books using Google Earth. The first is the ruler. This is how I am able to measure distances between points on the hole. The second piece is on the bottom right, there are some numbers to give you some more details, which I particularly focus on the <code>elev</code> which tells you what the elevation is when you hover over it..</p>"},{"location":"google-earth-golf/#mapping-the-course","title":"Mapping the Course","text":"<p>The first thing is to search for the golf course within Google Earth. In the search box on the upper right I am typing in the first course, <code>Chaska Town Course</code>. This will take you from the globe view down to the golf course. Then you need to be able to find the first hole to start taking a screen shot for each hole. This may take some more investigation if you have never played the course before. Look for practice ranges and practice greens which are likely to be around the first tee. Also, if you are able to find a scorecard with the distances of each hole, you can get an idea using the ruler about which hole is which.</p> <p></p> <p>Once you have the hole, it is time to zoom in on just the hole so you can get a good screenshot. For making a green book, use the navigation on the upper right to change the orientation (although you may want to keep the orientation for understanding the wind). Once you have the orientation the way that you like it, take the screen shot.</p> <p></p> <p></p> <p>Now take that screen shot and place it into an image editor such as GIMP which is available for Windows, Mac, and Linux. </p>"},{"location":"google-earth-golf/#measuring-distances-to-obstacles-tee-shot-expectations","title":"Measuring Distances to Obstacles / Tee Shot Expectations","text":"<p>The ruler on Google Earth is an amazing option to work with. This provides as you expect a measurement between two points. Now for the golf side of things, off of the tee you may want to make either one measurement from the middle of the expected tees, or a couple of measurements from the front and back of the tee boxes. This gives you the starting point of what to know once you are on the course. Measuring from the center of the tee box you will need to approximate where things are. I'm going to measure from the back of tee boxes, since often I will likely walk past the point and my pace of yards is known and I can get an understanding of how the course is playing from there.</p> <p></p> <p>With the ruler, you click once to set the first point, then click again to set the end point. The ruler itself then has a pop up window that provides the metrics, including the length of the line, and the heading at which the line is on. Measure the distance from your point to the various data points on the course. On the first hole at Chaska Town Course, I will map the distance to the bunker on the left hand side, it measures in the neighborhood of 274 yards from the back of the back most tee box to the sand trap, and 296 yards to the back of the sand trap. I may make two measurements, one from the most far back tee box, and one from the back of a middle tee box. This way there are two measurements to keep the pace of play moving in case one of the tees is at a location that is not expected.</p> <p></p> <p></p> <p>I'm not worrying about yardages from the objects to the green, except maybe on some par 5 holes. But once on the course and I've hit my tee shot, I will have a different distance to each area.</p>"},{"location":"google-earth-golf/#measuring-elevation","title":"Measuring Elevation","text":"<p>Now that there are some yardages off the tee, comes the part that you don't get from mobile golf apps today. You now want to plot the elevation. To do this, I hover my mouse over various points along the hole to give an idea of the elevation. And most importantly, I check out the 9 points of the green. I make a box around the green with the center elevation, elevation on the top left, top center, and so on. This is helpful to understand the general slope of the green and which way the ball is likely to break.</p> <p></p> <p>Abstract</p> <p>The USGA app itself also offers green books that have much more details slopes on them. There is an annual cost for this green book and I am one that is supporting technology innovations by the USGA. I'm glad to show interest and pay them a bit of revenue in order to continue to develop technology advances for golf. Android Apple</p> <p>Off of the tee is where you are likely to need this, as well as understanding various points along the way. So this way you know if you are hitting into an elevated green or not. Elevation changes do generally impact the distance that clubs go. So I'm taking the elevation at the tee box, the general fairway/rough areas, and since I've done this for the green book I then know if I'm going to be uphill or downhill to the hole.</p> <p>All of these are manually entered into the GIMP editor. I'm using one color for yardages to the obstacles and another for the elevation numbers. I'm also going to put the two measurement points with an X to make sure to know where the two measurements are sourced from.</p>"},{"location":"google-earth-golf/#final-look","title":"Final Look","text":""},{"location":"google-earth-golf/#putting-all-together","title":"Putting All Together","text":"<p>Once you have completed building your images of each hole, now you add the images to your document editor. This can be set up for however you may like size wise. You may need to work on how many pages you may want per page and the sizing that you would be looking for. I would be looking to put 6 holes on a page to be able to make the appropriate updates.</p>"},{"location":"google-earth-golf/#summary","title":"Summary","text":"<p>This is not the most professional of green books out there. But then again, I'm not getting paid to play the game of golf either (at least not today). This is a way to look at some of the many great tools out there that are generally available for you to be able to leverage in your day to day. It may not be built specifically for this purpose, but another great use.</p>"},{"location":"workstation-troubleshooting-2023/","title":"Workstation Troubleshooting 2023","text":"<p>In my previous post I wrote about a workstation that I was working on building. It took an incredibly long time to get up and into a stable environment. But I have finally accomplished stability (hoping to not jinx it here with the post). I went through a fair bit of troubleshooting to get to this point.</p>"},{"location":"workstation-troubleshooting-2023/#symptom","title":"Symptom","text":"<p>The symptom that was having instability was that the system would freeze randomly. There was not a particular application or otherwise that would be point me to an application that was causing the failures. The system would just freeze overnight or at the start of getting into the desktop UI.</p> <p>I first tried multiple versions of Linux to see if there was a flavor of Linux that was causing the issue. To no avail. I tried:</p> <ul> <li>Ubuntu 23</li> <li>Fedora 38</li> <li>PopOS 22.04</li> <li>Debian 12</li> </ul> <p>All of these had some sort of failure that was occurring within the system. Debian 12 wouldn't even complete it's full installation.</p> <p>Of these, PopOS did seem to work the longest. This was encouraging, but I was really interested in getting to Gnome 44, which has some excellent polish in the UI.</p>"},{"location":"workstation-troubleshooting-2023/#hardware-testing","title":"Hardware Testing","text":"<p>First up in the testing was to do some tests of the hardware. I went with doing a memory stress test using MemTest86+. With all 4 sticks of RAM in the system I received some errors on test #6 pretty quickly. So I was thinking that the next test would be to run tests on each individual stick of RAM. This test showed everything as clear. So I loaded all 4 sticks back into the system, and re-ran test #6 that gave errors pretty quickly. That was clear this time. Back to the OS! But then the freezing continued.</p> <p>Next up I had happened to pick up a second nVME SSD during Amazon's Prime Days deals. This just happened to arrive after being placed on backorder instantly (that is another interesting quirk). So I go to install the nVME and get started installing. But the freezing kept happening.</p> <p>Next up was since I was going down the path of Fedora and the possible challenges (unfounded claim) that Nvidia drivers could be causing issues, I ordered a new GPU. Put this into the system and still no change.</p> <p>What I was really impressed on the swapping of the GPU was how quickly that Fedora was able to just pick this up and get moving. I just booted up and bam I had my UI going. Nothing to have to work through.</p> <p>At this point, I am suspecting that maybe the motherboard would be my next stop on the troubleshooting chain. I decided to go to PC Parts Picker to look at what else would be compatible with the processor and RAM choices. I found the Gigabyte B765M DS3H AX would work, and it comes with some added benefits of having wireless (bluetooth) and a 2.5 Gbps NIC. I ordered up this motherboard and got to it the same night with completing the motherboard swap. Interestingly enough, it also includes a second M.2 interface that I was able to install both nVME drives that I have.</p> <p>Instantly things just felt better in the OS. Fedora was able to be installed smoothly and without any hesitation. The system BIOS already had XMP disabled, which was a recommendation that I had seen in some other posts. And the hardware was all detected.</p> <p>Fedora ran through the night at this point. Once waking up to get started with the day it felt good to just have the Dev Workstation up and running.</p>"},{"location":"workstation-troubleshooting-2023/#fedora-choice","title":"Fedora Choice","text":"<p>I'm going with Fedora at the moment for a couple of reasons. First, on top of being a solid, trusted OS, it is running the latest kernels and latest UI. I'm impressed with where Linux is at. Secondly, I just need to get better. In the world of Network Automation, as much as it is easy to just get moving with Ubuntu, I need to be able to work within the world of Fedora. So this will hopefully work out well.</p>"},{"location":"workstation-troubleshooting-2023/#dual-boot","title":"Dual Boot","text":"<p>One of the interesting things that I didn't think about with the second drive, is that I have an easy method to set up a dual booting system. Every time my workstation reboots at this point I am presented with a choice of a previous PopOS install or the Fedora install. I am continuing to run Fedora, but it is nice to know that I have the option.</p>"},{"location":"workstation-troubleshooting-2023/#summary","title":"Summary","text":"<p>So far it was an experience that I wasn't looking to have, but I did get to experience desktop hardware troubleshooting. It's interesting space that I hadn't had the opportunity to do for a while. Just the timing of this could have been better. Well, time to return that old motherboard.</p> <p>Josh</p>"},{"location":"nautobot-app-cookie/","title":"Nautobot App Baking Cookies","text":"<p>Just recently released at the beginning of 2024 is a project that I am super excited to see in the open source by Network to Code. This is the Nautobot App cookiecutter template. This may already be the biggest thing to become available for Network Automation in 2024. I know, its fresh at this point in the year, but this is something that is going to make getting started with your own Nautobot Application so much quicker.</p>"},{"location":"nautobot-app-cookie/#cookiecutter","title":"CookieCutter","text":"<p>First, a real quick summary and link to more documentation of what is a CookieCutter? Well, just like in making cookies with fancy designs, this is a template that will generate the entire project layout for you for a Nautobot App. Using Python CookieCutter, you tell Python to build a directory structure/layout according to the template defined. </p>"},{"location":"nautobot-app-cookie/#nautobot","title":"Nautobot","text":"<p>The first stop for getting started in your Network Automation journey should be to get Nautobot up and running. Get a Linux Virtual Machine, preferably Ubuntu or RHEL/RHEL derivative. From there you can follow the Nautobot installation instructions for your flavor of Linux. You should look to get a few users set up and then you can work to get your existing environment into Nautobot via the Onboarding Plugin followed by Network Importer to get the existing environment into Nautobot. This is the easiest path for getting your network ready to go. Or if you do not have a supported device type from the Network Importer process, then I have a process that leverages Ansible Facts to get the information from the network into Nautobot.</p> <p>Next step now that you have data and an inventory? My recommendation is to build a Nautobot App that you can use to install your custom code and data into Nautobot quickly and efficiently.</p>"},{"location":"nautobot-app-cookie/#why-the-nautobot-app","title":"Why the Nautobot App","text":"<p>So why would I be looking at adding my own Nautobot App? First, it is not to recreate something that already exists. So it is not to write code to audit your network configuration. There is already a proven Nautobot App that does this for you, Nautobot Golden Config. Also, it is not to do something that you could contribute back to an open source project. It is meant to help you build, enhance, and enforce your organization's business logic. </p> <p>Nautobot Jobs are a perfect place to centralize your Python scripts that may be on a single developers workstation. To that end, if there is a script that executes business logic, or solves a problem that can then be used by a help desk team member or an application team, this will help to make your organization more productive.</p> <p>Also, in an earlier post on Custom Validators I covered how to apply custom validators from Nautobot on the data. In particular that post showed how you can interact with other 3rd party systems to Nautobot in order to do validation based on other services.</p>"},{"location":"nautobot-app-cookie/#summary-and-next-posts","title":"Summary and Next Posts","text":"<p>Upcoming posts are going to get into the whats next. Where I will look at the following questions and more.</p> <ul> <li>Exploring the layout of the baked cookie</li> <li>How do you install this application into your environment?</li> <li>How to get started with Nautobot Jobs</li> <li>Nautobot Custom Validators</li> <li>Creating Your Own Views</li> </ul> <p>What else would you like to see as you get started with Nautobot and your own app?</p> <p>-Josh</p>"},{"location":"firewall-tables/","title":"Automation Redux: Firewall Tables","text":"<p>Today I'm going to dive into my getting started with network automation, and perhaps my first successful automation. There are definitely some things that I would re-do and complete differently, and some things that I consider a success.</p> <p>I'm working on a new series within my blog, about how I would look to have done things differently than I had done before, with the tooling and knowledge that I now have, years later. This is the first in the series.</p>"},{"location":"firewall-tables/#scenario","title":"Scenario","text":"<p>The issue at hand was there was an issue with a particular firewall system that was causing various connection issues to occur within the Microsoft Office suite of communications. There were some tables that were tracking Microsoft RPC connections through the firewall that were filling up. There were no log messages indicating that this was happening. Nor were there any metrics available via SNMP at the time.</p> <p>Info</p> <p>I forget the details about how it was diagnosed as an issue and made its way to the firewall as the culprit. But that is not important.</p>"},{"location":"firewall-tables/#how-the-issue-was-solved","title":"How The Issue Was Solved","text":"<p>The issue was initially solved by a crawl, walk, walk faster initiative. I would typically go down the path of crawl, walk, run path, but in this case as I look back at things, I would say that there are some other methods that I could have taken to get things further. But was not necessary.</p> <p>First thing was figuring out how to clear the tables, which was done with a quick command. This was being done whenever there was a report of the tables filling up, which was happening actually relatively frequently. On top of having this information at hand, there was an escalation to the firewall vendor to help alleviate the issue in the firmware.</p> <p>The next step was creating an HPNA script to be able to be executed by our second level support team whenever the issue was detected or had a call in to clear the tables. This was a start, but still not quite what was looking for.</p>"},{"location":"firewall-tables/#the-automation-used","title":"The Automation Used","text":"<p>I had been working on developing my Python automation skills at this time. So as a learning opportunity I took the initiative to look at what could I do with a Python script. In this learning time, I had not been so well connected to the community of network automation and thus was not aware of the awesome Netmiko library at the time. So what was I doing? I was essentially recreating it as an internal package. That will be covered more in the next section.</p> <p>So the script was able to log into the firewall. It then read the command output and using regex to parse the output, looking for the current counter value of the RPC tables. When the counter was maxed out, the script would then clear the tables. The script was then set to run on a cron schedule of every 30 minutes. And at that point, things were good to go until the bug could be solved in a main line code.</p>"},{"location":"firewall-tables/#why-not-upgrade","title":"Why Not Upgrade?","text":"<p>The impact was immediate. The anticipated amount of time that it would take the vendor to diagnose and re-create the issue, engineer a patch, and generate an engineering special patch (before being incorporated into the main code base) would take some time. On top of that, the comfort level of running an \"Engineering Special\" patch on the firewalls was not that comfortable. If that was the only fix, then that would be a route to go. But there was another option. Use the automation that was built.</p>"},{"location":"firewall-tables/#what-would-i-do-now","title":"What Would I Do Now?","text":"<p>So many years later, knowing what I know now, there are definitely some things that I would do differently. Many are around working with existing tooling (which did exist in a form back then) or contributing back to open source.</p>"},{"location":"firewall-tables/#what-i-would-do-the-same","title":"What I Would Do The Same","text":"<p>What would I do the same. First the iterative approach was a good path. I would absolutely look to do the same thing again there.</p> <p>The second piece that I am proud that I worked within was the use of the corporate security password management. There was no need to store the credentials to log into devices.</p> <p>Lastly, I'm glad that I used Python to complete the activity. It was something that I could iterate on, had a good time building the script out, and was effective.</p>"},{"location":"firewall-tables/#python-script","title":"Python Script","text":"<p>First, I wrote everything in Python 2.7. This was when there was still a debate between Python 2.7 and 3.4/3.5. I would absolutely change and move along with using Python 3.x, whichever is the latest as I get started.</p> <p>The reason that Python 2.7 was chosen at the time was to be able to have something that was compatible with running on network devices, so that as we continued our development, that we could leverage on box Python. This I should have just moved forward to 3.x.</p> <p>Today Python is moving the version forward rapidly in my opinion. So it is good to plan that if you are just getting started, start with the latest version of Python available. This will help keep longevity in the apps that are developed.</p>"},{"location":"firewall-tables/#netmiko","title":"Netmiko","text":"<p>At the time I was not aware of the network automation community. I ended up learning the same things that the community and the Kirk Byers Netmiko project had worked through. I was working through how to interact with network devices through the Paramiko shell, exactly what had been done before. So I absolutely would have changed to just using the Netmiko library. That is the first thing that I would have done for sure.</p>"},{"location":"firewall-tables/#textfsm","title":"TextFSM","text":"<p>Next up in what I would do differently is that coupled with using Netmiko, I would use TextFSM to handle the command regex parsing. I am doubting that there would have been an NTC-Template available for the command, but I would look to create a template based on the command output. At that point when the template was created, I would then look to submit a PR into the NTC-Templates library so that others would be able to use the same template.</p> <p>Note that Kirk has an excellent blog post about how to use NTC-Templates right in with Netmiko. Previous to the Netmiko 2.0 release, you would need to get the command output and then parse the output separately. These are now all in one with the Netmiko library.</p>"},{"location":"firewall-tables/#monitoring","title":"Monitoring","text":"<p>The last piece with having the script now, I would look to enhance the monitoring of it, to know how often that the tables were filling up, and what the pace was. There are still two methods here, of one generating a log message to send into a logging system. Or to set up a graphing capability using Telegraf, Prometheus, and Grafana. I would look at implementing the graphing solution that I wrote up several years ago on the NTC Blog about Monitoring VPN infrastructure with Netmiko, NTC-Templates, and a Time Series Database</p>"},{"location":"firewall-tables/#summary","title":"Summary","text":"<p>There are definitely some things that I would look to do a small bit differently. But the thing that I like most is that I was able to apply some recently developing skills at the time - Python, to directly get a business outcome. It may have been seen as a break/fix just fixing things, but it was a band aid that was needed, and helped save my personal work/life balance (not having to clear the tables often manually). Network Automation is an ever evolving field. There will be continued maintenance of what is built.</p> <p>Overall, the journey is about continuing to learn. Learning is one of the important aspects of the DevOps culture. I'm hopeful that this may help others on the journey to either learn from my past (would not call them mistakes), or inspire other thoughts of your own. Let me know what you think.</p>"},{"location":"nautobot-custom-link/","title":"Nautobot: Custom Links","text":"<p>One of my favorite features of Nautobot that may not be well known is the capability to put a button on pages that take you to other locations. This can be helpful when lining up the source of truth as that first place that you go, the idea of adding custom links will just help to enforce that as the first place to go. When you look at the idea of a source of truth to help feed other systems, you start to see the topology like below. </p> <p></p> <p>The documentation for creating your own custom links is here: https://docs.nautobot.com/projects/core/en/stable/user-guide/platform-functionality/customlink/</p>"},{"location":"nautobot-custom-link/#custom-link-build","title":"Custom Link Build","text":"<p>Custom Links are built in the Nautobot UI under the <code>Extensibility</code> menu. Let's build a basic button on the demo instance of Nautobot. Let's build a link for Locations that will point to the Nautobot docs page as an example. Some ideas for actual implementations include links to site dashboards in your monitoring tool, or a link on a circuit to the portal page, or a link to CI data inside of your ITSM tool for a device.</p> <ol> <li>Navigate and log into https://demo.nautobot.com (Log in credentials are on the site, as of now demo/nautobot)</li> <li>On the left hand navigation menu, select <code>EXTENSIBILITY</code> and then select <code>Custom Links</code> </li> <li>The site by default does not have any links, there may be a few that demo users create themselves. </li> <li>Select +Add on the upper right</li> <li>Fill out the form with the following:</li> </ol> Field Value Description Content Type <code>dcim \\| location</code> What Nautobot data type this applies to. Name Site Link The name of the custom link. Text Nautobot Docs What the button should say. URL https://docs.nautobot.com What the link is sending to. Weight 100 This helps tweak where the link will appear in relation to other links. Group Name If you want to group links together. Button Class Info (Aqua) This colors the button for the link. New Window Checked Opens the link in a new tab/window. <p></p> <p>Click on <code>Create</code> on the bottom. This will create the link. Now when you navigate to a Location, you will now see a button on the upper right that will say <code>Nautobot Docs</code> that will take you to the Nautobot documentation pages.</p> <p></p>"},{"location":"nautobot-custom-link/#advanced-link-creation","title":"Advanced Link Creation","text":"<p>One of the awesome things that comes along with the Custom Links is the ability to put some logic into the text that you put there, such as only show the button if it is a particular device type, or manufacturer. This may also play in with systems that are part of a controller based system. Perhaps a link to Arista CloudVision if the device is an Arista device. I have done links to Grafana dashboards using the Modern Telemetry system that we have deployed, only if the device is of a particular device type that would be created to be in the monitoring solution.</p> <p>Let's make two more links using the same steps above, but with the custom logic of a link to CloudVision when the device manufacturer is Arista. If the device manufacturer is Cisco then link to the Cisco controller. In both instances for the demo, it will be a link to the manufacturer's webpage. The custom text we will want is the following (will be a single line in the text of the button):</p>"},{"location":"nautobot-custom-link/#arista-cloudvision-custom-link","title":"Arista CloudVision Custom Link","text":"<pre><code>{% if obj.device_type.manufacturer.name == \"Arista\" %}\nCloudVision\n{% endif %}\n</code></pre> Field Value Description Content Type <code>dcim \\| device</code> What Nautobot data type this applies to. Name Arista CloudVision The name of the custom link. Text {% if obj.device_type.manufacturer.name == \"Arista\" %}CloudVision{% endif %} What the button should say. URL https://www.arista.com/en/products/eos/eos-cloudvision What the link is sending to. Weight 100 This helps tweak where the link will appear in relation to other links. Group Name If you want to group links together. Button Class Warning (Orange) This colors the button for the link. New Window Checked Opens the link in a new tab/window."},{"location":"nautobot-custom-link/#cisco-custom-link","title":"Cisco Custom Link","text":"<pre><code>{% if obj.device_type.manufacturer.name == \"Cisco\" %}\nCisco\n{% endif %}\n</code></pre> Field Value Description Content Type <code>dcim \\| device</code> What Nautobot data type this applies to. Name Cisco The name of the custom link. Text {% if obj.device_type.manufacturer.name == \"Cisco\" %}Cisco{% endif %} What the button should say. URL https://www.cisco.com What the link is sending to. Weight 100 This helps tweak where the link will appear in relation to other links. Group Name If you want to group links together. Button Class Warning (Orange) This colors the button for the link. New Window Checked Opens the link in a new tab/window."},{"location":"nautobot-custom-link/#demonstration","title":"Demonstration","text":"<p>Now when you navigate to devices on Nautobot, when the device type manufacturer is present, you have new buttons that are available. Taking a look at an Arista device you now see the CloudVision button the top right.</p> <p></p> <p>And when you navigate to a Cisco device as expected you have the Cisco button.</p> <p></p>"},{"location":"nautobot-custom-link/#summary","title":"Summary","text":"<p>Nautobot's Custom Links are a valuable part of building out the Network Automation Source of Truth. Knowing that there are going to be other systems, it is good to link to those other systems to help speed access to the particular device types. This is one more thing that will help drive the source of truth concept that is valuable inside of an organization, by allowing it to be a starting point of what the system should look like, and quickly accessing other data points that would be of value. I personally am a big fan of the Custom Links and have been driving them for a while. Custom Links are available in all versions of Nautobot.</p> <p>Thanks for reading!</p> <p>-Josh</p>"},{"location":"archive/2024/","title":"2024","text":""},{"location":"archive/2023/","title":"2023","text":""},{"location":"archive/2022/","title":"2022","text":""},{"location":"archive/2021/","title":"2021","text":""},{"location":"archive/2020/","title":"2020","text":""},{"location":"archive/2019/","title":"2019","text":""},{"location":"archive/2018/","title":"2018","text":""},{"location":"category/automation/","title":"automation","text":""},{"location":"category/redux/","title":"redux","text":""},{"location":"category/networking/","title":"networking","text":""},{"location":"category/documentation/","title":"documentation","text":""},{"location":"category/nautobot/","title":"nautobot","text":""},{"location":"category/programming/","title":"programming","text":""},{"location":"category/ansible/","title":"ansible","text":""},{"location":"category/productivity/","title":"productivity","text":""},{"location":"category/communications/","title":"communications","text":""},{"location":"category/slack/","title":"slack","text":""},{"location":"category/tags/","title":"tags","text":""},{"location":"category/network_design/","title":"network_design","text":""},{"location":"category/wan/","title":"wan","text":""},{"location":"category/devnet/","title":"devnet","text":""},{"location":"category/expert/","title":"expert","text":""},{"location":"category/debian/","title":"debian","text":""},{"location":"category/linux/","title":"linux","text":""},{"location":"category/authentication/","title":"authentication","text":""},{"location":"category/golf/","title":"golf","text":""},{"location":"category/earth/","title":"earth","text":""},{"location":"category/golang/","title":"golang","text":""},{"location":"category/python/","title":"python","text":""},{"location":"category/security/","title":"security","text":""},{"location":"category/flask/","title":"flask","text":""},{"location":"category/poetry/","title":"poetry","text":""},{"location":"category/hashicorp/","title":"hashicorp","text":""},{"location":"category/vault/","title":"vault","text":""},{"location":"category/cisco/","title":"cisco","text":""},{"location":"category/netmiko/","title":"netmiko","text":""},{"location":"category/blog/","title":"blog","text":""},{"location":"category/graphql/","title":"graphql","text":""},{"location":"category/meraki/","title":"meraki","text":""},{"location":"category/nornir/","title":"nornir","text":""},{"location":"category/arista/","title":"arista","text":""},{"location":"category/juniper/","title":"juniper","text":""},{"location":"category/netbox/","title":"netbox","text":""},{"location":"category/homeassistant/","title":"homeassistant","text":""},{"location":"category/prometheus/","title":"prometheus","text":""},{"location":"category/eveng/","title":"eveng","text":""},{"location":"category/network-simulation/","title":"network simulation","text":""},{"location":"category/netdevops/","title":"netdevops","text":""},{"location":"category/asa/","title":"asa","text":""},{"location":"category/asa_og/","title":"asa_og","text":""},{"location":"category/cisco_ios/","title":"cisco_ios","text":""},{"location":"category/cisco_wlc/","title":"cisco_wlc","text":""},{"location":"category/cisco_nxos/","title":"cisco_nxos","text":""},{"location":"category/saving_config/","title":"saving_config","text":""},{"location":"category/ios_interface/","title":"ios_interface","text":""},{"location":"page/2/","title":"\ud83c\udfe0 Home","text":""},{"location":"page/3/","title":"\ud83c\udfe0 Home","text":""},{"location":"page/4/","title":"\ud83c\udfe0 Home","text":""},{"location":"page/5/","title":"\ud83c\udfe0 Home","text":""},{"location":"page/6/","title":"\ud83c\udfe0 Home","text":""},{"location":"page/7/","title":"\ud83c\udfe0 Home","text":""},{"location":"page/8/","title":"\ud83c\udfe0 Home","text":""},{"location":"page/9/","title":"\ud83c\udfe0 Home","text":""},{"location":"archive/2023/page/2/","title":"2023","text":""},{"location":"archive/2023/page/3/","title":"2023","text":""},{"location":"archive/2021/page/2/","title":"2021","text":""},{"location":"archive/2020/page/2/","title":"2020","text":""},{"location":"archive/2019/page/2/","title":"2019","text":""},{"location":"category/ansible/page/2/","title":"ansible","text":""},{"location":"category/ansible/page/3/","title":"ansible","text":""},{"location":"category/ansible/page/4/","title":"ansible","text":""},{"location":"category/arista/page/2/","title":"arista","text":""},{"location":"category/automation/page/2/","title":"automation","text":""},{"location":"category/cisco/page/2/","title":"cisco","text":""},{"location":"category/juniper/page/2/","title":"juniper","text":""},{"location":"category/nautobot/page/2/","title":"nautobot","text":""},{"location":"category/nautobot/page/3/","title":"nautobot","text":""},{"location":"category/netbox/page/2/","title":"netbox","text":""},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#ansible","title":"ansible","text":"<ul> <li>Ansible Blocks</li> <li>Ansible - Working with command output</li> <li>Ansible differences between ios config and cli config</li> <li>Ansible Network Engine and NTC Templates</li> <li>Ansible IOS Banner</li> <li>Ansible IOS VLAN</li> <li>Ansible IOS BGP Module</li> <li>Ansible differences between ios command and cli command</li> <li>Ansible Cisco ios_interfaces module</li> <li>Ansible Cisco IOS User Module</li> <li>Docker for Automation Environment - Ansible 2.10</li> <li>Ansible for Enterprise</li> <li>Practicing Network Automation with GNS3</li> </ul>"},{"location":"tags/#blog","title":"blog","text":"<ul> <li>Getting Started with the Blog</li> <li>Ansible Cisco ios_interfaces module</li> <li>2020 Blog Update</li> <li>Jekyll - Adding a URL Redirection</li> </ul>"},{"location":"tags/#book","title":"book","text":"<ul> <li>New Book - Open Source Network Management</li> </ul>"},{"location":"tags/#ccdevnetexpert","title":"ccdevnetexpert","text":"<ul> <li>DevNet Expert - Starting Point</li> </ul>"},{"location":"tags/#cicd","title":"ci/cd","text":"<ul> <li>Network CI/CD - work in progress (Links to other videos/pages)</li> </ul>"},{"location":"tags/#cisco","title":"cisco","text":"<ul> <li>Ansible - Working with command output</li> <li>Ansible differences between ios config and cli config</li> <li>Ansible Network Engine and NTC Templates</li> <li>Ansible IOS Banner</li> <li>Ansible IOS VLAN</li> <li>Ansible IOS BGP Module</li> <li>Ansible differences between ios command and cli command</li> <li>Ansible Cisco ios_interfaces module</li> <li>Ansible Cisco IOS User Module</li> <li>Practicing Network Automation with GNS3</li> <li>Nautobot Jobs - Your Custom API Endpoint</li> <li>DevNet Expert - Starting Point</li> </ul>"},{"location":"tags/#cli_command","title":"cli_command","text":"<ul> <li>Ansible differences between ios config and cli config</li> <li>Ansible differences between ios command and cli command</li> </ul>"},{"location":"tags/#deprecation","title":"deprecation","text":"<ul> <li>Ansible Cisco ios_interfaces module</li> </ul>"},{"location":"tags/#devnet","title":"devnet","text":"<ul> <li>DevNet Expert - Starting Point</li> </ul>"},{"location":"tags/#disney","title":"disney","text":"<ul> <li>Disney Plus Streaming Bandwidth</li> </ul>"},{"location":"tags/#docker","title":"docker","text":"<ul> <li>Docker for Automation Environment - Ansible 2.10</li> </ul>"},{"location":"tags/#enterprise-design","title":"enterprise design","text":"<ul> <li>Ansible for Enterprise</li> </ul>"},{"location":"tags/#gns3","title":"gns3","text":"<ul> <li>Practicing Network Automation with GNS3</li> </ul>"},{"location":"tags/#hashnode","title":"hashnode","text":"<ul> <li>2020 Blog Update</li> </ul>"},{"location":"tags/#homenet","title":"homenet","text":"<ul> <li>Disney Plus Streaming Bandwidth</li> </ul>"},{"location":"tags/#hugo","title":"hugo","text":"<ul> <li>2020 Blog Update</li> </ul>"},{"location":"tags/#ios_banner","title":"ios_banner","text":"<ul> <li>Ansible IOS Banner</li> </ul>"},{"location":"tags/#ios_bgp","title":"ios_bgp","text":"<ul> <li>Ansible IOS BGP Module</li> </ul>"},{"location":"tags/#ios_command","title":"ios_command","text":"<ul> <li>Ansible differences between ios config and cli config</li> <li>Ansible differences between ios command and cli command</li> </ul>"},{"location":"tags/#ios_user","title":"ios_user","text":"<ul> <li>Ansible Cisco IOS User Module</li> </ul>"},{"location":"tags/#ios_vlan","title":"ios_vlan","text":"<ul> <li>Ansible IOS VLAN</li> </ul>"},{"location":"tags/#jekyll","title":"jekyll","text":"<ul> <li>2020 Blog Update</li> <li>Jekyll - Adding a URL Redirection</li> </ul>"},{"location":"tags/#jobs","title":"jobs","text":"<ul> <li>Nautobot Jobs - Your Custom API Endpoint</li> </ul>"},{"location":"tags/#learning","title":"learning","text":"<ul> <li>Keeping Up on Tech</li> </ul>"},{"location":"tags/#mac","title":"mac","text":"<ul> <li>Using Apple Automator to Open Projects</li> </ul>"},{"location":"tags/#meraki","title":"meraki","text":"<ul> <li>Nautobot Jobs - Your Custom API Endpoint</li> </ul>"},{"location":"tags/#nautobot","title":"nautobot","text":"<ul> <li>Nautobot Jobs - Your Custom API Endpoint</li> </ul>"},{"location":"tags/#netdevops","title":"netdevops","text":"<ul> <li>Network CI/CD - work in progress (Links to other videos/pages)</li> <li>Ansible Cisco IOS User Module</li> <li>Docker for Automation Environment - Ansible 2.10</li> </ul>"},{"location":"tags/#network-automation","title":"network automation","text":"<ul> <li>Ansible Cisco IOS User Module</li> </ul>"},{"location":"tags/#network-design","title":"network design","text":"<ul> <li>Micro Segmentation vs Segmentation</li> <li>Discontiguous Masks</li> </ul>"},{"location":"tags/#network-simulator","title":"network simulator","text":"<ul> <li>Practicing Network Automation with GNS3</li> </ul>"},{"location":"tags/#networkmanagment","title":"networkmanagment","text":"<ul> <li>New Book - Open Source Network Management</li> <li>Nautobot Jobs - Your Custom API Endpoint</li> </ul>"},{"location":"tags/#ntc","title":"ntc","text":"<ul> <li>Ansible Network Engine and NTC Templates</li> </ul>"},{"location":"tags/#opensource","title":"opensource","text":"<ul> <li>New Book - Open Source Network Management</li> </ul>"},{"location":"tags/#parsing","title":"parsing","text":"<ul> <li>Ansible Network Engine and NTC Templates</li> </ul>"},{"location":"tags/#productivity","title":"productivity","text":"<ul> <li>Keeping Up on Tech</li> <li>Using Apple Automator to Open Projects</li> </ul>"},{"location":"tags/#segmentation","title":"segmentation","text":"<ul> <li>Discontiguous Masks</li> </ul>"},{"location":"tags/#sourceoftruth","title":"sourceoftruth","text":"<ul> <li>Nautobot Jobs - Your Custom API Endpoint</li> </ul>"},{"location":"tags/#streaming","title":"streaming","text":"<ul> <li>Disney Plus Streaming Bandwidth</li> </ul>"},{"location":"tags/#vscode","title":"vscode","text":"<ul> <li>Using Apple Automator to Open Projects</li> </ul>"}]}